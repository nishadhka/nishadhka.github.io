<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Software on Nishadh KA</title>
    <link>/categories/software/</link>
    <description>Recent content in Software on Nishadh KA</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Dec 2014 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/software/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>WRF CHEM cluster</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_cluster/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_cluster/</guid>
      <description>####WRFCHEMsimluation by cluster####
#to Do 1. write the script, lazywrfchem 1. draw computational inventory of the cluster comparision experiment, and libnrary program needed 1. compile the library 1. do the experiment
#single computer HP laptop, parallell execution, three cores
Domain four: onehour=12:16 to 08:43, 8.5 hours so for 6 hours simulation= 48+24=72 hours Domain Three: one hour=01:24 to 02:24, 1 hour so for 6 hours simulation= 6 hours Domain two: onehour=9:10 to 9:23, 13 minutes so for 6 hours simulation=13minx6=1.</description>
    </item>
    
    <item>
      <title>WRF CHEM compile QN</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn/</guid>
      <description>####Compile wrf chem: QUICK NOTE####
 It is based on the NCAR wrf chem compile complete process and note on wrf chem compile by this Basic environemtn testl, passes all the tests which checks libs such as, gcc, gfortran, gcc+gfortran, perl, csh. Made sure the gcc and gfortran versions are matching, current case version is 4.8.2 for both gcc and gfortran Basic library compile MPICh and NETCDF  For setting up PATH for NETCDF</description>
    </item>
    
    <item>
      <title>WRF CHEM compile QN AWS</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_aws/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_aws/</guid>
      <description>#####Compile wrf chem in AWS: QUICK NOTE#####
 It is based on the NCAR wrf chem compile complete process and note on wrf chem compile by this Create a ubuntu trusty based image. Make a small elastic block storage 8GB, it will be permanent even if the instance is kept stop. There will be storage space in /mnt for upto 300GB based on instance, for example c3.large has it. This storage gets erased while stopping the instance but not the /home folder where the small 8GB EBS is being mounted/ Install basic required libs sudo apt-get install g++ sudo apt-get install gfortran sudo apt-get install gcc Basic environment test, make sure the test passes all good for libs checks such as, gcc, gfortran, gcc+gfortran, perl, csh.</description>
    </item>
    
    <item>
      <title>WRF CHEM compile QN CT</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_ct/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_ct/</guid>
      <description>####Compile wrf chem in CubieTruck CT: QUICK NOTE####
 It is based on the NCAR wrf chem compile complete process and note on wrf chem compile by this Loaded a ubuntu trusty based image from here. Solved the issue of no Internet in CT by editing the interfaces file as follows
 # interfaces(5) file used by ifup(8) and ifdown(8) # Include files from /etc/network/interfaces.d: source-directory /etc/network/interfaces.d auto lo eth0 iface lo inet loopback auto eth0 iface eth0 inet static address 192.</description>
    </item>
    
    <item>
      <title>WRF CHEM compile QN PL</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_pl/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_pl/</guid>
      <description>####Compile wrf chem in Parallella PL: QUICK NOTE####
 It is based on the NCAR wrf chem compile complete process and note on wrf chem compile by this Loaded a ubuntu trusty based image from here. Solved the issue of no Internet in PL by editing the interfaces file as follows and changing the files networkmanager.onf and remove net rules as stated in Parallella up and run note.
 # interfaces(5) file used by ifup(8) and ifdown(8) # Include files from /etc/network/interfaces.</description>
    </item>
    
    <item>
      <title>WRFCHEM output PM25PM10</title>
      <link>/working-notes/2014/wn_2014-11/wrfchem_output_pm25pm10/</link>
      <pubDate>Wed, 26 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/wrfchem_output_pm25pm10/</guid>
      <description>####WRF CHEM output into PM25 and PM10####
 To get various variables from WRF OUTPUT following python codes and references were used #####to get WRF output NETCDF into python##### The below code import netcdf file into python and subset the data based on Coimbatore domain of  from netCDF4 import Dataset import numpy as np wrfoutput = &#39;wrfout_d01_2014-06-05_05:00:00_D03&#39; fh = Dataset(wrfoutput, mode=&#39;r&#39;) #to view all the variables in imported wrf output netcdf vars = fh.</description>
    </item>
    
    <item>
      <title>NCL install ubunut1204</title>
      <link>/working-notes/2014/wn_2014-11/ncl_install_ubunut1204/</link>
      <pubDate>Thu, 20 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/ncl_install_ubunut1204/</guid>
      <description>http://www.ncl.ucar.edu/Download/install.shtml</description>
    </item>
    
    <item>
      <title>Plot Aerocet sample D3JS</title>
      <link>/working-notes/2014/wn_2014-11/plot_aerocet_sample_d3js/</link>
      <pubDate>Sun, 16 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/plot_aerocet_sample_d3js/</guid>
      <description>####Plot aerocet data with D3 JS scrip####
 Based on this and its source code. The meteor web application is based on one index.html file and marker visualizer based on JS drawMap.js. The index.html was edited for to remove the CHART in the line 14 to 19 contains  &amp;lt;div id=&amp;quot;year-chart&amp;quot; class=&amp;quot;chart display&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;title&amp;quot;&amp;gt;Year of Impact&amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div id=&amp;quot;mass-chart&amp;quot; class=&amp;quot;chart display&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;title&amp;quot;&amp;gt;Mass (g)&amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;   IN drawMap.js, following lines were removed ``` function renderAll(){ chart.</description>
    </item>
    
    <item>
      <title>git delete routine</title>
      <link>/working-notes/2014/wn_2014-11/git_delete_routine/</link>
      <pubDate>Sat, 15 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/git_delete_routine/</guid>
      <description>####git delete routine####
 Using git and github, to remove a file which is committed earlier is requier following steps. Even though the file will be removed with delete button option of OS. The committed files still be inside the git version control system and it will make problem in uploading it into github if it is large file size &amp;gt;100MB accidently committed. First to list the files deleted in the OS but not in git system, based on this.</description>
    </item>
    
    <item>
      <title>Excel Pandas LATEX PDF</title>
      <link>/working-notes/2014/wn_2014-10/excel_pandas_latex_pdf/</link>
      <pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-10/excel_pandas_latex_pdf/</guid>
      <description>###CSV data into pandas and LATEX then into PDF and then print!###
 To import csv file with date into pandas dataframe. Pands can parse date column, but it won&amp;rsquo;t be correct. The date column formate has top be mentiod and parsed for this, like below based on [this], then this can be imported into pandas datafrem emntioing the dateparse.  dateparse = lambda x: pd.datetime.strptime(x, &amp;lsquo;%d/%m/%y&amp;rsquo;) pro=pd.readcsv(&amp;lsquo;PROJECTEXPNDTR1.csv&amp;rsquo;,parsedates=[1],date_parser=dateparse)
dateparse = lambda x: pd.</description>
    </item>
    
    <item>
      <title>PythonGDALproblem</title>
      <link>/working-notes/2014/wn_2014-10/pythongdalproblem/</link>
      <pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-10/pythongdalproblem/</guid>
      <description>####Python GDAL problem####
ImportError: /usr/lib/libgdal.so.1: undefined symbol: sqlite3columntable_name
main.gdalconfigerror: [Errno 2] No such file or directory
sudo apt-cache showpkg sqlite3
ogrinfo grid.shp -dialect sqlite -sql &amp;ldquo;select sqlite_version()&amp;rdquo;
ldconfig -p
install.packages(filenameand_path, repos = NULL, type=&amp;ldquo;source&amp;rdquo;)
http://marc.info/?l=grass-dev&amp;amp;m=138736238422272&amp;amp;w=2
http://askubuntu.com/questions/443379/sqlite-header-and-source-version-mismatch
http://stackoverflow.com/questions/16095942/sqlite-header-and-source-version-mismatch/16366457#16366457
for R based netcdf view
https://gist.github.com/xuanlongma/5874674
http://stackoverflow.com/questions/11319698/how-to-install-r-packages-rnetcdf-and-ncdf-on-ubuntu
install.packages(repos=c(&amp;lsquo;http://cran.fhcrc.org/&#39;),pkgs=c(&#39;ncdf&#39;),lib=&amp;quot;/usr/lib/R/site-library/&amp;quot;,configure.args=&amp;quot;--with-netcdf-include=/usr/local/include &amp;ndash;with-netcdf-lib=/usr/local/lib&amp;rdquo;)
1.correcting the grid of coimbatore urban 1</description>
    </item>
    
    <item>
      <title>Timeseries Pandas</title>
      <link>/working-notes/2014/wn_2014-10/timeseries_pandas/</link>
      <pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-10/timeseries_pandas/</guid>
      <description>###Time series data plot and table creation with pandas and latex###
 to import the csv file into pandas df = pd.read_csv(&#39;/home/swl-sacon-dst/Documents/GISE_2013/LAB/Aerocet_DATA/TDM/TDM_MASS_20102014_171059-073359.csv&#39;), based on this To sepcifiy date time index in the dateframe df = df.set_index(pd.DatetimeIndex(df[&#39;Time&#39;])), based on this To resample 1 minute data into 15 minutes by avergae method, bars=df.resample(&#39;15min&#39;), here default method is mean. based on this To select specific columns in pandas df1=df[[&#39;Time&#39;,&#39;PM2.5(ug/m3)&#39;,&#39;PM10(ug/m3)&#39;,&#39;TSP(ug/m3)&#39;,&#39;AT(C)&#39;,&#39;RH(%)&#39;]] based on this To plot datframe by import matplotlib.</description>
    </item>
    
    <item>
      <title>compileGSIonUbuntu1404</title>
      <link>/working-notes/2014/wn_2014-09/compilegsionubuntu1404/</link>
      <pubDate>Sat, 20 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/compilegsionubuntu1404/</guid>
      <description>###compile GSI in Ubunut 14.04###
 GSI was tried to compile in Ubuntu 12.04, but ended with failure. GSI requieres Gfortran 4.7 and above, ubunut 12.04 repostory is updated upto 4.6, so ubunut has to upgraded from 12.04 to 14.04 to have gfortran 4.7. Due to this Server was upghraded from Ubuntu 12.04 to 14.04 and compiled WRFV4.3.1 and WPS 4.3.1 in it. It was by Gfortran 4.8.2, Netcdf 4.1.3, and other necssary libraries such as libpng, zlib and jasper as per apt-get method.</description>
    </item>
    
    <item>
      <title>WRF CHEM Compile completenote SERVER</title>
      <link>/working-notes/2014/wn_2014-09/wrf_chem_compile_completenote_server/</link>
      <pubDate>Wed, 10 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/wrf_chem_compile_completenote_server/</guid>
      <description>export DIR=/home/hoopoe/wrfchem341/lib/Netcdf4.1.3libs
sftp://hoopoe@192.168.1.100/home/hoopoe/wrfchem341/lib/Netcdf4.1.3libs/netcdf/include
export CC=gcc export CXX=g++ export FC=gfortran export FCFLAGS=-m64 export F77=gfortran export FFLAGS=-m64
export PATH=$DIR/mpich/bin:$PATH
export NETCDF=$DIR/netcdf export JASPERLIB=$DIR/grib2/lib export JASPERINC=$DIR/grib2/include export WRFEMCORE=1 export WRFNMMCORE=0 export WRFCHEM=1 export WRFKPP=0 export WRFIONCDLARGEFILESUPPORT=1
error 1. /bin/sh: 6: mpif90: not found 1. gfortran treating comments as code baased on this 1. still no wrf.exe was formed, by seeing configure.wrf thought environemnt WRFSRCROOTDIR has to be set export WRFSRCROOTDIR=/home/hoopoe/wrfchem341/WRFV3 1. installed sudo apt-get install libcloog-ppl1 1.</description>
    </item>
    
    <item>
      <title>forumpost</title>
      <link>/working-notes/2014/wn_2014-09/forumpost/</link>
      <pubDate>Tue, 09 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/forumpost/</guid>
      <description>there are example of porting WRF model in ARM processors. Fine grained parallellism, OpenMP, 16bytes genration in GPU exampel study
WRF is developed with single floating point oeprations and newer modules of WRF such as WRF CHEM for air quality predictions are having single as well as double floating point implemntaiton. There are studies comparing GPU enhanment with double and single point operations.
Plan of server board cluster Hardware enahmenet Limitation of OpenMP</description>
    </item>
    
    <item>
      <title>WRFCHEM CBE A1</title>
      <link>/working-notes/2014/wn_2014-09/wrfchem_cbe_a1/</link>
      <pubDate>Mon, 08 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/wrfchem_cbe_a1/</guid>
      <description>###WRF CHEM for Coimbatore A1 using ndown.exe###
####WPS#### 1. The WPS components geogrid.exe, ungrib.exe and metgrid.exe was from the output of last run on detailed in this
####real.exe#### 1. Real.exe also from the last run output. The output comprised of files namely wrfbdyd01, wrfinputd01, wrfinputd02, wrfinputd03, wrfinputd04 were used as such for convertemiss.exe run.
####PrepChemSrc.exe#### 1. It was executed as per the tutorial and PrepChemSrc.exe was executed by following point 3 in that tutorial.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part70 CBE DustOnlyTutorial</title>
      <link>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part70_cbe_dustonlytutorial/</link>
      <pubDate>Sat, 06 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part70_cbe_dustonlytutorial/</guid>
      <description>##Trail for WRF-CHEM, WRF CHEM simulation with dust only for Coimbatore using TWO WAY NESTING TWO files method## 1. It is based on this to run compiled wrf chem in serial mode with only dust.
###WPS:Geogrid### 9. As per this point 7, file GEOGRID.TBL.ARW_CHEM in WPS geogrid directory was linked as GEOGRID.TBL using GUI of Ubuntu 12.04. 10. The namelist.wps used was as follows,
 &amp;amp;share max_dom = 4, start_date =&#39;2014-06-05_00:00:00&#39;,&#39;2014-06-05_00:00:00&#39;,&#39;2014-06-05_00:00:00&#39;,&#39;2014-06-05_00:00:00&#39; end_date =&#39;2014-06-05_06:00:00&#39;,&#39;2014-06-05_06:00:00&#39;,&#39;2014-06-05_06:00:00&#39;,&#39;2014-06-05_06:00:00&#39; interval_seconds = 10800, io_form_geogrid = 2, / &amp;amp;geogrid parent_id = 1, 1, 2, 3 parent_grid_ratio = 1, 3, 3, 3 i_parent_start = 1, 26, 22, 22 j_parent_start = 1, 7, 15, 32 e_we = 90, 76, 97, 136 e_sn = 85, 73, 106, 157 geog_data_res = &#39;10m&#39;, &#39;5m&#39;, &#39;30s&#39;, &#39;30s&#39; dx = 27000 dy = 27000 map_proj = &#39;lambert&#39; ref_lat = 18.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part71 emission inventory CBE Ndown</title>
      <link>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part71_emission_inventory_cbe_ndown/</link>
      <pubDate>Sat, 06 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part71_emission_inventory_cbe_ndown/</guid>
      <description>###WRF CHEM for Coimbatore domain with emission inventory and one way nesting using ndown.exe###
####WPS#### 1. The WPS components geogrid.exe, ungrib.exe and metgrid.exe was from the output of last run on detailed in this
####real.exe#### 1. Real.exe also from the last run output. The output comprised of files namely wrfbdyd01, wrfinputd01, wrfinputd02, wrfinputd03, wrfinputd04 were used as such for convertemiss.exe run.
####PrepChemSrc.exe#### 1. It was executed as per the tutorial and PrepChemSrc.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part65 EmissionInventoryTut</title>
      <link>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part65_emissioninventorytut/</link>
      <pubDate>Thu, 04 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part65_emissioninventorytut/</guid>
      <description>##Trail for WRF-CHEM, WRF CHEM simulation with dust only tutorial## 1. It is based on this to run compiled wrf chem in serial mode with only dust.
###WPS### 1. WPS components ungrib.exe,geogrid.exe and metgrid.exe was from former dust only run&amp;rsquo;s output based on the note of . ###PrepChemSrc### 1. Prep chem src used was of compiled as per the note. This was ran as per the tutorial point4, the namelist used is this.</description>
    </item>
    
    <item>
      <title>GSI assimilation PM25PM10 WRFCHEM</title>
      <link>/working-notes/2014/wn_2014-08/gsi_assimilation_pm25pm10_wrfchem/</link>
      <pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/gsi_assimilation_pm25pm10_wrfchem/</guid>
      <description>###PM10,PM2.5 observation assimilation for WRF CHEM based on GSI###
based on this paper Implementation of aerosol assimilation in Gridpoint Statistical Interpolation (v. 3.2) and WRF-Chem (v. 3.4.1)
http://www.geosci-model-dev.net/7/1621/2014/gmd-7-1621-2014.pdf</description>
    </item>
    
    <item>
      <title>SOSforWRFCHEM output</title>
      <link>/working-notes/2014/wn_2014-08/sosforwrfchem_output/</link>
      <pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/sosforwrfchem_output/</guid>
      <description>###Sensor Observation service for WRF CHEM output###</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part63 WRF CoimbatoreNesting</title>
      <link>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part63_wrf_coimbatorenesting/</link>
      <pubDate>Tue, 19 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part63_wrf_coimbatorenesting/</guid>
      <description>##Trail for WRF-CHEM, WRF simulation with domain for Coimbatore and one way Nesting-Ndown.exe## 1. It is based on this to run compiled wrf in serial mode to execute domain over Coimbatore with nested model run by Ndown.exe based one way nesting.
###WPS: Ungrib###
 As per this page, the WPS ungrib stage was carried out. The gfs files downloaded was used, average size around 45MB for each files. The files for time peroid 00,03, 06 was kept under a folder gfs.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part64 DustOnlyTutorial</title>
      <link>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part64_dustonlytutorial/</link>
      <pubDate>Tue, 19 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part64_dustonlytutorial/</guid>
      <description>##Trail for WRF-CHEM, WRF CHEM simulation with dust only tutorial## 1. It is based on this to run compiled wrf chem in serial mode with only dust.
###WPS:Geogrid### 9. As per this point 7, file GEOGRID.TBL.ARW_CHEM in WPS geogrid directory was linked as GEOGRID.TBL using GUI of ubuntu 12.04. 10. The namelist.wps from the page was used. The namelist.wps is as follows,
 &amp;amp;share wrf_core = &#39;ARW&#39;, max_dom = 1, start_date = &#39;2010-07-14_00:00:00&#39;,&#39;2010-07-14_00:00:00&#39;, end_date = &#39;2010-07-19_00:00:00&#39;,&#39;2010-07-19_00:00:00&#39;, interval_seconds = 10800, io_form_geogrid = 2, / &amp;amp;geogrid parent_id = 1, 1, 1, parent_grid_ratio = 1, 5, 5, i_parent_start = 1, 6, 105, j_parent_start = 1, 65, 25, e_we = 41, 201, 226, e_sn = 41, 311, 231, geog_data_res = &#39;10m&#39;, &#39;2m&#39;, &#39;30s&#39; dx = 100000, dy = 100000, map_proj = &#39;lambert&#39;, ref_lat = 35.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part60 dustonly CBE</title>
      <link>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part60_dustonly_cbe/</link>
      <pubDate>Tue, 12 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part60_dustonly_cbe/</guid>
      <description>##WRF CHEM for Coimbatore domain with dust only##
 Made a copy of WPS folder and WRFv3, parallel compiled folder test/realem* into a new folder of cbedomain. By doing mere copy paste, the link was broken and no correct files were copied. Instead of this a try was made as per the m2lab tutorial and copied the files from em_real as like this cp /home/swl-sacon-dst/wrf/WRF341/WRFV3_par/test/em_real/* ., this time the link was not made but the executables was copied, it has to checked weather it is working For starting WPS of WRF with geogrid, used the namelist.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part61 WRF KatrinaSingleDomain</title>
      <link>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part61_wrf_katrinasingledomain/</link>
      <pubDate>Tue, 12 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part61_wrf_katrinasingledomain/</guid>
      <description>##Trail for WRF-CHEM, working with WRF simulation with Katrina single domain case## 1. It is based on this to run compiled wrf in serial mode to execute the Katrina case with single domain. 2. As per this page, the WPS ungrib stage was carried out
###WPS: Ungrib### 3. The tar file provided with the tutorial was unzipped and compiled WPS folder was copied and bothe these folders were kept under a folder named katrina.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part62 WRF KatrinaNesting</title>
      <link>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part62_wrf_katrinanesting/</link>
      <pubDate>Tue, 12 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part62_wrf_katrinanesting/</guid>
      <description>##Trail for WRF-CHEM, WRF simulation with Katrina case and one way Nesting-Ndown.exe## 1. It is based on this to run compiled wrf in serial mode to execute the Katrina case with nested model run by Ndown.exe based one way nesting.
###WPS: Ungrib###
 As per this page, the WPS ungrib stage was carried out. The tar file provided with the tutorial was unzipped and compiled WPS folder was copied and both these folders were kept under a folder named katrina as katrina_nesting.</description>
    </item>
    
    <item>
      <title>python script InsertObservation istSOS xml</title>
      <link>/working-notes/2014/wn_2014-08/python_script_insertobservation_istsos_xml/</link>
      <pubDate>Wed, 06 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/python_script_insertobservation_istsos_xml/</guid>
      <description>##python script to InsertObservation into istSOSxml way## 1. Based on the early experience of inserting observation in xml way with python using HTTP post in 52North SOS, made a try with Ist SOS, other way to do is using calling another python script to import data in csv formate. 2. Based on this note on OWS service [requests](https://geoservice.ist.supsi.ch/projects/istsos/index.php/ExampleofSOSrequest). to get the DescribeSensor information used this link, it always failed in specfying formate but other request was succeffull</description>
    </item>
    
    <item>
      <title>English Grammer checker</title>
      <link>/working-notes/2014/wn_2014-08/english_grammer_checker/</link>
      <pubDate>Fri, 01 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/english_grammer_checker/</guid>
      <description>###Find the tense and voices in a sentence: English grammar checker###
 Search made to find various type of tense in a sentence. Ended in this, sf page. Stanford core NLP and NLTK with python are good natural language processors used for this purposes. NLP gives statistical measure of likely role of words in a sentence, Standford NLP is giving a web service with visualization of various statical measures in part-of-speech tags.</description>
    </item>
    
    <item>
      <title>gh pages HTML css js</title>
      <link>/working-notes/2014/wn_2014-07/gh_pages_html_css_js/</link>
      <pubDate>Tue, 29 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/gh_pages_html_css_js/</guid>
      <description>###gh-pagesHTMLcss_js###
 HTML, css and js is enoght for creating a web site. In github this is determental in any gh-pages intended to create. There are several templates specifically available, useed in gh-pages For simple markdown template this template was used. https://github.com/aplib/markdown-site-template Final usage of this web site was selected based on its demonstration  </description>
    </item>
    
    <item>
      <title>Latex beamer based pdf presntation</title>
      <link>/working-notes/2014/wn_2014-07/latex_beamer_based_pdf_presntation/</link>
      <pubDate>Fri, 18 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/latex_beamer_based_pdf_presntation/</guid>
      <description>###Latexbeamerbasedpdfpresntation###
 For creating a presentation with lot of images in it, a latex based presentation was preferred It is easier than ppt with tight control of formating and content referencing The presentation is based on this note and chosen Berlin theme within beamer&amp;rsquo;s n number of variety themes. Main problem faced was bibtex based referencing and it couldn&amp;rsquo;t be solved and used normal superscript referencing. Another problem faced was box and its colour choosing, it was solved follwoing this wonderful tutorial  </description>
    </item>
    
    <item>
      <title>Workflow docear mendely Python</title>
      <link>/working-notes/2014/wn_2014-07/workflow_docear_mendely_python/</link>
      <pubDate>Fri, 18 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/workflow_docear_mendely_python/</guid>
      <description>creating a script to select pdf files from a bunch of folders, subfolders, html files create folders with only pdf files the code is ``` import os count = 0 d=[] f=[] for (dirname, dirs, files) in os.walk(&amp;lsquo;/home/swl-sacon-dst/Documents/GISE2013/LAB/labnotes/&amp;lsquo;): #sepecifiying the directory to search for pdf files from for filename in files: if filename.endswith(&amp;lsquo;.pdf&amp;rsquo;) : thefile = os.path.join(dirname,filename) dire = os.path.dirname(thefile) f.append(thefile) d.append(dire) #makung a list of ifle names and folders for copying #using list comphrehsnsion to change the folder path from dc = [word.</description>
    </item>
    
    <item>
      <title>Serial port problem lbm1 knmr</title>
      <link>/working-notes/2014/wn_2014-07/serial_port_problem_lbm1_knmr/</link>
      <pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/serial_port_problem_lbm1_knmr/</guid>
      <description>##Serialport and other problem, lbm1 knmr monitor## - lbm1 knmr monitor is setup with three threepin plugs extension box serially wired, it is connected with another serially connected extension box with two pin plug. - In which three plugs, one plug, dylos air quality monitor is connected, another 2A 7port USB hub adapter is connected and third and final pin from right to left, TP link 3G router is connected. - With the above setup, router LAN is connected with rapberry pi which is connected with 7hub USB, Dylos serial port is connected with rpi, RPi is accessed via wireless connectivity through router.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part4 cbe domain run</title>
      <link>/working-notes/2014/wn_2014-07/working_with_wrf_chem_part4_cbe_domain_run/</link>
      <pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/working_with_wrf_chem_part4_cbe_domain_run/</guid>
      <description>##Running WRF-CHEM for Coimbatore domain## - After doing wps with wrf-chem and problem faced to run wrf.exe, recompilation then decided to run the wrf-chem for Coimbatore domain - Coimbatore domain made with four nested domain feature has been run with wrf ems with resolution starting from 100&amp;gt;27&amp;gt;9&amp;gt;3&amp;gt;1km, this was made with dwiz application in wrf ems. - However to test the domain above the Coimbatore region the output was subject for wrfncxnj.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part5 prep emis</title>
      <link>/working-notes/2014/wn_2014-07/working_with_wrf_chem_part5_prep_emis/</link>
      <pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/working_with_wrf_chem_part5_prep_emis/</guid>
      <description>##Running prepchemsrc## - based on part 1 prepchemSRC was compiled and ready to executed for emission inventory creation. - Following WRF-chem nepal tutorial and PREPCHEMSRC, README edited prep_chem_sources.inp. - then run the preogram by executing ./prep_chem_sources_RADM_WRF_FIM.exe, it ran for some steps but exited with error of Warning! ***HDF5 library version mismatched error*** saying the HDF5 version used in compiling is 1.8.8 and version for running PREPCHEMSRC is 1.8.12 - to check what hdf5 is used by prepchesrc, run a command h5dump -V which gives h5dump: Version 1.</description>
    </item>
    
    <item>
      <title>Textfile from WRFoutput pythonic</title>
      <link>/working-notes/2014/wn_2014-06/textfile_from_wrfoutput_pythonic/</link>
      <pubDate>Sat, 28 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/textfile_from_wrfoutput_pythonic/</guid>
      <description>##TextfilefromWRFoutputpythonic## - to convert wrf output into text file for the value of a specified lat long - Using the script WrfncXnj.py, convert wrf output in cf abiding nc file - the steps are as follows - cp wrfoutd04* /home/hoopoe/wrfncxnj-0.1_r2120/ - to get the list of available files in a directory
import os a=[] for file in os.listdir(&amp;quot;/home/hoopoe/wrfncxnj-0.1_r2120/&amp;quot;): if file.startswith(&amp;quot;wrfout_d04&amp;quot;): a.append(file)   to run a external python script from another python script,  import subprocess a= wrfout_d04_2014-06-11_00:00:00 subprocess.</description>
    </item>
    
    <item>
      <title>Latex into HTML</title>
      <link>/working-notes/2014/wn_2014-06/latex_into_html/</link>
      <pubDate>Fri, 27 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/latex_into_html/</guid>
      <description>##Latex into HTML## - For a workflow with latex and peer editing, latex in PDF has to be converted into some editieable formate can be used in texteditor - based on this, latex tex files can be directly converted into html without going for pdf. - have to use this command htlatex mydocument.tex, but asks to install the progrma sudo apt-get install tex4ht, so install tex4ht - then exectute the command of htlatex mydocument.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part1 PrepChemSrc</title>
      <link>/working-notes/2014/wn_2014-06/working_with_wrf_chem_part1_prepchemsrc/</link>
      <pubDate>Thu, 26 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/working_with_wrf_chem_part1_prepchemsrc/</guid>
      <description>##working with WRF_CHEM:compiling and install prep-chem-src##
 to isntall prep-chem-src, it requiers to install HDF5,ZLIB and NETCDF based on the readme of PREPCHEMSRC as follows ```
Third Party Software Requirements  ZLIB 1.2.5(libz.a) or later distribution. You may download the software from the http://www.zlib.net/ site.
 HDF5-1.8.8(libhdf5fortran.a, libhdf5hlfortran.a) or later distribution. You may download the software from http://www.hdfgroup.org/HDF5/release/obtain5.html.
 NetCDF 4.1.1 (libnetcdf.a). You may download the software from http://www.unidata.ucar.edu/downloads/netcdf/netcdf-4_0_1/index.jsp</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part2 WPS</title>
      <link>/working-notes/2014/wn_2014-06/working_with_wrf_chem_part2_wps/</link>
      <pubDate>Thu, 26 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/working_with_wrf_chem_part2_wps/</guid>
      <description>##working with WRF_CHEM:Running WPS##
The steps for running WPS as per m2lab tutorial ###Geogrid### - create namelist and geogrid - copy wps folder and edit namelist for domain - use ncl program to view domain - reflat and reflon center point of domain, ewe, esn, number of point in x and y direction - edit geog file location in name list and geogrid outptut location - then run geogrid.exe, run geogrid after editing the name list as .</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part3 compile wrf exe</title>
      <link>/working-notes/2014/wn_2014-06/working_with_wrf_chem_part3_compile_wrf_exe/</link>
      <pubDate>Thu, 26 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/working_with_wrf_chem_part3_compile_wrf_exe/</guid>
      <description>##WorkingwithWRFCHEMpart3-compilewrfexe## - for running wrf.exe after wps, it is found that no link for wrf.exe and real.exe was made during the compilation of wrf-chem. - so now need of fresh compilation of wrf alone to make it work - the compilation involves only setting environment, installing dependent packages and running,  ./configure then ./compile em_real and ./compile emi_conv - the first step of .configure ended with error saying can&amp;rsquo;t find netcdf in the path, the path specified is wrong one.</description>
    </item>
    
    <item>
      <title>Installing GDAL with Python in Ubunut1204</title>
      <link>/working-notes/2014/wn_2014-06/installing_gdal_with_python_in_ubunut1204/</link>
      <pubDate>Wed, 18 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/installing_gdal_with_python_in_ubunut1204/</guid>
      <description>##InstallingGDALwith_Python## - to make geotiff file from wrf output, python script requiers gdal such as for this from osgeo import gdal and from osgeo import osr - to install it in Ubuntu 12.04, it reuiers to install gdal-bin, libgdal1 and ptthon-gdal, after installing running python and importing crahes python with this message segmentation error(core dumped) - first it was detected of version difference, so installed with deb packages avaolable for the version for 1.</description>
    </item>
    
    <item>
      <title>Reprojecting SHAPE file python</title>
      <link>/working-notes/2014/wn_2014-06/reprojecting_shape_file_python/</link>
      <pubDate>Wed, 18 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/reprojecting_shape_file_python/</guid>
      <description>##reporjecting shape file using python##
 WRF output netcdf(nc) or wrfncx.py output nc files are converted into geotiff(tiff) file with coustom projections, see the codes for nc to tiff conversion.
from osgeo import gdal from osgeo import osr import numpy import numpy.ma as ma datafile = &#39;ZZZG3wrfout_psl.nc&#39; proj_out = osr.SpatialReference() proj_out.SetMercator(0.0, 115.02, 0.98931892612652, 0.0, 0.0) ds_in = gdal.Open(datafile) #subdatasets = ds_in.GetSubDatasets() #variables = [] #for subdataset in subdatasets: # variables.</description>
    </item>
    
    <item>
      <title>Shapefile edit QGIS</title>
      <link>/working-notes/2014/wn_2014-06/shapefile_edit_qgis/</link>
      <pubDate>Wed, 18 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/shapefile_edit_qgis/</guid>
      <description>##to edit shape file in qgis## - to edit multipart shape file in qgis into a one outline map - used qgis multipart into single feature by selected attributes - it conveert shape file with multiple attirbutes feature-multiple rows into single rows - this can be edited to delete any redisuals in the operaation, used delete ring in qgis for that</description>
    </item>
    
    <item>
      <title>Installing R and Openair in Ubuntu1204</title>
      <link>/working-notes/2014/wn_2014-06/installing_r_and_openair_in_ubuntu1204/</link>
      <pubDate>Tue, 17 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/installing_r_and_openair_in_ubuntu1204/</guid>
      <description>##InstallingRandOpenairin_Ubuntu1204## - installation of R in ubuntu 12.04, there are deb packages available for R from its source page here - But after installation it shows error in installing openair package, and depndancy chain goes from 2 to 10 and much more package with version mismacth problem, starting from lattice saying Error: package ‘lattice’ was built before R 3.0.0: please re-install it - Even calling package by ```library(lattice&amp;rsquo;) thoughed error - So installation in apt-get library might be problem solving, followed these</description>
    </item>
    
    <item>
      <title>data editing with pandas</title>
      <link>/working-notes/2014/wn_2014-06/data_editing_with_pandas/</link>
      <pubDate>Tue, 17 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/data_editing_with_pandas/</guid>
      <description>##data editing with pandas## - to import data into python
import pandas as pd data=pd.read_csv(&#39;value.txt&#39;)   to view dat ahead or sepcified rows  data.iloc[:5, :4]  to sort data based on specific oclumn, here data column  d2=d1.sort([&#39;observation_time&#39;])  to make a dataetime column recognized as date column in padnas dataframe  d1[&#39;SamplingDate&#39;] = pd.to_datetime(d1[&#39;SamplingDate&#39;])  to remove NaN valued rows in any of the columns of dataframe  d1=data.</description>
    </item>
    
    <item>
      <title>editing WRF logfile Python</title>
      <link>/working-notes/2014/wn_2014-06/editing_wrf_logfile_python/</link>
      <pubDate>Sun, 08 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/editing_wrf_logfile_python/</guid>
      <description>##editingWRFlogfile_Python and pandas##
 to get sum seconds wrf is running, the wrf.out.log was imported into python to sum the elapsed seconds in each domains to get the log file into wrf bf = open(&#39;run_wrfm.log&#39;, &#39;r&#39;) to read each lines in the file bf_lines=bf.readlines() to make the lines into list array and select only the list with particular words in it, here the word &amp;ldquo;Timing for Writing&amp;rdquo;  f=[] for line in bf_lines: if &#39;Timing for Writing&#39; in line: f.</description>
    </item>
    
    <item>
      <title>Bewolf cluster for WRF EMS</title>
      <link>/working-notes/2014/wn_2014-06/bewolf_cluster_for_wrf_ems/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/bewolf_cluster_for_wrf_ems/</guid>
      <description>##BewolfclusterforWRFEMS##
 The four domain running of WRF EMS with reoslution startng from 27km, 9km, 3km to 1km gives error of exit with status of -9,
this showing that it would because of outof memeory status([more from this])(http://forum.wrfforum.com/viewtopic.php?f=6&amp;amp;t=407), this gives motivation further for cluster running of wrf ems passwordless SSH is not suffecieidnt for cluster running of WRF EMS, as specfied in run_ncpus.conf, the created cluster has to be checked with netcheck script given with WRF-EMS, it is in strc folder hint So while running netcheck with passwordless ssh, it gives error that ssh localhostname hostname is not doing pasword less ssh, in the begning, this error seems to be ridiculus, how and what!</description>
    </item>
    
    <item>
      <title>Localinstall WRFEMS</title>
      <link>/working-notes/2014/wn_2014-06/localinstall_wrfems/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/localinstall_wrfems/</guid>
      <description>##localinstall_WRFEMS##
 WRF ems can be installed locally using the file inside the folder releases based on the WRF ems manual, chapter2, installaation from local netwrok, use ./ems_install.pl --install --repodir &#39;releases folder content&#39; After this the ems will be installed, for running the user&amp;rsquo;s default shell has to be set into tcsh, by following this as follows echo $SHELL if it gives /bin/bash chnage into tcsh by chsh -s /bin/tcsh, still after this it was showing the terminal without any $ or username.</description>
    </item>
    
    <item>
      <title>WRF EMS install and running in IBM X3100 M4</title>
      <link>/working-notes/2014/wn_2014-06/wrf_ems_install_and_running_in_ibm_x3100_m4/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/wrf_ems_install_and_running_in_ibm_x3100_m4/</guid>
      <description>##WRFEMSinstallandrunninginIBMX3100M4##
 Working in HP i5 system with quad core processor and 8GB memory, memory limit error was getting for 1km resolution domains. So made a try to make a cluster with another HP laptop with same configuration, this step also returen EXIT file9, memeory limit error. changed domain to make it 3 km resoliotn and ran in cluster steup and it took 1 hour for four doamins from 81km to 3km at 3 hour interval, so for 48 hour simurlaiotn it would be taking a 16 hour model running.</description>
    </item>
    
    <item>
      <title>WRF EMS log</title>
      <link>/working-notes/2014/wn_2014-06/wrf_ems_log/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/wrf_ems_log/</guid>
      <description>##WRFEMSlog##
 WRF ems instaled through local file in laptop WRF ems was installed by online and script in server wrf ems was installed by online and script in server Four netsed domain is not running in desktop saying error code 9 during wrf real.exe  ###four nested domain run of wrf ems above trivandrum area###
 the domain made for trivandrum tvm-gfs4, aftre running a test, it gives hostory output for eavery 30 minitres and took some four hours to complete To check the domain is covering the trivandrum districts, after converting the wrf ouput into netcdf using wrfncxnj.</description>
    </item>
    
    <item>
      <title>running WRF EMS</title>
      <link>/working-notes/2014/wn_2014-06/running_wrf_ems/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/running_wrf_ems/</guid>
      <description>##Running WRF EMS## - WRF EMS is a pearl scripted implementation of weather research and forecast (WRF) model. By user easy scripting implementations, it hides the complex compiling and running steps in WRF model for operational and research purposes. - It&amp;rsquo;s installation is by running a perl script provided by mail request, the installation size goes around 22 GB and so being a long processes. - The foremost step in model execution in creation of model domain, for high resolution forecasting of weather in Coimbatore, has to make a four nested domain.</description>
    </item>
    
    <item>
      <title>python_querying_and_editing_json</title>
      <link>/working-notes/2014/wn_2014-05/python_querying_and_editing_json/</link>
      <pubDate>Wed, 28 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/python_querying_and_editing_json/</guid>
      <description>##pythonqueryingandeditingjson## - For most of the works related with this and this involves editing and querying of json and its formates such as GeoJson and Topojson - In cbe-air web application, topjson is going to act as map element and its editing is required for real time map generation and for map styling - In node.js based web application for visualizing model output, netcdf output from WRF has to converted into geojson and made similar with the earth wind data formate.</description>
    </item>
    
    <item>
      <title>Merge geojsons into one</title>
      <link>/working-notes/2014/wn_2014-05/merge_geojsons_into_one/</link>
      <pubDate>Sat, 24 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/merge_geojsons_into_one/</guid>
      <description>##Mergegeojsonsinto_one## - For converting qs into cbe-air, map is rendered using geojson rendering capability of github - the marker was easiliy made into geojson from qgis and org2ogr as a shape file. based on this under section &amp;ldquo;getting map data&amp;rdquo;, the command is
ogr2ogr -f GeoJSON point.json point.shp //and adding this script line in html &amp;lt;script src=&amp;quot;https://embed.github.com/view/geojson/saconswl/cbeair/gh-pages/cbe-s.json?height=530&amp;amp;width=1300&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; //under div map   To include coimbatore city limits along with point marker, adding another script line of github embed renderes another map!</description>
    </item>
    
    <item>
      <title>Node js withAIRwindandEarthwind</title>
      <link>/working-notes/2014/wn_2014-05/node_js_withairwindandearthwind/</link>
      <pubDate>Fri, 23 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/node_js_withairwindandearthwind/</guid>
      <description>##Node.jswithairwindandearthwindintocbe-air##
 for addressing objective2 of web processing service for real time air pollution model, the implementation with node.jsearth or airis planned. These are perfect match for this objective in showing the grandioseness of air circulation and how air pollution effect this grandioseness in real time animation of wind As a node.js web application, node has to installed as specified in the project&amp;rsquo;s readme. for node installation followed this wonderful tutorial, it basically involves and checking with node -v and npm -v  apt-get install python-software-properties apt-add-repository ppa:chris-lea/node.</description>
    </item>
    
    <item>
      <title>json data into SQLinsert with python</title>
      <link>/working-notes/2014/wn_2014-05/json_data_into_sqlinsert_with_python/</link>
      <pubDate>Fri, 23 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/json_data_into_sqlinsert_with_python/</guid>
      <description>##jsondataintoSQLinsertwith python## - to start with json data in python and loop over its elements
import json json_data=open(&#39;data.json&#39;) data=json.load(json_data) a=data[0][&#39;samples&#39;] for rs in a: print rs[&#39;wind&#39;]   to join two list as a column in python  to join two list inpython for c1, c2 in zip(de, c): print &amp;quot;%-9s %s&amp;quot; % (c1, c2)  to append loop items into a array  c=[] for rs in a: c.append(rs[&#39;wind&#39;])  to remove u from list elemnt  de=[] for x in d: de.</description>
    </item>
    
    <item>
      <title>Querying netcdf with python_kdtree</title>
      <link>/working-notes/2014/wn_2014-05/querying_netcdf_with_python_kdtree/</link>
      <pubDate>Wed, 21 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/querying_netcdf_with_python_kdtree/</guid>
      <description>##QueryingnetcdfwithPYTHONKDTREE## - To query a netcdf with latitude and longitude is required for objective three, in which user pointed lat long, revived as SMS from Android app has to parsed and find its model and nearest dylos monitoring station to send replay. - there is very useful tutorial on this with elaboration on different implementations advantages - In which most advanced querying was based on KDtree, this implementation was used to query netcdf generated from WRF model the code is as follows</description>
    </item>
    
    <item>
      <title>Installing netcdf python in Ubuntu1204</title>
      <link>/working-notes/2014/wn_2014-05/installing_netcdf_python_in_ubuntu1204/</link>
      <pubDate>Tue, 20 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/installing_netcdf_python_in_ubuntu1204/</guid>
      <description>##InstallingnetcdfpythoninUbuntu12.04##
 to install netcdf-python requiers HDF, based on this HDF isntallation from source got failed, used synaptic package manager to install HDF5 Downloaded netcdf-python, extracted and run python setup.py install Failed, saying netcdf is not found in usr/ So follwoed this, downloaded version of netcdf-4.0.1, placed in /usr/local cd into netcdf-4.0.1, and run the code LDFLAGS=-L/usr/local/lib CPPFLAGS=-I/usr/local/include ./configure --enable-netcdf-4 --enable-dap --enable-shared --prefix=/usr/local then sudo make and then sudo make install seems got installed, then went into netcdf4-python as given in this and run sduo python setup.</description>
    </item>
    
    <item>
      <title>converting WRF ouput netcdf into json</title>
      <link>/working-notes/2014/wn_2014-05/converting_wrf_ouput_netcdf_into_json/</link>
      <pubDate>Mon, 19 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/converting_wrf_ouput_netcdf_into_json/</guid>
      <description>##convertingWRFouputnetcdfinto_json## - tried with grib2json for converting netcdf into json 1 - for this netcdf has to converted into grib2 - for converting into grib2, python based iris is useful 2, but only work with cf compliant netcdf - WRF output in netcdf is not a cf compliant - So has to use a tool which convert WRF netcdf into CF compliant - Wrfncxnj.py tool 3 exactly do this with more functions such as extraction of variables etc - Wrfncxnj.</description>
    </item>
    
    <item>
      <title>gammu MYSQL</title>
      <link>/working-notes/2014/wn_2014-05/gammu_mysql/</link>
      <pubDate>Fri, 16 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/gammu_mysql/</guid>
      <description>gammu_mysql
 followed this,this the running, file based gammu-smsd backend configuration file is as follows    nano /etc/gammu-smsdrc # Configuration file for Gammu SMS Daemon # Gammu library configuration, see gammurc(5) [gammu] # Please configure this! port = /dev/ttyUSB2 model = connection = at synchronizetime = yes #logfile = /home/debian/gammulog #logformat = textalldate use_locking = gammuloc = # SMSD configuration, see gammu-smsdrc(5) [smsd] #debuglevel = 255 #Service = sql #Driver = sqlite3 #database = kalkun.</description>
    </item>
    
    <item>
      <title>QualitySCHU to cbe air</title>
      <link>/working-notes/2014/wn_2014-05/qualityschu_to_cbe_air/</link>
      <pubDate>Thu, 15 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/qualityschu_to_cbe_air/</guid>
      <description>##QualitySCHU to cbe-air##
 qualityschu(qs) is a web application coupled with istsos sensor web, it&amp;rsquo;s elegant, simple html with javascript design makes easy to work with it and being educative. It is base code to build particulate matter air quality monitors web application in Coimbatore named cbeair web application. The app&amp;rsquo;s main difference with source(qs) would be its ability to work with github pages it is basically a html file with map and menu javascript files, main functions it provide is map view, table, chart view and download functions all are coupled with istSOS.</description>
    </item>
    
    <item>
      <title>postgresql</title>
      <link>/working-notes/2014/wn_2014-05/postgresql/</link>
      <pubDate>Mon, 12 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/postgresql/</guid>
      <description>#to edit a table in postgresql
 to list databases  \list  to list tables in a database based on this  SELECT table_schema,table_name FROM information_schema.tables ORDER BY table_schema,table_name;  to view details about a table  \d table.name  to edit a column in a table  ALTER TABLE cbed.measures ALTER COLUMN val_msr TYPE numeric(14,6);   </description>
    </item>
    
    <item>
      <title>OnPython</title>
      <link>/working-notes/2014/wn_2014-05/onpython/</link>
      <pubDate>Sat, 10 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/onpython/</guid>
      <description>##Python with sqlite, istsos dat formate and time downscaling#
###WIth sqlite - to convert a list, for example cur.fetch from database like sqlite,
[&#39;2014-04-30T10:25,2797,147&#39;, &#39;2014-04-30T10:27,2639,174&#39;, &#39;2014-04-30T10:29,2645,158&#39;, &#39;2014-04-30T10:31,2676,149&#39;]   use print &amp;quot;\n&amp;quot;.join(b) based on this gives  &amp;quot;2014-04-30T10:25,2797,147 2014-04-30T10:27,2639,174 2014-04-30T10:29,2645,158 2014-04-30T10:31,2676,149&amp;quot;  to remove double quotes from above to write into a .DAT, tryed almost two hours then find out that the used mehtod will not do this. the full code is as follows with uncommented lines are failed attmepts.</description>
    </item>
    
    <item>
      <title>pandas dataframe into LATEX PDF</title>
      <link>/working-notes/2014/wn_2014-05/pandas_dataframe_into_latex_pdf/</link>
      <pubDate>Sat, 10 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/pandas_dataframe_into_latex_pdf/</guid>
      <description>##pandasdataframeinto_PDF## - Huge dataframes whihc goes for multiple A4 pages landscape is difficult to make in excel - Alternative is use python pandas and convert the pandas dataframe into pdf thorugh latex or html, latex is promising for just printing - Basically from ([1])(http://stackoverflow.com/questions/14380371/export-a-latex-table-from-pandas-dataframe) for convert dataframe into tex and this for converting tex into landscape pdf document - the python script to make table text is as follows, It is mostly from 3rd answer [1] and bold column heading write python trick from</description>
    </item>
    
    <item>
      <title>Cross Origin Resource Sharing</title>
      <link>/working-notes/2014/wn_2014-05/cross_origin_resource_sharing/</link>
      <pubDate>Wed, 30 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/cross_origin_resource_sharing/</guid>
      <description>#Cross-OriginResourceSharing#
 WHile working with qs into cbe-air, in populating table and charts with istsos json data, stucked with a error the error was not informative in firebug.firefox, a simple correct repsonse of 200 with red fonts and also not giving any console.log in JS after trying with changing jquery version, different old edited version of qs, experimenting with differneet json url no clue was found, finally tryed to run the aptana server in chromium and its devloper tools option gives verbal error of No &#39;Access-Control-Allow-Origin&#39; header is present on the requested resource.</description>
    </item>
    
    <item>
      <title>File system based SOS using github AJAX</title>
      <link>/working-notes/2014/wn_2014-04/file_system_based_sos_using_github_ajax/</link>
      <pubDate>Thu, 24 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/file_system_based_sos_using_github_ajax/</guid>
      <description>FilesystembasedSOSusinggithubAJAX Found a article discussing similar to this line.
https://www.academia.edu/1502083/A_flexible_geospatial_sensor_observation_service_for_diverse_sensor_data_based_on
Monitoring real-time environmental information using Web 2.0 and GIServices technology
Integrating Sensor Webs with Modeling and Data-assimilation Applications: An SOA Implementation
see
http://buyya.com/papers/SensorWebChapter.pdf
http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=4526452&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4526452
simple SOS https://github.com/jcu-eresearch/python-simplesos</description>
    </item>
    
    <item>
      <title>IRIS install</title>
      <link>/working-notes/2014/wn_2014-04/iris_install/</link>
      <pubDate>Thu, 24 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/iris_install/</guid>
      <description>Installing_IRIS
 IRIS is a python tool for working with netcdf and grib files. It is installed to convert WRF output in netCDF to grib2 formate, which is need for grib2json tool. IRIS is dependent of large number of python scientific libraries. Most of the libraries are python and it can be isntalled though  pip install libarry  based on the failure report It&amp;rsquo;s installation further gets erroneous due to unavailability of netcdf, HDF5, netcdf-python packages.</description>
    </item>
    
    <item>
      <title>package using maven</title>
      <link>/working-notes/2014/wn_2014-04/package_using_maven/</link>
      <pubDate>Thu, 24 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/package_using_maven/</guid>
      <description>PackageusingMavenforGRIB2JSON
 Java based tools such as grib2json has to compiled with maven latest maven is installed, following as per this note (https://github.com/saconswl/Real_time_air_pollution_Mod_Proj-2013-2014/blob/home/working_notes/wn_2013-10/Installing_maven_in_Ubuntu_12.04.md) the tool grib2json requieres java 1.7 made huge search due to error while trying for  mvn package  it gives error of  Error: JAVA_HOME is not defined correctly. We cannot execute ”/usr/lib/jvm/jdk1.7.0”/bin/java   later found that, the system doesn&amp;rsquo;t containing jdk1.7.0 installed jdk1.7.0 follwoing (http://askubuntu.</description>
    </item>
    
    <item>
      <title>Gammu smsd shared memeory error for Huwaei E303F</title>
      <link>/working-notes/2014/wn_2014-04/gammu_smsd_shared_memeory_error_for_huwaei_e303f/</link>
      <pubDate>Mon, 21 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/gammu_smsd_shared_memeory_error_for_huwaei_e303f/</guid>
      <description>I am running gammu and gammu-smsd backed by Mysql in Ubuntu 12.04, All these setup was running with out error using Huwaei E173 data card. But upgraded model of this, Huwaei E303F, working fine with gammu, but starting gammu-smsd collapsing gammu. for example gammu &amp;ndash;identify says phone not connected
gammurc ~/.gammurc  port = /dev/ttyUSB0 model = auto connection = at synchronizetime = yes logfile = /home/user/gammu.log logformat = textalldate use_locking = gammuloc =</description>
    </item>
    
    <item>
      <title>Shared memory ERROR MORE</title>
      <link>/working-notes/2014/wn_2014-04/shared_memory_error_more/</link>
      <pubDate>Mon, 21 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/shared_memory_error_more/</guid>
      <description>More Insight into shared memory error -Was working on Beagle bone balck with Huwaei E173 -gammu &amp;ndash;identify -tail -f /home/debian/gammu-smsd.log - gammu-smsd -d - /etc/init.d/gammu-smsd restart - top - kill -9 2383 - ipcs -a - ipcrm -m 32768</description>
    </item>
    
    <item>
      <title>usbreset Program</title>
      <link>/working-notes/2014/wn_2014-04/usbreset_program/</link>
      <pubDate>Mon, 21 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/usbreset_program/</guid>
      <description>USBRESETPROGRAM From this ask ubuntu answer. to reset the USB data card
 saving this code as usbreset.c  /* usbreset -- send a USB port reset to a USB device */ #include &amp;lt;stdio.h&amp;gt; #include &amp;lt;unistd.h&amp;gt; #include &amp;lt;fcntl.h&amp;gt; #include &amp;lt;errno.h&amp;gt; #include &amp;lt;sys/ioctl.h&amp;gt; #include &amp;lt;linux/usbdevice_fs.h&amp;gt; int main(int argc, char **argv) { const char *filename; int fd; int rc; if (argc != 2) { fprintf(stderr, &amp;quot;Usage: usbreset device-filename\n&amp;quot;); return 1; } filename = argv[1]; fd = open(filename, O_WRONLY); if (fd &amp;lt; 0) { perror(&amp;quot;Error opening output file&amp;quot;); return 1; } printf(&amp;quot;Resetting USB device %s\n&amp;quot;, filename); rc = ioctl(fd, USBDEVFS_RESET, 0); if (rc &amp;lt; 0) { perror(&amp;quot;Error in ioctl&amp;quot;); return 1; } printf(&amp;quot;Reset successful\n&amp;quot;); close(fd); return 0; }   compile code using cc usbreset.</description>
    </item>
    
    <item>
      <title>Compileling WRF CHEM</title>
      <link>/working-notes/2014/wn_2014-04/compileling_wrf_chem/</link>
      <pubDate>Fri, 18 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/compileling_wrf_chem/</guid>
      <description>Compiling WRF-CHEM
Based on this Most exhaustive tutorial
 in WRFV3 folder, (the &amp;lsquo;chem&amp;rsquo; folder has to copied inside of this folder) entering  $ ./configure checking for perl5... no checking for perl... found /usr/bin/perl (perl) ** WARNING: No path to NETCDF and environment variable NETCDF not set. ** would you like me to try to fix? [y] y Enter full path to NetCDF include directory on your system /usr/include Enter full path to NetCDF library directory on your system /usr/lib created new .</description>
    </item>
    
    <item>
      <title>CSV edit by pandas</title>
      <link>/working-notes/2014/wn_2014-04/csv_edit_by_pandas/</link>
      <pubDate>Thu, 10 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/csv_edit_by_pandas/</guid>
      <description>Modfying csv files using pandas, python
 to import csv file into python  import pandas data = pd.read_csv(&#39;/home/hoopoe/Documents/Real_time_air_pollution_Mod_Proj-2013-2014/obj2/237.csv&#39;)   to query the specfic column in data frame  data[&#39;SamplingDate&#39;]   to specifiy the column as datetime formate column for pandas  data[&#39;SamplingDate&#39;] = pd.to_datetime(data[&#39;SamplingDate&#39;])  to avoid date and month mismatch specify the formate of date as
data[&#39;SamplingDate&#39;] = pd.to_datetime(data[&#39;SamplingDate&#39;],format=&#39;%d/%m/%Y&#39;)   to sort the data based on date column descending  dataso=data.</description>
    </item>
    
    <item>
      <title>Dylos monitor setup full with log</title>
      <link>/working-notes/2014/wn_2014-04/dylos_monitor_setup_full_with_log/</link>
      <pubDate>Thu, 10 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/dylos_monitor_setup_full_with_log/</guid>
      <description>Dylosmonitorsetupfullwith_log Following http://hintshop.ludvig.co.nz/show/persistent-names-usb-serial-devices/ and UdevrulesforUSB&amp;rsquo;sattachedtoRPi
create a udev rule for RPI
edited the file /etc/udev/rules.d
using command
sudo nano /etc/udev/rules.d/90-phone.rules
and add follwoing lines to giving persistant name for USB data card (HUWAEI E303F) and USB to serial cable for Dylos monitor.
KERNEL==&amp;ldquo;ttyUSB&amp;rdquo;, ATTRS{idVendor}==&amp;ldquo;12d1&amp;rdquo;, ATTRS{idProduct}==&amp;ldquo;1506&amp;rdquo;, NAME=&amp;ldquo;phone&amp;rdquo;, MODE=&amp;ldquo;0666&amp;rdquo;,SYMLINK+=&amp;ldquo;mobile&amp;rdquo; KERNEL==&amp;ldquo;ttyUSB&amp;rdquo;, ATTRS{idVendor}==&amp;ldquo;067b&amp;rdquo;, ATTRS{idProduct}==&amp;ldquo;2303&amp;rdquo;, NAME=&amp;ldquo;dylos&amp;rdquo;, MODE=&amp;ldquo;0666&amp;rdquo;,SYMLINK+=&amp;ldquo;dylos&amp;rdquo;
by this, while connecting these two devices, the folder /dev shows the files for USB data card and USB to serial cable for Dylos monitor.</description>
    </item>
    
    <item>
      <title>Sending SMS with AT and python</title>
      <link>/working-notes/2014/wn_2014-04/sending_sms_with_at_and_python/</link>
      <pubDate>Tue, 08 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/sending_sms_with_at_and_python/</guid>
      <description>SendingSMSwithATand_PYTHON
This is related with issue recorded here. Huwaei E303F is not working with Gammu, especially in RPi. So found a method to send SMS using this data card with out using gammu but using simple AT commands.
 primarly based on this 1 and this 2
 the code written for sending SMS from dylos serial is as follows
  #!/usr/bin/python import serial import time from curses import ascii import sqlite3 as lite import logging logger = logging.</description>
    </item>
    
    <item>
      <title>HYSPLIT compile</title>
      <link>/working-notes/2014/wn_2014-04/hysplit_compile/</link>
      <pubDate>Fri, 04 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/hysplit_compile/</guid>
      <description>#HYSPLIT_compile
Compiling HYSPLIT in Ubuntu 12.04 was hurdled by a error related with netcdf.
 Installation of netcdf is through compiling and package installation from synaptic package manager. Synaptic and latest source download compilation gives error of &amp;ldquo;no netcdf.inc&amp;rdquo; in hysplit compile. It is due to a fortran binding lapse in latest verision. So used a old version of the netcdf 3.6.3 and compiled following this&amp;ndash; http://code.google.com/p/netcdf4-python/wiki/UbuntuInstall has to give sudo in make and make install, with the first comment disable-shared LDFLAGS=-L/usr/local/lib CPPFLAGS=-I/usr/local/include .</description>
    </item>
    
    <item>
      <title>Sending SMS with Beagle bone black</title>
      <link>/working-notes/2014/wn_2014-03/sending_sms_with_beagle_bone_black/</link>
      <pubDate>Tue, 25 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-03/sending_sms_with_beagle_bone_black/</guid>
      <description>Sending SMS in Beagle bone black
It is an angstrom linux BBB, first connected to Ubuntu lap using usb wire from BBB, then a USB was connected to the powered USB hub, in which a serial USB connector and a Huwaei e 173 data (This was tested for Huwaei E303F also, it worked) card was connected. * ssh&amp;rsquo;s into it using ssh 192.168.7.2 -l root and password blank (a enter) * Now lsusb showing Huwaei with modem and serial usb.</description>
    </item>
    
    <item>
      <title>gammu and mysql</title>
      <link>/working-notes/2014/wn_2014-03/gammu_and_mysql/</link>
      <pubDate>Tue, 25 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-03/gammu_and_mysql/</guid>
      <description>gammuandmysql</description>
    </item>
    
    <item>
      <title>usb modeswitch on reboot</title>
      <link>/working-notes/2014/wn_2014-03/usb_modeswitch_on_reboot/</link>
      <pubDate>Tue, 25 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-03/usb_modeswitch_on_reboot/</guid>
      <description>usb_modeswitch.conf</description>
    </item>
    
    <item>
      <title>EMS WRF</title>
      <link>/working-notes/2014/wn_2014-03/ems_wrf/</link>
      <pubDate>Thu, 06 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-03/ems_wrf/</guid>
      <description>EMS WRF OF COIMBATORE REGION USING 4 DOMAINS :-
Five simple steps required for the execution of EMS (Environmental modeling system) WRF.
Step 1: Create a computational domain with the Domain Wizard(DW) GUI After the successful installation of EMS follow these steps,
a) Start the DW by executing the command &amp;ldquo;dwiz&amp;rdquo; in terminal . b) DW window should appear and gives you an opportunity to select from creating a new domain or modifying existing domain.</description>
    </item>
    
    <item>
      <title>Installing 55north SOS with tomcat7</title>
      <link>/working-notes/2014/wn_2014-02/installing-55north-sos-with-tomcat7/</link>
      <pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-02/installing-55north-sos-with-tomcat7/</guid>
      <description>Installed tomcat 7 using http://askubuntu.com/questions/339169/how-to-install tomcat-7-0-42-on-ubuntu-12-04-3-lts to find tomcat is running followed by http://stackoverflow.com/questions/3944157/is-tomcat-running netstat -a | grep 8080 ps -ef | grep tomcat
both commands gives some bulge result and responses if it is working. if it is not working the command used to start tomcat7 is from the installation page sudo $CATALINA_HOME/bin/startup.sh Installing the SOS war following tomcat manager and war file upload to upload the data into istsos used this command following all its installation documentation.</description>
    </item>
    
    <item>
      <title>Python for fetching Mysql table</title>
      <link>/working-notes/2014/wn_2014-02/python_for_fetching_mysql_table/</link>
      <pubDate>Tue, 18 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-02/python_for_fetching_mysql_table/</guid>
      <description>Converting the MySQL backed SMS gateway data into (Sensor Observation Service (SOS) import formate. COCEMS_lbm are sending the data every 15 minutes through SMS and received by server-side data card and Gammu SMS gateway backed by MySQL, the data is in inbox table. Following python script do the job. It took a long time in understanding the difference between array, and list objects in python, a clear understanding of this would not cost this much time to solve the error.</description>
    </item>
    
    <item>
      <title>wind power forecasting map</title>
      <link>/working-notes/2014/wn_2014-02/wind-power-forecasting-map/</link>
      <pubDate>Wed, 05 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-02/wind-power-forecasting-map/</guid>
      <description>Problem statement  In Tamil Nadu, electricity is significantly relying on wind power based renewable energy source. If much wind is there, there will be minimal power cuts and vice versa. The second-tier urban area like Coimbatore, this dependence is much visible, and so one of the simple predictors of long power cuts is lack of adequate wind power in the nearby wind park area for example. On the other hand operators of windmills or power transmission sector, if they know much early about the forecast of wind power in their area, they have many advantages in preparing for storing the surplus energy source or find alternatives in the situation of low wind power.</description>
    </item>
    
    <item>
      <title>python script for inserting SOS</title>
      <link>/working-notes/2014/wn_2014-01/python-script-for-inserting-sos/</link>
      <pubDate>Sat, 25 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/python-script-for-inserting-sos/</guid>
      <description>A python script to insert data into 52 NORTH SOS through HTTP POST. Save this script as python file and run the script in the terminal as python “scriptname”.py. It will insert the data and report the status as given by test client “send” button. 52 north SOS needs to run in localhost.
based on https://github.com/mpfeil/qualitySCHU/blob/master/Parser/LANUV/main.py http://stackoverflow.com/questions/16055334/post-xml-request-using-python
import urllib import httplib from xml.dom.minidom import parse, parseString target_url = “http://localhost:8080/52n-sos-webapp-4.0.0-Beta1/sos/soap” #the insert observation requests from test client 52 north SOS xml_request = “”&amp;quot; &amp;lt;sos:offering&amp;gt;test_offering_1&amp;lt;/sos:offering&amp;gt; &amp;lt;sos:observation&amp;gt; &amp;lt;om:OM_Observation gml:id=&amp;quot;o1&amp;quot;&amp;gt; &amp;lt;om:type xlink:href=&amp;quot;http://www.</description>
    </item>
    
    <item>
      <title>recover password for postgresql</title>
      <link>/working-notes/2014/wn_2014-01/recover-password-for-postgresql/</link>
      <pubDate>Sat, 25 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/recover-password-for-postgresql/</guid>
      <description>based on http://scratching.psybermonkey.net/2009/06/postgresql-how-to-reset-user-name.html
 sudo nano /etc/postgresql/9.1/main/pg_hba.conf change |local all postgres md5| to | local all postgres trust| sudo service postgresql restart now enter into the PostgreSQL using psql -U postgres ALTER USER postgres with password &amp;lsquo;new password&amp;rsquo;; then again change the pg_hba.conf as earlier and restart the PostgreSQL for invoking the password protection  </description>
    </item>
    
    <item>
      <title>Installing Munin for ubuntu server</title>
      <link>/working-notes/2014/wn_2014-01/installing-munin-for-ubuntu-server/</link>
      <pubDate>Fri, 17 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/installing-munin-for-ubuntu-server/</guid>
      <description>Based on
http://ubuntuserverguide.com/2012/08/how-to-install-and-configure-munin-on-ubuntu-server-12-04.html http://www.hashbangcode.com/blog/monitoring-performance-munin-713.html  </description>
    </item>
    
    <item>
      <title>more with SMS gateway for ubuntu 1204</title>
      <link>/working-notes/2014/wn_2014-01/more-with-sms-gateway-for-ubuntu-1204/</link>
      <pubDate>Fri, 17 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/more-with-sms-gateway-for-ubuntu-1204/</guid>
      <description>based on this  http://blog.sleeplessbeastie.eu/2012/07/16/kalkun-how-to-setup-sms-gateway-at-home/
http://back2arie.wordpress.com/2010/07/27/using-gammu-smsd-with-multiple-phone/
 problem rectified by  http://askubuntu.com/questions/211739/gammu-and-device-permissions
 steps followed are  installed
 sudo apt-get install gammu gammu-smsd sudo cp /usr/share/doc/gammu/examples/config/gammurc /etc/gammurc the gammurc file will not be there, has to do this step, instaed of running gammu-conifg is a problem and make gammurc files in home folder now run gammu &amp;ndash;identify, it gives no phone detected or some other error
 for this the config file gammurc has to edited as per like this</description>
    </item>
    
    <item>
      <title>SCREEN usage</title>
      <link>/working-notes/2014/wn_2014-01/screen-usage/</link>
      <pubDate>Thu, 16 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/screen-usage/</guid>
      <description>SCREEN is a utility for multiple session in command line Linux. * To start a screen with the name screen -S &amp;ldquo;name without mark&amp;rdquo; * To view running screen screen -r * to get into a particular screen screen -xr pid(of the screen) * to get out of the screen CTRL+A+D
Ubuntu problem atkbd.c spamming the logs. How to get rid? What is this? using this http://askubuntu.com/questions/116538/atkbd-c-spamming-the-logs-how-to-get-rid-what-is-this</description>
    </item>
    
    <item>
      <title>Secure ubuntu SERVER 1204</title>
      <link>/working-notes/2014/wn_2014-01/secure-ubuntu-server-1204/</link>
      <pubDate>Thu, 16 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/secure-ubuntu-server-1204/</guid>
      <description>For key pair login and removing the password based on
http://ubuntuforums.org/archive/index.php/t-30709.html http://blog.nas-admin.org/?p=63 http://www.thefanclub.co.za/how-to/how-install-psad-intrusion-detection-ubuntu-1204-lts-server following first one
 cd .ssh/
 ssh-keygen -t dsa
 scp iddsa.pub serverusername@IP:./iddsa.pub
 ssh into server
 cd .ssh
 touch authorized_keys2
 chmod 600 authorized_keys2
 cat ../iddsa.pub &amp;gt;&amp;gt; authorizedkeys2
 rm ../id_dsa.pub
 edited the /etc/ssh/sshd_config for #PasswordAuthentication yes &amp;gt;&amp;gt;&amp;gt; PasswordAuthentication no PermitRootLogin yes &amp;gt;&amp;gt;&amp;gt; PermitRootLogin no &amp;gt;&amp;gt;&amp;gt; DebianBanner no
 have to restart the ssh sudo /etc/init.</description>
    </item>
    
    <item>
      <title>USB DATA card for ubuntu 1204</title>
      <link>/working-notes/2014/wn_2014-01/usb_data_card_for_ubuntu_1204/</link>
      <pubDate>Thu, 16 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/usb_data_card_for_ubuntu_1204/</guid>
      <description>Do dmesg |grep tty
 if not showing anything, solved following final answer of this question
  http://ubuntuforums.org/archive/index.php/t-1853306.html
 &amp;ldquo;dmesg is a ring buffer, so if many messages are being logged, will lose the initial boot messages. Try this instead:  cd /var/log ; grep ttyUSB dmesg messages *log | more&amp;rdquo;
 now showing   grep: messagesdmesg:[ 23.391523] usb 1-1.1: GSM modem (1-port) converter now attached to ttyUSB0 No such file or directory dmesg:[ 23.</description>
    </item>
    
    <item>
      <title>removing PIN in android</title>
      <link>/working-notes/2014/wn_2014-01/removing-pin-in-android/</link>
      <pubDate>Mon, 06 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/removing-pin-in-android/</guid>
      <description>To remove the forgotten pin in android tablet, it has to be USB debugging enabled in past. The using adb shell
adb shell su cd data/data/system rm gesture.key</description>
    </item>
    
    <item>
      <title>Script for json to csv for weather underground API fetching historical data</title>
      <link>/working-notes/2013/wn_2013-12/script-for-json-to-csv-for-weather-underground-api-fetching-historical-data/</link>
      <pubDate>Mon, 16 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-12/script-for-json-to-csv-for-weather-underground-api-fetching-historical-data/</guid>
      <description>based on this https://github.com/PythonJournos/LearningPython/blob/master/tutorials/convert_json_to_csv.py a sample script  import urllib2 import json import csv outfile_path=&#39;history.csv&#39; writer = csv.writer(open(outfile_path, &#39;w&#39;)) headers = [&#39;date&#39;] writer.writerow(headers) req = urllib2.Request(&amp;quot;http://api.wunderground.com/api/YOUR_KEY/history_20131001/q/India/Coimbatore.json&amp;quot;) opener = urllib2.build_opener() f = opener.open(req) data = json.load(f) for history in data[&#39;history&#39;][&#39;observations&#39;]: row = [] row.append(str(history[&#39;date&#39;][&#39;pretty&#39;])) row.append(str(history[&#39;tempm&#39;])) writer.writerow(row)   Now the URL has to be iterated to give a range of historical data required, and most important the date range has to set.</description>
    </item>
    
    <item>
      <title>Adding font in Ubuntu 1204</title>
      <link>/working-notes/2013/wn_2013-12/adding-font-in-ubuntu-1204/</link>
      <pubDate>Wed, 04 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-12/adding-font-in-ubuntu-1204/</guid>
      <description>To add font
 move the ttf file into the folder usr/share/fonts/ttf then remove fonts cache by rm -f /usr/share/fonts/*fonts.cache-1 then create the cache again sudo fc-cache that&amp;rsquo;s it, added ttf can be seen in all the text editors.  </description>
    </item>
    
    <item>
      <title>Notes on Geoserver</title>
      <link>/working-notes/2013/wn_2013-11/notes-on-geoserver/</link>
      <pubDate>Tue, 05 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-11/notes-on-geoserver/</guid>
      <description>Started based on this link http://gis.stackexchange.com/questions/69430/best-way-to-visualize-the-postgis-raster-in-openlayers Geoserver very easy to install, followed installation based on war and Apache tomcat. For know more about rest api for Geoserver http://boundlessgeo.com/2012/10/adding-layers-to-geoserver-using-the-rest-api/ to more on the python script for automatic the MODIS visualization http://gis.stackexchange.com/questions/16515/how-to-import-a-raster-into-postgis</description>
    </item>
    
    <item>
      <title>PostGresql table edit in command line</title>
      <link>/working-notes/2013/wn_2013-11/postgresql-table-edit-in-command-line/</link>
      <pubDate>Tue, 05 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-11/postgresql-table-edit-in-command-line/</guid>
      <description>To edit the Istsos database table “measures” in PostgreSQL it is to overcome to the error in istsos import saying the Dylos reading is exceeding the digit limit of the column val_meas in the table. Normally it is easy to change column digit size through pgadmin. However, for command line, it requires following commands.
 Get access to PostgreSQL  sudo -u postres psql
 To view the database in PostgreSQL \dt</description>
    </item>
    
    <item>
      <title>SMS management web application</title>
      <link>/working-notes/2013/wn_2013-11/sms-management-web-application/</link>
      <pubDate>Tue, 05 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-11/sms-management-web-application/</guid>
      <description>Use
http://kalkun.sourceforge.net/</description>
    </item>
    
    <item>
      <title>Screen for remote head less sever</title>
      <link>/working-notes/2013/wn_2013-11/screen-for-remote-head-less-sever/</link>
      <pubDate>Tue, 05 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-11/screen-for-remote-head-less-sever/</guid>
      <description>Execute multiple programs simultaneously in the terminal, use the program screen. based on http://askubuntu.com/questions/163567/start-program-from-terminal and http://www.howtoforge.com/linux_screen
Basic running steps
 screen -r: to view any running screen If there is no running screen, by this command a new terminal will be open, and any commands can be run here, such as to get a file using sftp To get out of the screen type CTRL +A+D after this the cursor goes to the base terminal  based on http://askubuntu.</description>
    </item>
    
    <item>
      <title>Notes on selenium</title>
      <link>/working-notes/2013/wn_2013-11/notes-on-selenium/</link>
      <pubDate>Fri, 01 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-11/notes-on-selenium/</guid>
      <description>Selenium installation is based on http://www.pererikstrandberg.se/blog/index.cgi?page=InstallingSelenium</description>
    </item>
    
    <item>
      <title>Installing maven in Ubuntu 1204</title>
      <link>/working-notes/2013/wn_2013-10/installing-maven-in-ubuntu-1204/</link>
      <pubDate>Tue, 08 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-10/installing-maven-in-ubuntu-1204/</guid>
      <description>Based on links http://docs.geotools.org/latest/userguide/build/install/mvn.html http://www.mkyong.com/maven/how-to-install-maven-in-ubuntu/ http://lukieb.wordpress.com/2011/02/15/installing-maven-3-on-ubuntu-10-04-lts-server/
 Download, untar and copy maven in /usr/local/
  wget http://archive.apache.org/dist/maven/binaries/apache-maven-3.0.4-bin.tar.gz tar -zxf apache-maven-3.0.4-bin.tar.gz sudo cp -R apache-maven-3.0.4 /usr/local
 then link with bin folder  sudo ln -s /usr/local/apache-maven-3.0.4/bin/mvn /usr/bin/mvn
 Then add java home link in the .bashrc sudo nano .bashrc &amp;mdash;JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-amd64 Thats it maven is installed Test it  mvn –version
 Seems every think ok but there will be an issue while mvn install, the error of javac&amp;rdquo;: java.</description>
    </item>
    
  </channel>
</rss>