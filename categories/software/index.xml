<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Software on Nishadh KA</title>
    <link>/categories/software/</link>
    <description>Recent content in Software on Nishadh KA</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Dec 2014 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/software/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>WRF CHEM cluster</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_cluster/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_cluster/</guid>
      <description>####WRFCHEMsimluation by cluster####
#to Do 1. write the script, lazywrfchem 1. draw computational inventory of the cluster comparision experiment, and libnrary program needed 1. compile the library 1. do the experiment
#single computer HP laptop, parallell execution, three cores
Domain four: onehour=12:16 to 08:43, 8.5 hours so for 6 hours simulation= 48+24=72 hours Domain Three: one hour=01:24 to 02:24, 1 hour so for 6 hours simulation= 6 hours Domain two: onehour=9:10 to 9:23, 13 minutes so for 6 hours simulation=13minx6=1.</description>
    </item>
    
    <item>
      <title>WRF CHEM compile QN</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn/</guid>
      <description>####Compile wrf chem: QUICK NOTE####
 It is based on the NCAR wrf chem compile complete process and note on wrf chem compile by this Basic environemtn testl, passes all the tests which checks libs such as, gcc, gfortran, gcc+gfortran, perl, csh. Made sure the gcc and gfortran versions are matching, current case version is 4.8.2 for both gcc and gfortran Basic library compile MPICh and NETCDF  For setting up PATH for NETCDF</description>
    </item>
    
    <item>
      <title>WRF CHEM compile QN AWS</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_aws/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_aws/</guid>
      <description>#####Compile wrf chem in AWS: QUICK NOTE#####
 It is based on the NCAR wrf chem compile complete process and note on wrf chem compile by this Create a ubuntu trusty based image. Make a small elastic block storage 8GB, it will be permanent even if the instance is kept stop. There will be storage space in /mnt for upto 300GB based on instance, for example c3.large has it. This storage gets erased while stopping the instance but not the /home folder where the small 8GB EBS is being mounted/ Install basic required libs sudo apt-get install g++ sudo apt-get install gfortran sudo apt-get install gcc Basic environment test, make sure the test passes all good for libs checks such as, gcc, gfortran, gcc+gfortran, perl, csh.</description>
    </item>
    
    <item>
      <title>WRF CHEM compile QN CT</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_ct/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_ct/</guid>
      <description>####Compile wrf chem in CubieTruck CT: QUICK NOTE####
 It is based on the NCAR wrf chem compile complete process and note on wrf chem compile by this Loaded a ubuntu trusty based image from here. Solved the issue of no Internet in CT by editing the interfaces file as follows
 # interfaces(5) file used by ifup(8) and ifdown(8) # Include files from /etc/network/interfaces.d: source-directory /etc/network/interfaces.d auto lo eth0 iface lo inet loopback auto eth0 iface eth0 inet static address 192.</description>
    </item>
    
    <item>
      <title>WRF CHEM compile QN PL</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_pl/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_pl/</guid>
      <description>####Compile wrf chem in Parallella PL: QUICK NOTE####
 It is based on the NCAR wrf chem compile complete process and note on wrf chem compile by this Loaded a ubuntu trusty based image from here. Solved the issue of no Internet in PL by editing the interfaces file as follows and changing the files networkmanager.onf and remove net rules as stated in Parallella up and run note.
 # interfaces(5) file used by ifup(8) and ifdown(8) # Include files from /etc/network/interfaces.</description>
    </item>
    
    <item>
      <title>WRFCHEM output PM25PM10</title>
      <link>/working-notes/2014/wn_2014-11/wrfchem_output_pm25pm10/</link>
      <pubDate>Wed, 26 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/wrfchem_output_pm25pm10/</guid>
      <description>####WRF CHEM output into PM25 and PM10####
 To get various variables from WRF OUTPUT following python codes and references were used #####to get WRF output NETCDF into python##### The below code import netcdf file into python and subset the data based on Coimbatore domain of  from netCDF4 import Dataset import numpy as np wrfoutput = &#39;wrfout_d01_2014-06-05_05:00:00_D03&#39; fh = Dataset(wrfoutput, mode=&#39;r&#39;) #to view all the variables in imported wrf output netcdf vars = fh.</description>
    </item>
    
    <item>
      <title>NCL install ubunut1204</title>
      <link>/working-notes/2014/wn_2014-11/ncl_install_ubunut1204/</link>
      <pubDate>Thu, 20 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/ncl_install_ubunut1204/</guid>
      <description>http://www.ncl.ucar.edu/Download/install.shtml</description>
    </item>
    
    <item>
      <title>Plot Aerocet sample D3JS</title>
      <link>/working-notes/2014/wn_2014-11/plot_aerocet_sample_d3js/</link>
      <pubDate>Sun, 16 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/plot_aerocet_sample_d3js/</guid>
      <description>####Plot aerocet data with D3 JS scrip####
 Based on this and its source code. The meteor web application is based on one index.html file and marker visualizer based on JS drawMap.js. The index.html was edited for to remove the CHART in the line 14 to 19 contains  &amp;lt;div id=&amp;quot;year-chart&amp;quot; class=&amp;quot;chart display&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;title&amp;quot;&amp;gt;Year of Impact&amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div id=&amp;quot;mass-chart&amp;quot; class=&amp;quot;chart display&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;title&amp;quot;&amp;gt;Mass (g)&amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;   IN drawMap.js, following lines were removed ``` function renderAll(){ chart.</description>
    </item>
    
    <item>
      <title>git delete routine</title>
      <link>/working-notes/2014/wn_2014-11/git_delete_routine/</link>
      <pubDate>Sat, 15 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/git_delete_routine/</guid>
      <description>####git delete routine####
 Using git and github, to remove a file which is committed earlier is requier following steps. Even though the file will be removed with delete button option of OS. The committed files still be inside the git version control system and it will make problem in uploading it into github if it is large file size &amp;gt;100MB accidently committed. First to list the files deleted in the OS but not in git system, based on this.</description>
    </item>
    
    <item>
      <title>Excel Pandas LATEX PDF</title>
      <link>/working-notes/2014/wn_2014-10/excel_pandas_latex_pdf/</link>
      <pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-10/excel_pandas_latex_pdf/</guid>
      <description>###CSV data into pandas and LATEX then into PDF and then print!###
 To import csv file with date into pandas dataframe. Pands can parse date column, but it won&amp;rsquo;t be correct. The date column formate has top be mentiod and parsed for this, like below based on [this], then this can be imported into pandas datafrem emntioing the dateparse.  dateparse = lambda x: pd.datetime.strptime(x, &amp;lsquo;%d/%m/%y&amp;rsquo;) pro=pd.readcsv(&amp;lsquo;PROJECTEXPNDTR1.csv&amp;rsquo;,parsedates=[1],date_parser=dateparse)
dateparse = lambda x: pd.</description>
    </item>
    
    <item>
      <title>PythonGDALproblem</title>
      <link>/working-notes/2014/wn_2014-10/pythongdalproblem/</link>
      <pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-10/pythongdalproblem/</guid>
      <description>####Python GDAL problem####
ImportError: /usr/lib/libgdal.so.1: undefined symbol: sqlite3columntable_name
main.gdalconfigerror: [Errno 2] No such file or directory
sudo apt-cache showpkg sqlite3
ogrinfo grid.shp -dialect sqlite -sql &amp;ldquo;select sqlite_version()&amp;rdquo;
ldconfig -p
install.packages(filenameand_path, repos = NULL, type=&amp;ldquo;source&amp;rdquo;)
http://marc.info/?l=grass-dev&amp;amp;m=138736238422272&amp;amp;w=2
http://askubuntu.com/questions/443379/sqlite-header-and-source-version-mismatch
http://stackoverflow.com/questions/16095942/sqlite-header-and-source-version-mismatch/16366457#16366457
for R based netcdf view
https://gist.github.com/xuanlongma/5874674
http://stackoverflow.com/questions/11319698/how-to-install-r-packages-rnetcdf-and-ncdf-on-ubuntu
install.packages(repos=c(&amp;lsquo;http://cran.fhcrc.org/&#39;),pkgs=c(&#39;ncdf&#39;),lib=&amp;quot;/usr/lib/R/site-library/&amp;quot;,configure.args=&amp;quot;--with-netcdf-include=/usr/local/include &amp;ndash;with-netcdf-lib=/usr/local/lib&amp;rdquo;)
1.correcting the grid of coimbatore urban 1</description>
    </item>
    
    <item>
      <title>Timeseries Pandas</title>
      <link>/working-notes/2014/wn_2014-10/timeseries_pandas/</link>
      <pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-10/timeseries_pandas/</guid>
      <description>###Time series data plot and table creation with pandas and latex###
 to import the csv file into pandas df = pd.read_csv(&#39;/home/swl-sacon-dst/Documents/GISE_2013/LAB/Aerocet_DATA/TDM/TDM_MASS_20102014_171059-073359.csv&#39;), based on this To sepcifiy date time index in the dateframe df = df.set_index(pd.DatetimeIndex(df[&#39;Time&#39;])), based on this To resample 1 minute data into 15 minutes by avergae method, bars=df.resample(&#39;15min&#39;), here default method is mean. based on this To select specific columns in pandas df1=df[[&#39;Time&#39;,&#39;PM2.5(ug/m3)&#39;,&#39;PM10(ug/m3)&#39;,&#39;TSP(ug/m3)&#39;,&#39;AT(C)&#39;,&#39;RH(%)&#39;]] based on this To plot datframe by import matplotlib.</description>
    </item>
    
    <item>
      <title>compileGSIonUbuntu1404</title>
      <link>/working-notes/2014/wn_2014-09/compilegsionubuntu1404/</link>
      <pubDate>Sat, 20 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/compilegsionubuntu1404/</guid>
      <description>###compile GSI in Ubunut 14.04###
 GSI was tried to compile in Ubuntu 12.04, but ended with failure. GSI requieres Gfortran 4.7 and above, ubunut 12.04 repostory is updated upto 4.6, so ubunut has to upgraded from 12.04 to 14.04 to have gfortran 4.7. Due to this Server was upghraded from Ubuntu 12.04 to 14.04 and compiled WRFV4.3.1 and WPS 4.3.1 in it. It was by Gfortran 4.8.2, Netcdf 4.1.3, and other necssary libraries such as libpng, zlib and jasper as per apt-get method.</description>
    </item>
    
    <item>
      <title>WRF CHEM Compile completenote SERVER</title>
      <link>/working-notes/2014/wn_2014-09/wrf_chem_compile_completenote_server/</link>
      <pubDate>Wed, 10 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/wrf_chem_compile_completenote_server/</guid>
      <description>export DIR=/home/hoopoe/wrfchem341/lib/Netcdf4.1.3libs
sftp://hoopoe@192.168.1.100/home/hoopoe/wrfchem341/lib/Netcdf4.1.3libs/netcdf/include
export CC=gcc export CXX=g++ export FC=gfortran export FCFLAGS=-m64 export F77=gfortran export FFLAGS=-m64
export PATH=$DIR/mpich/bin:$PATH
export NETCDF=$DIR/netcdf export JASPERLIB=$DIR/grib2/lib export JASPERINC=$DIR/grib2/include export WRFEMCORE=1 export WRFNMMCORE=0 export WRFCHEM=1 export WRFKPP=0 export WRFIONCDLARGEFILESUPPORT=1
error 1. /bin/sh: 6: mpif90: not found 1. gfortran treating comments as code baased on this 1. still no wrf.exe was formed, by seeing configure.wrf thought environemnt WRFSRCROOTDIR has to be set export WRFSRCROOTDIR=/home/hoopoe/wrfchem341/WRFV3 1. installed sudo apt-get install libcloog-ppl1 1.</description>
    </item>
    
    <item>
      <title>forumpost</title>
      <link>/working-notes/2014/wn_2014-09/forumpost/</link>
      <pubDate>Tue, 09 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/forumpost/</guid>
      <description>there are example of porting WRF model in ARM processors. Fine grained parallellism, OpenMP, 16bytes genration in GPU exampel study
WRF is developed with single floating point oeprations and newer modules of WRF such as WRF CHEM for air quality predictions are having single as well as double floating point implemntaiton. There are studies comparing GPU enhanment with double and single point operations.
Plan of server board cluster Hardware enahmenet Limitation of OpenMP</description>
    </item>
    
    <item>
      <title>WRFCHEM CBE A1</title>
      <link>/working-notes/2014/wn_2014-09/wrfchem_cbe_a1/</link>
      <pubDate>Mon, 08 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/wrfchem_cbe_a1/</guid>
      <description>###WRF CHEM for Coimbatore A1 using ndown.exe###
####WPS#### 1. The WPS components geogrid.exe, ungrib.exe and metgrid.exe was from the output of last run on detailed in this
####real.exe#### 1. Real.exe also from the last run output. The output comprised of files namely wrfbdyd01, wrfinputd01, wrfinputd02, wrfinputd03, wrfinputd04 were used as such for convertemiss.exe run.
####PrepChemSrc.exe#### 1. It was executed as per the tutorial and PrepChemSrc.exe was executed by following point 3 in that tutorial.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part70 CBE DustOnlyTutorial</title>
      <link>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part70_cbe_dustonlytutorial/</link>
      <pubDate>Sat, 06 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part70_cbe_dustonlytutorial/</guid>
      <description>##Trail for WRF-CHEM, WRF CHEM simulation with dust only for Coimbatore using TWO WAY NESTING TWO files method## 1. It is based on this to run compiled wrf chem in serial mode with only dust.
###WPS:Geogrid### 9. As per this point 7, file GEOGRID.TBL.ARW_CHEM in WPS geogrid directory was linked as GEOGRID.TBL using GUI of Ubuntu 12.04. 10. The namelist.wps used was as follows,
 &amp;amp;share max_dom = 4, start_date =&#39;2014-06-05_00:00:00&#39;,&#39;2014-06-05_00:00:00&#39;,&#39;2014-06-05_00:00:00&#39;,&#39;2014-06-05_00:00:00&#39; end_date =&#39;2014-06-05_06:00:00&#39;,&#39;2014-06-05_06:00:00&#39;,&#39;2014-06-05_06:00:00&#39;,&#39;2014-06-05_06:00:00&#39; interval_seconds = 10800, io_form_geogrid = 2, / &amp;amp;geogrid parent_id = 1, 1, 2, 3 parent_grid_ratio = 1, 3, 3, 3 i_parent_start = 1, 26, 22, 22 j_parent_start = 1, 7, 15, 32 e_we = 90, 76, 97, 136 e_sn = 85, 73, 106, 157 geog_data_res = &#39;10m&#39;, &#39;5m&#39;, &#39;30s&#39;, &#39;30s&#39; dx = 27000 dy = 27000 map_proj = &#39;lambert&#39; ref_lat = 18.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part71 emission inventory CBE Ndown</title>
      <link>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part71_emission_inventory_cbe_ndown/</link>
      <pubDate>Sat, 06 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part71_emission_inventory_cbe_ndown/</guid>
      <description>###WRF CHEM for Coimbatore domain with emission inventory and one way nesting using ndown.exe###
####WPS#### 1. The WPS components geogrid.exe, ungrib.exe and metgrid.exe was from the output of last run on detailed in this
####real.exe#### 1. Real.exe also from the last run output. The output comprised of files namely wrfbdyd01, wrfinputd01, wrfinputd02, wrfinputd03, wrfinputd04 were used as such for convertemiss.exe run.
####PrepChemSrc.exe#### 1. It was executed as per the tutorial and PrepChemSrc.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part65 EmissionInventoryTut</title>
      <link>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part65_emissioninventorytut/</link>
      <pubDate>Thu, 04 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part65_emissioninventorytut/</guid>
      <description>##Trail for WRF-CHEM, WRF CHEM simulation with dust only tutorial## 1. It is based on this to run compiled wrf chem in serial mode with only dust.
###WPS### 1. WPS components ungrib.exe,geogrid.exe and metgrid.exe was from former dust only run&amp;rsquo;s output based on the note of . ###PrepChemSrc### 1. Prep chem src used was of compiled as per the note. This was ran as per the tutorial point4, the namelist used is this.</description>
    </item>
    
    <item>
      <title>GSI assimilation PM25PM10 WRFCHEM</title>
      <link>/working-notes/2014/wn_2014-08/gsi_assimilation_pm25pm10_wrfchem/</link>
      <pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/gsi_assimilation_pm25pm10_wrfchem/</guid>
      <description>###PM10,PM2.5 observation assimilation for WRF CHEM based on GSI###
based on this paper Implementation of aerosol assimilation in Gridpoint Statistical Interpolation (v. 3.2) and WRF-Chem (v. 3.4.1)
http://www.geosci-model-dev.net/7/1621/2014/gmd-7-1621-2014.pdf</description>
    </item>
    
    <item>
      <title>SOSforWRFCHEM output</title>
      <link>/working-notes/2014/wn_2014-08/sosforwrfchem_output/</link>
      <pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/sosforwrfchem_output/</guid>
      <description>###Sensor Observation service for WRF CHEM output###</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part63 WRF CoimbatoreNesting</title>
      <link>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part63_wrf_coimbatorenesting/</link>
      <pubDate>Tue, 19 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part63_wrf_coimbatorenesting/</guid>
      <description>##Trail for WRF-CHEM, WRF simulation with domain for Coimbatore and one way Nesting-Ndown.exe## 1. It is based on this to run compiled wrf in serial mode to execute domain over Coimbatore with nested model run by Ndown.exe based one way nesting.
###WPS: Ungrib###
 As per this page, the WPS ungrib stage was carried out. The gfs files downloaded was used, average size around 45MB for each files. The files for time peroid 00,03, 06 was kept under a folder gfs.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part64 DustOnlyTutorial</title>
      <link>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part64_dustonlytutorial/</link>
      <pubDate>Tue, 19 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part64_dustonlytutorial/</guid>
      <description>##Trail for WRF-CHEM, WRF CHEM simulation with dust only tutorial## 1. It is based on this to run compiled wrf chem in serial mode with only dust.
###WPS:Geogrid### 9. As per this point 7, file GEOGRID.TBL.ARW_CHEM in WPS geogrid directory was linked as GEOGRID.TBL using GUI of ubuntu 12.04. 10. The namelist.wps from the page was used. The namelist.wps is as follows,
 &amp;amp;share wrf_core = &#39;ARW&#39;, max_dom = 1, start_date = &#39;2010-07-14_00:00:00&#39;,&#39;2010-07-14_00:00:00&#39;, end_date = &#39;2010-07-19_00:00:00&#39;,&#39;2010-07-19_00:00:00&#39;, interval_seconds = 10800, io_form_geogrid = 2, / &amp;amp;geogrid parent_id = 1, 1, 1, parent_grid_ratio = 1, 5, 5, i_parent_start = 1, 6, 105, j_parent_start = 1, 65, 25, e_we = 41, 201, 226, e_sn = 41, 311, 231, geog_data_res = &#39;10m&#39;, &#39;2m&#39;, &#39;30s&#39; dx = 100000, dy = 100000, map_proj = &#39;lambert&#39;, ref_lat = 35.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part60 dustonly CBE</title>
      <link>/working-notes/2014/wn_2014-08/working-with-wrf-chem-part60-dustonly-cbe/</link>
      <pubDate>Tue, 12 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working-with-wrf-chem-part60-dustonly-cbe/</guid>
      <description>In a measure to make the compiled executables reuse, made a copy of WPS folder and WRFv3, parallel compiled folder test/real_em* into a new folder of cbe_domain. The link gets broken, and no correct files were copied by doing mere copy paste. Instead, a try was made as per the m2lab tutorial and copied the files from em_real as like this cp /em_real/* ., this time the link was not made, but the executables get copied, it has to check whether it is working For starting WPS of WRF with geogrid, used the namelist.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part61 WRF KatrinaSingleDomain</title>
      <link>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part61_wrf_katrinasingledomain/</link>
      <pubDate>Tue, 12 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part61_wrf_katrinasingledomain/</guid>
      <description>##Trail for WRF-CHEM, working with WRF simulation with Katrina single domain case## 1. It is based on this to run compiled WRF in serial mode to execute the Katrina case with a single domain. 2. As per this page, the WPS ungrib stage was carried out
###WPS: Ungrib### 3. The tar file provided with the tutorial was unzipped and compiled WPS folder was copied and bothe these folders were kept under a folder named katrina.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part62 WRF KatrinaNesting</title>
      <link>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part62_wrf_katrinanesting/</link>
      <pubDate>Tue, 12 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part62_wrf_katrinanesting/</guid>
      <description>##Trail for WRF-CHEM, WRF simulation with Katrina case and one-way Nesting-Ndown.exe## 1. Based on this to run compiled wrf in serial mode to execute the Katrina case with the nested model run by Ndown.exe based one-way nesting.
###WPS: Ungrib###
 As per this page, the WPS ungrib stage was carried out. Unzipped the tar file provided with the tutorial. Copied WPS folder both under a shared folder named as katrina_nesting. The vtable was linked inside the WPS folder by ln -sf ungrib/Variable_Tables/Vtable.</description>
    </item>
    
    <item>
      <title>Insert Observation in istSOS xml</title>
      <link>/working-notes/2014/wn_2014-08/python-script-insertobservation-istsos-xml/</link>
      <pubDate>Wed, 06 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/python-script-insertobservation-istsos-xml/</guid>
      <description>Based on the early experience of inserting observation in xml way with python using HTTP post in 52North SOS, made a try with Ist SOS, another way to do is using calling another python script to import data in CSV format. Based on this note on OWS service requests. to get the DescribeSensor information used this link, it always failed in specifying formate, but another request was successful
http://54.255.173.125/istsos/cbed?request=describeSensor&amp;amp;procedure=KNMR&amp;amp;outputFormat= text xml;subtype=&amp;quot;sensorML/1.</description>
    </item>
    
    <item>
      <title>Find the tense and voices in a sentence by English grammar checker</title>
      <link>/working-notes/2014/wn_2014-08/english-grammer-checker/</link>
      <pubDate>Fri, 01 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/english-grammer-checker/</guid>
      <description>Search made to find various type of tense in a sentence. Ended in this, page. Stanford core NLP and NLTK with python are good natural language processors used for this purposes. NLP gives a statistical measure of the possible role of words in a sentence, Standford NLP is giving a web service with visualization of various statical measures in part-of-speech tags. Its grammatical state is inferred using the Penn tags, yes machine based!</description>
    </item>
    
    <item>
      <title>The Github pages, HTML, css and js</title>
      <link>/working-notes/2014/wn_2014-07/gh-pages-html-css-js/</link>
      <pubDate>Tue, 29 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/gh-pages-html-css-js/</guid>
      <description> The files HTML, css and js, is enough for creating a web site. In GitHub this is detrimental in any gh-pages intended to create. There are several templates available to render web pages A simple markdown template used is, . https://github.com/aplib/markdown-site-template Final usage of this web site was selected based on its demonstration  </description>
    </item>
    
    <item>
      <title>Latex beamer based pdf presentation</title>
      <link>/working-notes/2014/wn_2014-07/latex-beamer-based-pdf-presentation/</link>
      <pubDate>Fri, 18 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/latex-beamer-based-pdf-presentation/</guid>
      <description> For creating a presentation with many images in it, better to use latex based presentation It is easier than ppt with tight control on formatting and content referencing The presentation is based on this note and chosen Berlin theme within beamer&amp;rsquo;s n number of various themes. The main problem faced is with BibTeX based referencing, and could not be solved and used normal superscript referencing. Another problem faced was box and its colour choosing, it was solved following this excellent tutorial  </description>
    </item>
    
    <item>
      <title>Workflow with Docear, Mendely and Python</title>
      <link>/working-notes/2014/wn_2014-07/workflow-docear-mendely-python/</link>
      <pubDate>Fri, 18 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/workflow-docear-mendely-python/</guid>
      <description>A script to select pdf files from a bunch of folders, subfolders, html files and create folders with only pdf files, the code is
import os count = 0 d=[] f=[] for (dirname, dirs, files) in os.walk(&#39;/home/swl-sacon-dst/Documents/GISE_2013/LAB/lab_notes/&#39;): #sepecifiying the directory to search for pdf files [from](http://stackoverflow.com/questions/273192/check-if-a-directory-exists-and-create-it-if-necessary) for filename in files: if filename.endswith(&#39;.pdf&#39;) : thefile = os.path.join(dirname,filename) dire = os.path.dirname(thefile) f.append(thefile) d.append(dire) #makung a list of ifle names and folders for copying #using list comphrehsnsion to change the folder path [from](http://stackoverflow.</description>
    </item>
    
    <item>
      <title>Serial port problem in Dylos Air quality monitor</title>
      <link>/working-notes/2014/wn_2014-07/serial-port-problem-lbm1-knmr/</link>
      <pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/serial-port-problem-lbm1-knmr/</guid>
      <description>A monitor is set up with three three-pin plugs extension box serially wired, and connected to another serially connected extension box with two pin plug. In which three plugs, one Dylos air quality monitor is connected, another 2A 7port USB hub adapter is connected and third and final pin from right to left, TP-link 3G router is connected. With the above setup, router LAN connected with raspberry pi which connected with 7hub USB, Dylos serial port connected with RPI, RPi accessed via wireless connectivity through a router.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part4 cbe domain run</title>
      <link>/working-notes/2014/wn_2014-07/working_with_wrf_chem_part4_cbe_domain_run/</link>
      <pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/working_with_wrf_chem_part4_cbe_domain_run/</guid>
      <description>Coimbatore domain made with four nests has been run with WRF-EMS with resolution starting from 100&amp;gt;27&amp;gt;9&amp;gt;3&amp;gt;1km. The domain is made using dwiz application in WRF-EMS. However, to test the domain above the Coimbatore region, the output was subject for wrfncxnj.py utility. It took 4 hours and 33 minutes to run this domain in IBM X4 series server computer with 16 GB ram. To overlay the Coimbatore city shape file above the WRF output NetCDF from wrfcnxnj.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part5 prep emis</title>
      <link>/working-notes/2014/wn_2014-07/working-with-wrf-chem-part5-prep-emiss/</link>
      <pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/working-with-wrf-chem-part5-prep-emiss/</guid>
      <description>Running prep-chem-src
 Based on part 1 prep-chem-SRC was compiled and ready to execute for emission inventory creation. Following WRF-chem Nepal tutorial and PREP-CHEM-SRC, README edited prep_chem_sources.inp. Then run the program by executing ./prep_chem_sources_RADM_WRF_FIM.exe, it ran for some steps but exited with error of Warning! ***HDF5 library version mismatched error*** saying the HDF5 version used in compiling is 1.8.8 and version for running PREP-CHEM-SRC is 1.8.12 To check what hdf5 is used by prepchesrc, run a command h5dump -V which gives h5dump: Version 1.</description>
    </item>
    
    <item>
      <title>Textfile from WRFoutput using Python</title>
      <link>/working-notes/2014/wn_2014-06/textfile-from-wrfoutput-pythonic/</link>
      <pubDate>Sat, 28 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/textfile-from-wrfoutput-pythonic/</guid>
      <description>To convert wrf output into a text file for the value of a specified lat long Using the script WrfncXnj.py, convert wrf output in cf abiding nc file The steps are as follows, cp wrfout_d04* /home/hoopoe/wrfncxnj-0.1_r2120/. To get the list of available files in a directory
import os a=[] for file in os.listdir(&amp;quot;/home/hoopoe/wrfncxnj-0.1_r2120/&amp;quot;): if file.startswith(&amp;quot;wrfout_d04&amp;quot;): a.append(file)  To run a external python script from another python script,
import subprocess a= wrfout_d04_2014-06-11_00:00:00 subprocess.</description>
    </item>
    
    <item>
      <title>Convert Latex into HTML</title>
      <link>/working-notes/2014/wn_2014-06/latex-into-html/</link>
      <pubDate>Fri, 27 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/latex-into-html/</guid>
      <description> For a workflow with latex and peer editing, latex in PDF has to converted into some editable formate can be used in text editor/word processor Based on this, latex tex files can be directly converted into HTML without going for pdf. Have to use this command htlatex mydocument.tex, but asks to install the program sudo apt-get install tex4ht, so install tex4ht Then execute the command of htlatex mydocument.tex and it produced HTML with css and other required files, this can be open in Libre text editor to make peers to edit or correct the content  </description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part1 PrepChemSrc</title>
      <link>/working-notes/2014/wn_2014-06/working-with-wrf-chem-part1-prepchemsrc/</link>
      <pubDate>Thu, 26 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/working-with-wrf-chem-part1-prepchemsrc/</guid>
      <description>compiling and install prep-chem-src
 To install prep-chem-src, it requires to install HDF5,ZLIB and NETCDF based on the readme of PREPCHEMSRC as follows
Third Party Software Requirements --------------------------------- 1. ZLIB 1.2.5(libz.a) or later distribution. You may download the software from the http://www.zlib.net/ site. 2. HDF5-1.8.8(libhdf5_fortran.a, libhdf5hl_fortran.a) or later distribution. You may download the software from http://www.hdfgroup.org/HDF5/release/obtain5.html. 3. NetCDF 4.1.1 (libnetcdf.a). You may download the software from http://www.unidata.ucar.edu/downloads/netcdf/netcdf-4_0_1/index.jsp Configuring/Installing HDF 5 ------------------------------ When compiling PREP-CHEM-SRC codes on a Linux system using the PGI (Intel) compiler, make sure the netCDF and HDF* library has been installed using the same PGI (Intel) compiler.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part2 WPS</title>
      <link>/working-notes/2014/wn_2014-06/working-with-wrf-chem-part2-wps/</link>
      <pubDate>Thu, 26 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/working-with-wrf-chem-part2-wps/</guid>
      <description>working with WRF-CHEM: Running WPS
The steps for running WPS as per the m2lab tutorial
Geogrid
 Create namelist and geogrid Copy wps folder and edit namelist for domain Use ncl program to view the domain Reflat and reflon center point of the domain, ewe, esn, number of point in x and y-direction Edit geog file location in name list and geogrid output location Then run geogrid.exe, run geogrid after editing the name list as .</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part3 compile wrf exe</title>
      <link>/working-notes/2014/wn_2014-06/working-with-wrf-chem-part3-compile-wrf-exe/</link>
      <pubDate>Thu, 26 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/working-with-wrf-chem-part3-compile-wrf-exe/</guid>
      <description>For running wrf.exe after wps, wrf.exe and real.exe is not being made during the compilation of wrf-chem. Made a new compilation of wrf alone to make it work. The compilation involves only setting environment, installing dependent packages and running, ./configure then ./compile em_real and ./compile emi_conv The first step of .configure ended with an error saying cannot find NetCDF in the path, the path specified is the wrong one. The file sudo nano bash.</description>
    </item>
    
    <item>
      <title>Editing shapefile using QGIS</title>
      <link>/working-notes/2014/wn_2014-06/shapefile-edit-qgis/</link>
      <pubDate>Wed, 18 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/shapefile-edit-qgis/</guid>
      <description> To edit multipart shapefile in QGIS into a one outline map Used QGIS multipart into a single feature by selected attributes It converts shapefile with multiple attributes feature(multiple rows) into single rows The resultant shapefile need to be edited to delete any residuals in operation, used delete ring in QGIS for that  </description>
    </item>
    
    <item>
      <title>Installing GDAL with Python in Ubunut1204</title>
      <link>/working-notes/2014/wn_2014-06/installing-gdal-with-python-in-ubunut1204/</link>
      <pubDate>Wed, 18 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/installing-gdal-with-python-in-ubunut1204/</guid>
      <description>Convert WRF output into geotiff. Python script requires gdal such as for this from osgeo import gdal and from osgeo import osr To install it in Ubuntu 12.04, it requires to install gdal-bin, libgdal1 and python-gdal, after installing and running python import of these libs ends up in crashes with this message segmentation error(core dumped) Detected error due to version difference, so installed with deb packages available for the version for 1.</description>
    </item>
    
    <item>
      <title>Reprojecting SHAPE file uing Python</title>
      <link>/working-notes/2014/wn_2014-06/reprojecting-shape-file-python/</link>
      <pubDate>Wed, 18 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/reprojecting-shape-file-python/</guid>
      <description>WRF output netcdf(nc) or wrfncx.py output nc files converted into geotiff(tiff) file with custom projections, see the codes for nc to tiff conversion.
from osgeo import gdal from osgeo import osr import numpy import numpy.ma as ma datafile = &#39;ZZZG3wrfout_psl.nc&#39; proj_out = osr.SpatialReference() proj_out.SetMercator(0.0, 115.02, 0.98931892612652, 0.0, 0.0) ds_in = gdal.Open(datafile) #subdatasets = ds_in.GetSubDatasets() #variables = [] #for subdataset in subdatasets: # variables.append(subdataset[1].split(&amp;quot; &amp;quot;)[1]) ds_lon = gdal.Open(&#39;NETCDF:&amp;quot;ZZZG3wrfout_psl.nc&amp;quot;:lon&#39;) ds_lat = gdal.</description>
    </item>
    
    <item>
      <title>Installing R and Openair in Ubuntu1204</title>
      <link>/working-notes/2014/wn_2014-06/installing-r-and-openair-in-ubuntu1204/</link>
      <pubDate>Tue, 17 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/installing-r-and-openair-in-ubuntu1204/</guid>
      <description>Installation of R in Ubuntu 12.04, there are deb packages available for R from its source page here However, after installation, it shows error in installing openair package, and dependency chain goes from 2 to 10 and much more package with version mismatch problem, starting from lattice saying Error: package ‘lattice’ was built before R 3.0.0: please re-install it Even calling package by the library(lattice&amp;rsquo;) given error So installation in apt-get library might be a problem-solving step, followed these</description>
    </item>
    
    <item>
      <title>Working with CSV using Python Pandas</title>
      <link>/working-notes/2014/wn_2014-06/data-editing-with-pandas/</link>
      <pubDate>Tue, 17 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/data-editing-with-pandas/</guid>
      <description>To import data into python
import pandas as pd data=pd.read_csv(&#39;value.txt&#39;)  To view data head or specified rows
data.iloc[:5, :4]  To sort data based on a specific column, here data column
d2=d1.sort([&#39;observation_time&#39;])  To make a dataetime column recognized as date column in padnas dataframe
d1[&#39;SamplingDate&#39;] = pd.to_datetime(d1[&#39;SamplingDate&#39;])  To remove NaN valued rows in any of the columns of the data frame
 d1=data.dropna()  To reindex data frame with date time columns</description>
    </item>
    
    <item>
      <title>Parsing the WRF logfile in Python</title>
      <link>/working-notes/2014/wn_2014-06/editing-wrf-logfile-python/</link>
      <pubDate>Sun, 08 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/editing-wrf-logfile-python/</guid>
      <description>To get a sum of seconds WRF is running, the wrf.out.log was imported into python to sum the elapsed seconds in each domain To get the log file into wrf bf = open(&#39;run_wrfm.log&#39;, &#39;r&#39;) To read each lines in the file bf_lines=bf.readlines() To make the lines into list array and select only the list with particular words in it, here the word &amp;ldquo;Timing for Writing&amp;rdquo;
f=[] for line in bf_lines: if &#39;Timing for Writing&#39; in line: f.</description>
    </item>
    
    <item>
      <title>Bewolf cluster setup or WRF EMS</title>
      <link>/working-notes/2014/wn_2014-06/bewolf-cluster-for-wrf-ems/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/bewolf-cluster-for-wrf-ems/</guid>
      <description>The four domain running of WRF EMS with resolution starting from 27km, 9km, 3km to 1km gives an error of exit with the status of -9. This showing that it would because of out of memory statusmore from this, this gives motivation further for a cluster running of WRF-EMS Mere passwordless SSH is not sufficient for cluster running of WRF EMS, as specified in run_ncpus.conf, the created cluster has to be checked with netcheck script given with WRF-EMS, it is in strc folder hint So while running netcheck with passwordless ssh, it gives an error that ssh localhostname hostname is not doing passwordless ssh, in the begning, this error seems to be ridiculous, how and what!</description>
    </item>
    
    <item>
      <title>Four Nested WRF-EMS simulation and its log file</title>
      <link>/working-notes/2014/wn_2014-06/wrf-ems-log/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/wrf-ems-log/</guid>
      <description>WRF-EMS installed through a local file in the laptop WRF-EMS was installed by online and script in server WRF-EMS was installed by online and script in server Four nested domain is not running in desktop saying error code 9 during wrf real.exe  Four nested domain run of WRF-EMS above Trivandrum area
 The domain made for Trivandrum tvm-gfs4, after running a test, it gives history output for every 30 minutes and took some four hours to complete To check the domain is covering the Trivandrum districts, after converting the WRF output into NetCDF using wrfncxnj.</description>
    </item>
    
    <item>
      <title>Installing and running the WRF EMS</title>
      <link>/working-notes/2014/wn_2014-06/running-wrf-ems/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/running-wrf-ems/</guid>
      <description>WRF EMS is a pearl scripted implementation of weather research and forecast (WRF) model. Relatively simple scripting implementations, it hides the complex compiling and running steps in the WRF model for operational and research purposes. Its installation is by running a Perl script provided by mail request; the total installation size goes around 22 GB, and so a long process. The first step in model execution in the creation of model domain, for high-resolution forecasting of weather in Coimbatore, has to make a four nested domain.</description>
    </item>
    
    <item>
      <title>Local install tion of WRF-EMS</title>
      <link>/working-notes/2014/wn_2014-06/local-install-wrf-ems/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/local-install-wrf-ems/</guid>
      <description>WRF ems can be installed locally using the file inside the folder releases Based on the WRF ems manual, chapter2, installation from local network, use ./ems_install.pl --install --repodir &#39;releases folder content&#39; After this the ems will be installed, for running the user&amp;rsquo;s default shell has to be set into tcsh, by following this as follows echo $SHELL if it gives /bin/bash change into tcsh by chsh -s /bin/tcsh, still after this it was showing the terminal without any $ or username.</description>
    </item>
    
    <item>
      <title>WRF EMS install and running in IBM X3100 M4</title>
      <link>/working-notes/2014/wn_2014-06/wrf-ems-install-and-running-in-ibm-x3100-m4/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/wrf-ems-install-and-running-in-ibm-x3100-m4/</guid>
      <description>Memory limit error is getting for 1km resolution domains in HP i5 system with a quad-core processor and 8GB memory. So made a try to make a cluster with another HP laptop with the same configuration, this step also returns EXIT file9, memory limit error. Changed the domain to make it 3 km resolution and ran in a cluster setup and it took 1 hour for four domains from 81km to 3km at 3-hour interval, so for 48-hour simulation, it would be taking a 16-hour model running.</description>
    </item>
    
    <item>
      <title>Python to query and edit json files</title>
      <link>/working-notes/2014/wn_2014-05/python-querying-and-editing-json/</link>
      <pubDate>Wed, 28 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/python-querying-and-editing-json/</guid>
      <description>For most of the works related to this and this It involves editing and querying of JSON and its formats such as GeoJson and Topojson In cbe-air web application, topjson is going to act as map element, and its editing required for real-time map generation and map styling In node.js based web application for visualizing model output, NetCDF output from WRF has to converted into geojson and made similar with the earth wind data formate.</description>
    </item>
    
    <item>
      <title>Merge multiple geojsons into single file</title>
      <link>/working-notes/2014/wn_2014-05/merge-multiple-geojsons-into-single-file/</link>
      <pubDate>Sat, 24 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/merge-multiple-geojsons-into-single-file/</guid>
      <description>For converting qs into cbe-air, the map is rendered using geojson rendering capability of GitHub The marker was easily made into geojson from QGIS and org2ogr as a shapefile. based on this under section &amp;ldquo;getting map data,&amp;rdquo; the command is
ogr2ogr -f GeoJSON point.json point.shp  and adding this script line in html
&amp;lt;script src=&amp;quot;https://embed.github.com/view/geojson/saconswl/cbeair/gh-pages/cbe-s.json?height=530&amp;amp;width=1300&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;  under div map
 To include Coimbatore city limits along with point marker, adding another script line of GitHub embed renderers another map!</description>
    </item>
    
    <item>
      <title>Node js with AIRwind and Earthwind applications</title>
      <link>/working-notes/2014/wn_2014-05/node-js-withairwindandearthwind/</link>
      <pubDate>Fri, 23 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/node-js-withairwindandearthwind/</guid>
      <description>To address objective2 of web processing service for real-time air pollution model, the implementation with node.jsearth or airis planned. These are the perfect match for this objective in showing the grandioseness of air circulation and how air pollution effect this grandioseness in real time animation of wind As a node.js web application, it has to installed as specified in the project&amp;rsquo;s readme. For node installation followed this wonderful tutorial, it involves and checking with node -v and npm -v</description>
    </item>
    
    <item>
      <title>The json data into SQLinsert with python</title>
      <link>/working-notes/2014/wn_2014-05/json-data-into-sqlinsert-with-python/</link>
      <pubDate>Fri, 23 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/json-data-into-sqlinsert-with-python/</guid>
      <description>To start with json data in python and loop over its elements
import json json_data=open(&#39;data.json&#39;) data=json.load(json_data) a=data[0][&#39;samples&#39;] for rs in a: print rs[&#39;wind&#39;]  To join two list as a column in python, to join two list inpython
for c1, c2 in zip(de, c): print &amp;quot;%-9s %s&amp;quot; % (c1, c2)  To append loop items into a array
c=[] for rs in a: c.append(rs[&#39;wind&#39;])  To remove u from list elemnt</description>
    </item>
    
    <item>
      <title>Querying netcdf with python and KDtree algorithm</title>
      <link>/working-notes/2014/wn_2014-05/querying-netcdf-with-python-kdtree/</link>
      <pubDate>Wed, 21 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/querying-netcdf-with-python-kdtree/</guid>
      <description>To query a NetCDF with latitude and longitude is required for objective three, in which user pointed lat-long, revived as SMS from Android app has to parsed and find its model and nearest Dylos monitoring station to send replay. There is handy tutorial on this with elaboration on different implementations advantages In which most advanced querying based on KDtree, this implementation was used to query NetCDF generated from WRF model the code is as follows</description>
    </item>
    
    <item>
      <title>Installing netcdf python in Ubuntu1204</title>
      <link>/working-notes/2014/wn_2014-05/installing-netcdf-python-in-ubuntu1204/</link>
      <pubDate>Tue, 20 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/installing-netcdf-python-in-ubuntu1204/</guid>
      <description>To install NetCDF-python requires HDF, based on this The HDF installation from source got failed, used the synaptic package manager instead to install HDF5 Downloaded NetCDF-python, extracted and run python setup.py install Failed, saying NetCDF is not found in usr/ So followed this, downloaded version of NetCDF-4.0.1, placed in /usr/local Doing cd into netcdf-4.0.1, and run the code
LDFLAGS=-L/usr/local/lib CPPFLAGS=-I/usr/local/include ./configure --enable-netcdf-4 --enable-dap --enable-shared --prefix=/usr/local  Then doing sudo make and then sudo make install, seems got installed, then went into netcdf4-python as given in this</description>
    </item>
    
    <item>
      <title>Converting WRF ouput netcdf into json</title>
      <link>/working-notes/2014/wn_2014-05/converting-wrf-ouput-netcdf-into-json/</link>
      <pubDate>Mon, 19 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/converting-wrf-ouput-netcdf-into-json/</guid>
      <description>Tried with grib2json for converting NetCDF into JSON 1 For this NetCDF has to converted into grib2 For converting into grib2, python based iris is useful 2, but only work with cf compliant NetCDF WRF output in NetCDF is not a cf compliant So has to use a tool which converts WRF NetCDF into CF compliant The Wrfncxnj.py tool 3 exactly do this with more functions such as extraction of variables.</description>
    </item>
    
    <item>
      <title>The database MySQL for Gammu</title>
      <link>/working-notes/2014/wn_2014-05/gammu-mysql/</link>
      <pubDate>Fri, 16 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/gammu-mysql/</guid>
      <description>Followed this,this The running, file-based gammu-smsd backend configuration file is as follows
 nano /etc/gammu-smsdrc # Configuration file for Gammu SMS Daemon # Gammu library configuration, see gammurc(5) [gammu] # Please configure this! port = /dev/ttyUSB2 model = connection = at synchronizetime = yes #logfile = /home/debian/gammulog #logformat = textalldate use_locking = gammuloc = # SMSD configuration, see gammu-smsdrc(5) [smsd] #debuglevel = 255 #Service = sql #Driver = sqlite3 #database = kalkun.</description>
    </item>
    
    <item>
      <title>QualitySCHU to cbe air</title>
      <link>/working-notes/2014/wn_2014-05/qualityschu-to-cbe-air/</link>
      <pubDate>Thu, 15 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/qualityschu-to-cbe-air/</guid>
      <description>Qualityschu(qs) is a web application coupled with istsos sensor web; it&amp;rsquo;s elegant, simple HTML with javascript design makes it easy to work with and to be educative. It is base code to build particulate matter air quality monitors web application in Coimbatore named cbeair web application. The app&amp;rsquo;s main difference with the source(qs) would be its ability to work with GitHub pages It is an HTML file with map and menu javascript files, main functions it provides is map view, table, chart view, and download functions coupled with istSOS.</description>
    </item>
    
    <item>
      <title>Edit a table in postgresql</title>
      <link>/working-notes/2014/wn_2014-05/edit-a-table-in-postgresql/</link>
      <pubDate>Mon, 12 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/edit-a-table-in-postgresql/</guid>
      <description>to list databases  \list
 to list tables in a database based on this  SELECT tableschema,tablename FROM informationschema.tables ORDER BY tableschema,table_name;
 to view details about a table  \d table.name
 to edit a column in a table  ALTER TABLE cbed.measures ALTER COLUMN val_msr TYPE numeric(14,6);</description>
    </item>
    
    <item>
      <title>Convert pandas dataframe into a PDF file using Latex</title>
      <link>/working-notes/2014/wn_2014-05/pandas-dataframe-into-latex-pdf/</link>
      <pubDate>Sat, 10 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/pandas-dataframe-into-latex-pdf/</guid>
      <description>Huge data frames which go for multiple A4 pages landscape is difficult to make in excel The alternative uses python pandas and convert the pandas&amp;rsquo; data frame into pdf through latex or HTML, latex is promising for just printing Basically from ([1])(http://stackoverflow.com/questions/14380371/export-a-latex-table-from-pandas-dataframe) for convert data frame into tex and this for converting Tex into landscape pdf document The python script to make table text is as follows, It is mostly from third answer [1], and bold column heading write python trick from</description>
    </item>
    
    <item>
      <title>IStSOS Data formating using Python</title>
      <link>/working-notes/2014/wn_2014-05/onpython/</link>
      <pubDate>Sat, 10 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/onpython/</guid>
      <description>Use Python with SQLite, istsos data formate and timeseries subsampling (downscaling).
 With sqlite to convert a list, for example cur.fetch from database like sqlite,
[&#39;2014-04-30T10:25,2797,147&#39;, &#39;2014-04-30T10:27,2639,174&#39;, &#39;2014-04-30T10:29,2645,158&#39;, &#39;2014-04-30T10:31,2676,149&#39;]  Use print &amp;ldquo;\n&amp;rdquo;.join(b) based on this gives
&amp;quot;2014-04-30T10:25,2797,147 2014-04-30T10:27,2639,174 2014-04-30T10:29,2645,158 2014-04-30T10:31,2676,149&amp;quot;  To remove double quotes from above to write into a.DAT, tried almost two hours then find out that the used method will not do this. The full code is as follows with uncommented lines showing failed attempts.</description>
    </item>
    
    <item>
      <title>Cross Origin Resource Sharing</title>
      <link>/working-notes/2014/wn_2014-04/cross-origin-resource-sharing/</link>
      <pubDate>Wed, 30 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/cross-origin-resource-sharing/</guid>
      <description>While working with qs into cbe-air, in populating table and charts with istsos JSON data, stuck with an error The error was not informative in firebug. firefox, a simple correct response of 200 with red fonts and also not giving any console.log in JS After trying with changing jquery version, different old edited version of qs, experimenting with different JSON URL found no clue, finally Tried to run the server in chromium and its developer tools option gives an error of</description>
    </item>
    
    <item>
      <title>File system based SOS using github AJAX</title>
      <link>/working-notes/2014/wn_2014-04/file-system-based-sos-using-github-ajax/</link>
      <pubDate>Thu, 24 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/file-system-based-sos-using-github-ajax/</guid>
      <description> Found an article discussing similar to this line. https://www.academia.edu/1502083/A_flexible_geospatial_sensor_observation_service_for_diverse_sensor_data_based_on Some related paperes are Monitoring real-time environmental information using Web 2.0 and GIServices technology and Integrating Sensor Webs with Modeling and Data-assimilation Applications: An SOA Implementation Also see  http://buyya.com/papers/SensorWebChapter.pdf http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=4526452&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4526452  For simple SOS implementation https://github.com/jcu-eresearch/python-simplesos  </description>
    </item>
    
    <item>
      <title>IRIS install</title>
      <link>/working-notes/2014/wn_2014-04/iris-install/</link>
      <pubDate>Thu, 24 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/iris-install/</guid>
      <description>IRIS is a python tool for working with NetCDF and Grib files. It is installed to convert WRF output in netCDF to grib2 formate, which is the need for grib2json tool. IRIS is dependent on a large number of scientific python libraries. Most of the libraries are the python and installed through
pip install library  Installation further gets erroneous due to unavailability of NetCDF, HDF5, NetCDF-python packages.
 Has to follow this note (http://code.</description>
    </item>
    
    <item>
      <title>Package using maven</title>
      <link>/working-notes/2014/wn_2014-04/package-using-maven/</link>
      <pubDate>Thu, 24 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/package-using-maven/</guid>
      <description>Java-based tools such as grib2json have to compiled with maven The latest maven is installed, following as per this note (https://github.com/saconswl/Real_time_air_pollution_Mod_Proj-2013-2014/blob/home/working_notes/wn_2013-10/Installing_maven_in_Ubuntu_12.04.md) The tool grib2json requires java 1.7
 Done search due to error while trying for
mvn package  It gives the error of
Error: JAVA_HOME is not defined correctly. We cannot execute ”/usr/lib/jvm/jdk1.7.0”/bin/java  Later found that the system does not contain jdk1.7.0
 Installed jdk1.7.0 following (http://askubuntu.com/questions/117189/apt-get-install-openjdk-7-jdk-doesnt-install-javac-why)</description>
    </item>
    
    <item>
      <title>Gammu smsd shared memeory error for Huwaei E303F</title>
      <link>/working-notes/2014/wn_2014-04/gammu-smsd-shared-memeory-error-for-huwaei-e303f/</link>
      <pubDate>Mon, 21 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/gammu-smsd-shared-memeory-error-for-huwaei-e303f/</guid>
      <description>Running gammu and gammu-smsd backed by Mysql in Ubuntu 12.04, All these setup was running without error using Huwaei E173 data card. However, upgraded model of this, Huwaei E303F, working fine with gammu, but starting gammu-smsd collapsing gammu. For example gammu --identify says phone not connected Looking into file of gammurc ~/.gammurc
port = /dev/ttyUSB0 model = auto connection = at synchronizetime = yes logfile = /home/user/gammu.log logformat = textalldate use_locking = gammuloc =  Looking into file gammu-smsdrc ~/etc/gammu-smsdrc</description>
    </item>
    
    <item>
      <title>Program for usbreset</title>
      <link>/working-notes/2014/wn_2014-04/usbreset-program/</link>
      <pubDate>Mon, 21 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/usbreset-program/</guid>
      <description>From this ask ubuntu answer. to reset the USB data card Saving this code as usbreset.c
/* usbreset -- send a USB port reset to a USB device */ #include &amp;lt;stdio.h&amp;gt; #include &amp;lt;unistd.h&amp;gt; #include &amp;lt;fcntl.h&amp;gt; #include &amp;lt;errno.h&amp;gt; #include &amp;lt;sys/ioctl.h&amp;gt; #include &amp;lt;linux/usbdevice_fs.h&amp;gt; int main(int argc, char **argv) { const char *filename; int fd; int rc; if (argc != 2) { fprintf(stderr, &amp;quot;Usage: usbreset device-filename\n&amp;quot;); return 1; } filename = argv[1]; fd = open(filename, O_WRONLY); if (fd &amp;lt; 0) { perror(&amp;quot;Error opening output file&amp;quot;); return 1; } printf(&amp;quot;Resetting USB device %s\n&amp;quot;, filename); rc = ioctl(fd, USBDEVFS_RESET, 0); if (rc &amp;lt; 0) { perror(&amp;quot;Error in ioctl&amp;quot;); return 1; } printf(&amp;quot;Reset successful\n&amp;quot;); close(fd); return 0; }  Compile code using cc usbreset.</description>
    </item>
    
    <item>
      <title>Compiling WRF CHEM</title>
      <link>/working-notes/2014/wn_2014-04/compiling-wrf-chem/</link>
      <pubDate>Fri, 18 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/compiling-wrf-chem/</guid>
      <description>Based on this tutorial
 In WRFV3 folder, (the &amp;lsquo;chem&amp;rsquo; folder has to copied inside of this folder) entering
$ ./configure Checking for perl5... no checking for perl... found /usr/bin/perl (perl) ** WARNING: No path to NETCDF and environment variable NETCDF not set. ** would you like me to try to fix? [y] y Enter full path to NetCDF include directory on your system /usr/include Enter full path to NetCDF library directory on your system /usr/lib created new .</description>
    </item>
    
    <item>
      <title>CSV edit by pandas</title>
      <link>/working-notes/2014/wn_2014-04/csv-edit-by-pandas/</link>
      <pubDate>Thu, 10 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/csv-edit-by-pandas/</guid>
      <description>to import csv file into python
import pandas data = pd.read_csv(&#39;/home/hoopoe/Documents/Real_time_air_pollution_Mod_Proj-2013-2014/obj2/237.csv&#39;)  to query the specific column in the data frame
data[&#39;SamplingDate&#39;]  to specify the column as DateTime formate column for pandas
data[&#39;SamplingDate&#39;] = pd.to_datetime(data[&#39;SamplingDate&#39;])  to avoid date and month mismatch specify the format of date as
data[&#39;SamplingDate&#39;] = pd.to_datetime(data[&#39;SamplingDate&#39;],format=&#39;%d/%m/%Y&#39;)  to sort the data based on date column descending
dataso=data.sort(&#39;SamplingDate&#39;, ascending=False)  to join two data frame in particular column</description>
    </item>
    
    <item>
      <title>Dylos monitor setup full with log</title>
      <link>/working-notes/2014/wn_2014-04/dylos-monitor-setup-full-with-log/</link>
      <pubDate>Thu, 10 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/dylos-monitor-setup-full-with-log/</guid>
      <description>Following http://hintshop.ludvig.co.nz/show/persistent-names-usb-serial-devices/ and UdevrulesforUSB&amp;rsquo;sattachedtoRPi
 Create a udev rule for RPI, edited the file /etc/udev/rules.d using command sudo nano /etc/udev/rules.d/90-phone.rules and add following lines to giving a persistent name for USB data card (HUWAEI E303F) and USB to serial cable for Dylos monitor.
KERNEL==&amp;quot;ttyUSB*&amp;quot;, ATTRS{idVendor}==&amp;quot;12d1&amp;quot;, ATTRS{idProduct}==&amp;quot;1506&amp;quot;, NAME=&amp;quot;phone&amp;quot;, MODE=&amp;quot;0666&amp;quot;,SYMLINK+=&amp;quot;mobile&amp;quot; KERNEL==&amp;quot;ttyUSB*&amp;quot;, ATTRS{idVendor}==&amp;quot;067b&amp;quot;, ATTRS{idProduct}==&amp;quot;2303&amp;quot;, NAME=&amp;quot;dylos&amp;quot;, MODE=&amp;quot;0666&amp;quot;,SYMLINK+=&amp;quot;dylos&amp;quot;  Adding udev rules gives option select devices, while connecting two or more devices. The folder /dev shows the files for USB data card and USB to serial cable for Dylos monitor.</description>
    </item>
    
    <item>
      <title>Sending SMS with AT and python</title>
      <link>/working-notes/2014/wn_2014-04/sending-sms-with-at-and-python/</link>
      <pubDate>Tue, 08 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/sending-sms-with-at-and-python/</guid>
      <description>Huwaei E303F is not working with Gammu, especially in RPi. So found a method to send SMS using this data card without using gammu but using simple AT commands.
 primarly based on this 1 and this 2 the code written for sending SMS from Dylos serial is as follows
#!/usr/bin/python import serial import time from curses import ascii import sqlite3 as lite import logging logger = logging.getLogger(&#39;lbm1&#39;) hdlr = logging.</description>
    </item>
    
    <item>
      <title>HYSPLIT compile</title>
      <link>/working-notes/2014/wn_2014-04/hysplit_compile/</link>
      <pubDate>Fri, 04 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/hysplit_compile/</guid>
      <description>Compiling HYSPLIT in Ubuntu 12.04 was hurdled by a error related with netcdf.
 Installation of netcdf is through compiling and package installation from synaptic package manager. Synaptic and latest source download compilation gives error of &amp;ldquo;no netcdf.inc&amp;rdquo; in hysplit compile. It is due to a fortran binding lapse in latest verision. So used a old version of the netcdf 3.6.3 and compiled following this&amp;ndash; http://code.google.com/p/netcdf4-python/wiki/UbuntuInstall has to give sudo in make and make install, with the first comment disable-shared</description>
    </item>
    
    <item>
      <title>Sending SMS with Beagle bone black</title>
      <link>/working-notes/2014/wn_2014-03/sending_sms_with_beagle_bone_black/</link>
      <pubDate>Tue, 25 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-03/sending_sms_with_beagle_bone_black/</guid>
      <description>BBB is setup with Angstrom Linux, first connected to Ubuntu lap using USB wire from BBB, then a USB was connected to the powered USB hub, in which a serial USB connector and a Huwaei e 173 data (This tested for Huwaei E303F, it worked) card were connected. ssh&amp;rsquo;s into it using ssh 192.168.7.2 -l root and password blank (an enter) lsusb shows Huwaei with modem and serial usb. Need to make usb_modeswitch for Huwaei GSM to sent SMS, so usb_modeswitch has to be installed in the Angstrom, downloaded ipk file (deb in angstrom) from http://feeds.</description>
    </item>
    
    <item>
      <title>EMS WRF</title>
      <link>/working-notes/2014/wn_2014-03/ems_wrf/</link>
      <pubDate>Thu, 06 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-03/ems_wrf/</guid>
      <description>EMS-WRF for Coimbatore region using four domains
Five simple steps required for the execution of EMS (Environmental modeling system) WRF.
After the successful installation of EMS follow these steps,
Step 1: Create a computational domain with the Domain Wizard(DW) GUI
 Start the DW by executing the command &amp;ldquo;dwiz&amp;rdquo; in the terminal. DW window should appear and gives an option to select from creating a new domain or modifying an existing Domain.</description>
    </item>
    
    <item>
      <title>Installing 55north SOS with tomcat7</title>
      <link>/working-notes/2014/wn_2014-02/installing-55north-sos-with-tomcat7/</link>
      <pubDate>Fri, 21 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-02/installing-55north-sos-with-tomcat7/</guid>
      <description>Installed tomcat 7 using http://askubuntu.com/questions/339169/how-to-install tomcat-7-0-42-on-ubuntu-12-04-3-lts to find tomcat is running followed by http://stackoverflow.com/questions/3944157/is-tomcat-running netstat -a | grep 8080 ps -ef | grep tomcat
both commands gives some bulge result and responses if it is working. if it is not working the command used to start tomcat7 is from the installation page sudo $CATALINA_HOME/bin/startup.sh Installing the SOS war following tomcat manager and war file upload to upload the data into istsos used this command following all its installation documentation.</description>
    </item>
    
    <item>
      <title>Python for fetching Mysql table</title>
      <link>/working-notes/2014/wn_2014-02/python_for_fetching_mysql_table/</link>
      <pubDate>Tue, 18 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-02/python_for_fetching_mysql_table/</guid>
      <description>Converting the MySQL backed SMS gateway data into (Sensor Observation Service, SOS) import formate. COCEMS_lbm is real time Dylos air quality montior sending the data every 15 minutes through SMS and received by server-side data card and Gammu SMS gateway backed by MySQL, the data is in inbox table. Following python script do the job. It took a long time in understanding the difference between array, and list objects in python, a clear understanding of this would not cost this much time to solve the error.</description>
    </item>
    
    <item>
      <title>Wind power forecasting map</title>
      <link>/working-notes/2014/wn_2014-02/wind-power-forecasting-map/</link>
      <pubDate>Wed, 05 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-02/wind-power-forecasting-map/</guid>
      <description>Problem statement
In Tamil Nadu, electricity is significantly relying on wind power based renewable energy source. If much wind is there, there will be minimal power cuts and vice versa. The second-tier urban area like Coimbatore, this dependence is much visible, and so one of the simple predictors of long power cuts is lack of adequate wind power in the nearby wind park area for example. On the other hand operators of windmills or power transmission sector, if they know much early about the forecast of wind power in their area, they have many advantages in preparing for storing the surplus energy source or find alternatives in the situation of low wind power.</description>
    </item>
    
    <item>
      <title>Python script for inserting SOS</title>
      <link>/working-notes/2014/wn_2014-01/python-script-for-inserting-sos/</link>
      <pubDate>Sat, 25 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/python-script-for-inserting-sos/</guid>
      <description>A python script to insert data into 52 NORTH SOS through HTTP POST. Save this script as python file and run the script in the terminal as python “scriptname”.py. It will insert the data and report the status as given by test client “send” button. 52 north SOS needs to run in localhost.
based on https://github.com/mpfeil/qualitySCHU/blob/master/Parser/LANUV/main.py http://stackoverflow.com/questions/16055334/post-xml-request-using-python
import urllib import httplib from xml.dom.minidom import parse, parseString target_url = “http://localhost:8080/52n-sos-webapp-4.0.0-Beta1/sos/soap” #the insert observation requests from test client 52 north SOS xml_request = “”&amp;quot; &amp;lt;sos:offering&amp;gt;test_offering_1&amp;lt;/sos:offering&amp;gt; &amp;lt;sos:observation&amp;gt; &amp;lt;om:OM_Observation gml:id=&amp;quot;o1&amp;quot;&amp;gt; &amp;lt;om:type xlink:href=&amp;quot;http://www.</description>
    </item>
    
    <item>
      <title>Recover password for postgresql</title>
      <link>/working-notes/2014/wn_2014-01/recover-password-for-postgresql/</link>
      <pubDate>Sat, 25 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/recover-password-for-postgresql/</guid>
      <description>based on http://scratching.psybermonkey.net/2009/06/postgresql-how-to-reset-user-name.html
 Edit the file sudo nano /etc/postgresql/9.1/main/pg_hba.conf change into local all postgres md5 local all postgres trust| then do a restart sudo service postgresql restart now enter into the PostgreSQL using
psql -U postgres ALTER USER postgres with password &#39;new password&#39;;  then again change the pg_hba.conf as earlier and restart the PostgreSQL for invoking the password protection
  </description>
    </item>
    
    <item>
      <title>Installing Munin for ubuntu server</title>
      <link>/working-notes/2014/wn_2014-01/installing-munin-for-ubuntu-server/</link>
      <pubDate>Fri, 17 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/installing-munin-for-ubuntu-server/</guid>
      <description>Based on
http://ubuntuserverguide.com/2012/08/how-to-install-and-configure-munin-on-ubuntu-server-12-04.html http://www.hashbangcode.com/blog/monitoring-performance-munin-713.html  </description>
    </item>
    
    <item>
      <title>More with SMS gateway for ubuntu 1204</title>
      <link>/working-notes/2014/wn_2014-01/more-with-sms-gateway-for-ubuntu-1204/</link>
      <pubDate>Fri, 17 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/more-with-sms-gateway-for-ubuntu-1204/</guid>
      <description>based on this http://blog.sleeplessbeastie.eu/2012/07/16/kalkun-how-to-setup-sms-gateway-at-home/ and http://back2arie.wordpress.com/2010/07/27/using-gammu-smsd-with-multiple-phone/ problem rectified by http://askubuntu.com/questions/211739/gammu-and-device-permissions following are installed sudo apt-get install gammu gammu-smsd sudo cp /usr/share/doc/gammu/examples/config/gammurc /etc/gammurc the gammurc file will not be there, has to do this step, instaed of running gammu-conifg is a problem and make gammurc files in home folder now runing gammu --identify, gives no phone detected and other error for this the config file gammurc has to edited as per like this</description>
    </item>
    
    <item>
      <title>SCREEN usage</title>
      <link>/working-notes/2014/wn_2014-01/screen-usage/</link>
      <pubDate>Thu, 16 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/screen-usage/</guid>
      <description>SCREEN is a utility for multiple session in command line Linux.
 To start a screen with the name
screen -S &amp;quot;name without mark&amp;quot;  To view running screen
screen -r  to get into a particular screen
screen -xr pid(of the screen)  to get out of the screen
CTRL+A+D   Ubuntu problem atkbd.c spamming the logs. How to get rid? What is this? using this http://askubuntu.com/questions/116538/atkbd-c-spamming-the-logs-how-to-get-rid-what-is-this</description>
    </item>
    
    <item>
      <title>Secure ubuntu SERVER 1204</title>
      <link>/working-notes/2014/wn_2014-01/secure-ubuntu-server-1204/</link>
      <pubDate>Thu, 16 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/secure-ubuntu-server-1204/</guid>
      <description>For key pair login and removing the password based on  http://ubuntuforums.org/archive/index.php/t-30709.html http://blog.nas-admin.org/?p=63 http://www.thefanclub.co.za/how-to/how-install-psad-intrusion-detection-ubuntu-1204-lts-server following first one  following steps
cd .ssh/ ssh-keygen -t dsa scp id_dsa.pub serverusername@IP:./id_dsa.pub ssh into server cd .ssh touch authorized_keys2 chmod 600 authorized_keys2 cat ../id_dsa.pub &amp;gt;&amp;gt; authorized_keys2 rm ../id_dsa.pub  edited the /etc/ssh/sshd_config for for pass word less authentication
#PasswordAuthentication yes &amp;gt;&amp;gt;&amp;gt; PasswordAuthentication no PermitRootLogin yes &amp;gt;&amp;gt;&amp;gt; PermitRootLogin no &amp;gt;&amp;gt;&amp;gt; DebianBanner no  then restarted the ssh</description>
    </item>
    
    <item>
      <title>USB DATA card for ubuntu 1204</title>
      <link>/working-notes/2014/wn_2014-01/usb_data_card_for_ubuntu_1204/</link>
      <pubDate>Thu, 16 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/usb_data_card_for_ubuntu_1204/</guid>
      <description>To detect the USB connected to system, Do a dmesg |grep tty If not showing anything, the issue was solved following final answer of this question http://ubuntuforums.org/archive/index.php/t-1853306.html dmesg is a &amp;ldquo;ring buffer, so if many messages are being logged, will lose the initial boot messages. Try this instead:&amp;rdquo;
cd /var/log grep ttyUSB dmesg messages *log | more&amp;quot;  now showing
fellow@dhcppc3:~$ cd /var/log ; grep ttyUSB dmesg messages *log | more grep: messagesdmesg:[ 23.</description>
    </item>
    
    <item>
      <title>Removing PIN in android</title>
      <link>/working-notes/2014/wn_2014-01/removing-pin-in-android/</link>
      <pubDate>Mon, 06 Jan 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-01/removing-pin-in-android/</guid>
      <description>To remove the forgotten pin in android tablet, it has to be USB debugging enabled in past. The using adb shell
adb shell su cd data/data/system rm gesture.key  </description>
    </item>
    
    <item>
      <title>Script for json to csv for weather underground API fetching historical data</title>
      <link>/working-notes/2013/wn_2013-12/script-for-json-to-csv-for-weather-underground-api-fetching-historical-data/</link>
      <pubDate>Mon, 16 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-12/script-for-json-to-csv-for-weather-underground-api-fetching-historical-data/</guid>
      <description>Based on this https://github.com/PythonJournos/LearningPython/blob/master/tutorials/convert_json_to_csv.py, the script as follows,
import urllib2 import json import csv outfile_path=&#39;history.csv&#39; writer = csv.writer(open(outfile_path, &#39;w&#39;)) headers = [&#39;date&#39;] writer.writerow(headers) req = urllib2.Request(&amp;quot;http://api.wunderground.com/api/YOUR_KEY/history_20131001/q/India/Coimbatore.json&amp;quot;) opener = urllib2.build_opener() f = opener.open(req) data = json.load(f) for history in data[&#39;history&#39;][&#39;observations&#39;]: row = [] row.append(str(history[&#39;date&#39;][&#39;pretty&#39;])) row.append(str(history[&#39;tempm&#39;])) writer.writerow(row)  Now the URL has to be iterated to give a range of historical data required, and most important the date range has to set.</description>
    </item>
    
    <item>
      <title>Adding font in Ubuntu 1204</title>
      <link>/working-notes/2013/wn_2013-12/adding-font-in-ubuntu-1204/</link>
      <pubDate>Wed, 04 Dec 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-12/adding-font-in-ubuntu-1204/</guid>
      <description>To add font
 move the ttf file into the folder usr/share/fonts/ttf then remove fonts cache by rm -f /usr/share/fonts/*fonts.cache-1 then create the cache again sudo fc-cache that&amp;rsquo;s it, added ttf can be seen in all the text editors.  </description>
    </item>
    
    <item>
      <title>Notes on Geoserver</title>
      <link>/working-notes/2013/wn_2013-11/notes-on-geoserver/</link>
      <pubDate>Tue, 05 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-11/notes-on-geoserver/</guid>
      <description> Started based on this link http://gis.stackexchange.com/questions/69430/best-way-to-visualize-the-postgis-raster-in-openlayers Geoserver very easy to install, followed installation based on war and Apache tomcat. For know more about rest api for Geoserver http://boundlessgeo.com/2012/10/adding-layers-to-geoserver-using-the-rest-api/ to more on the python script for automatic MODIS sat visualization http://gis.stackexchange.com/questions/16515/how-to-import-a-raster-into-postgis  </description>
    </item>
    
    <item>
      <title>PostGresql table edit in command line</title>
      <link>/working-notes/2013/wn_2013-11/postgresql-table-edit-in-command-line/</link>
      <pubDate>Tue, 05 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-11/postgresql-table-edit-in-command-line/</guid>
      <description>To edit the Istsos database table “measures” in PostgreSQL it is to overcome to the error in istsos import saying the Dylos reading is exceeding the digit limit of the column val_meas in the table. Normally it is easy to change column digit size through pgadmin. However, for command line, it requires following commands.
 Get access to PostgreSQL
sudo -u postres psql  To view the database in PostgreSQL</description>
    </item>
    
    <item>
      <title>SMS management web application</title>
      <link>/working-notes/2013/wn_2013-11/sms-management-web-application/</link>
      <pubDate>Tue, 05 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-11/sms-management-web-application/</guid>
      <description>Use
http://kalkun.sourceforge.net/</description>
    </item>
    
    <item>
      <title>Screen for remote head less sever</title>
      <link>/working-notes/2013/wn_2013-11/screen-for-remote-head-less-sever/</link>
      <pubDate>Tue, 05 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-11/screen-for-remote-head-less-sever/</guid>
      <description>Execute multiple programs simultaneously in the terminal, use the program screen.
 based on http://askubuntu.com/questions/163567/start-program-from-terminal and http://www.howtoforge.com/linux_screen Basic running steps  screen -r: to view any running screen If there is no running screen, by this command a new terminal will be open, and any commands can be run here, such as to get a file using sftp To get out of the screen type CTRL +A+D after this the cursor goes to the base terminal   based on</description>
    </item>
    
    <item>
      <title>Notes on selenium</title>
      <link>/working-notes/2013/wn_2013-11/notes-on-selenium/</link>
      <pubDate>Fri, 01 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-11/notes-on-selenium/</guid>
      <description>Selenium installation is based on http://www.pererikstrandberg.se/blog/index.cgi?page=InstallingSelenium</description>
    </item>
    
    <item>
      <title>Installing maven in Ubuntu 1204</title>
      <link>/working-notes/2013/wn_2013-10/installing-maven-in-ubuntu-1204/</link>
      <pubDate>Tue, 08 Oct 2013 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2013/wn_2013-10/installing-maven-in-ubuntu-1204/</guid>
      <description>Based on links
 http://docs.geotools.org/latest/userguide/build/install/mvn.html http://www.mkyong.com/maven/how-to-install-maven-in-ubuntu/ http://lukieb.wordpress.com/2011/02/15/installing-maven-3-on-ubuntu-10-04-lts-server/  Download, untar and copy maven in /usr/local/ wget http://archive.apache.org/dist/maven/binaries/apache-maven-3.0.4-bin.tar.gz tar -zxf apache-maven-3.0.4-bin.tar.gz sudo cp -R apache-maven-3.0.4 /usr/local   then link with bin folder
sudo ln -s /usr/local/apache-maven-3.0.4/bin/mvn /usr/bin/mvn
 Then add java home link in the .bashrc sudo nano .bashrc
---JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-amd64 Thats it maven is installed Test it
 mvn –version
 Seems every think ok but there will be an issue while mvn install, the error of</description>
    </item>
    
  </channel>
</rss>