<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2014 on Nishadh KA</title>
    <link>/archives/2014/</link>
    <description>Recent content in 2014 on Nishadh KA</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Dec 2014 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/archives/2014/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>WRF CHEM cluster</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_cluster/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_cluster/</guid>
      <description>####WRFCHEMsimluation by cluster####
#to Do 1. write the script, lazywrfchem 1. draw computational inventory of the cluster comparision experiment, and libnrary program needed 1. compile the library 1. do the experiment
#single computer HP laptop, parallell execution, three cores
Domain four: onehour=12:16 to 08:43, 8.5 hours so for 6 hours simulation= 48+24=72 hours Domain Three: one hour=01:24 to 02:24, 1 hour so for 6 hours simulation= 6 hours Domain two: onehour=9:10 to 9:23, 13 minutes so for 6 hours simulation=13minx6=1.</description>
    </item>
    
    <item>
      <title>WRF CHEM compile QN</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn/</guid>
      <description>####Compile wrf chem: QUICK NOTE####
 It is based on the NCAR wrf chem compile complete process and note on wrf chem compile by this Basic environemtn testl, passes all the tests which checks libs such as, gcc, gfortran, gcc+gfortran, perl, csh. Made sure the gcc and gfortran versions are matching, current case version is 4.8.2 for both gcc and gfortran Basic library compile MPICh and NETCDF  For setting up PATH for NETCDF</description>
    </item>
    
    <item>
      <title>WRF CHEM compile QN AWS</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_aws/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_aws/</guid>
      <description>#####Compile wrf chem in AWS: QUICK NOTE#####
 It is based on the NCAR wrf chem compile complete process and note on wrf chem compile by this Create a ubuntu trusty based image. Make a small elastic block storage 8GB, it will be permanent even if the instance is kept stop. There will be storage space in /mnt for upto 300GB based on instance, for example c3.large has it. This storage gets erased while stopping the instance but not the /home folder where the small 8GB EBS is being mounted/ Install basic required libs sudo apt-get install g++ sudo apt-get install gfortran sudo apt-get install gcc Basic environment test, make sure the test passes all good for libs checks such as, gcc, gfortran, gcc+gfortran, perl, csh.</description>
    </item>
    
    <item>
      <title>WRF CHEM compile QN CT</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_ct/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_ct/</guid>
      <description>####Compile wrf chem in CubieTruck CT: QUICK NOTE####
 It is based on the NCAR wrf chem compile complete process and note on wrf chem compile by this Loaded a ubuntu trusty based image from here. Solved the issue of no Internet in CT by editing the interfaces file as follows
 # interfaces(5) file used by ifup(8) and ifdown(8) # Include files from /etc/network/interfaces.d: source-directory /etc/network/interfaces.d auto lo eth0 iface lo inet loopback auto eth0 iface eth0 inet static address 192.</description>
    </item>
    
    <item>
      <title>WRF CHEM compile QN PL</title>
      <link>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_pl/</link>
      <pubDate>Mon, 29 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/wrf_chem_compile_qn_pl/</guid>
      <description>####Compile wrf chem in Parallella PL: QUICK NOTE####
 It is based on the NCAR wrf chem compile complete process and note on wrf chem compile by this Loaded a ubuntu trusty based image from here. Solved the issue of no Internet in PL by editing the interfaces file as follows and changing the files networkmanager.onf and remove net rules as stated in Parallella up and run note.
 # interfaces(5) file used by ifup(8) and ifdown(8) # Include files from /etc/network/interfaces.</description>
    </item>
    
    <item>
      <title>Print map shape file python</title>
      <link>/working-notes/2014/wn_2014-12/print_map_shape_file_python/</link>
      <pubDate>Wed, 17 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/print_map_shape_file_python/</guid>
      <description>####Create and print map using pandas in python from SHAPE file#####
 Based on this note to print map from shape file, a sort of scripting map creation and printing without using qgis kind GUI. The above note uses fiona lib. Intsallation of fiona is super easy with anaconda. Command conda install fiona does all the jobs of installing with all its depndancy starting from gdal. Native installtion of gdal and linkingit with python libs are super hard.</description>
    </item>
    
    <item>
      <title>Parallella upand run</title>
      <link>/working-notes/2014/wn_2014-12/parallella_upand_run/</link>
      <pubDate>Wed, 10 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-12/parallella_upand_run/</guid>
      <description>####Parallellaupand_run#### 1. Ubunut image for parallella can be downloaded from this note, it was dd&amp;rsquo;d into 8GB SD card micro. 1. The BOOT foilder content was not as such of this note,these files were making problem related with rebooting of the parallella. For this there is a foruym post states that need of device tree.dtb has to be changed and there is a file was attached with that. Using that file the problem of no rebooting of parallella was solved 1.</description>
    </item>
    
    <item>
      <title>Geonames Pandas Shapefile</title>
      <link>/working-notes/2014/wn_2014-11/geonames_pandas_shapefile/</link>
      <pubDate>Fri, 28 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/geonames_pandas_shapefile/</guid>
      <description>###GEONAMESPandasintoSHape file#### ####For the scripts /script/ /scripts/csvtoshp.py, csvtoshpTEST.py, Geoname.py#### 1. The industry data for emission inventory is in address with its details. To get get the latitude and longitude value of each address has to have a database with address with its latitude and longitude. 1. One such database is Geonames, its country wise data is having smaller amount of data for Coimbatore case, but POSTAL CODE data has more than 641 postal code details with latitude and longitude information.</description>
    </item>
    
    <item>
      <title>Reverse Geocoding</title>
      <link>/working-notes/2014/wn_2014-11/reverse_geocoding/</link>
      <pubDate>Fri, 28 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/reverse_geocoding/</guid>
      <description>####REVERSEGEocdoing#### #####It is for this script /scripts/apiscript.py and jsontopandas.py##### 1. As per the notes on Geonames_Pandas_Shapefile.md, the postal code latitude and longitude do not have high resolution to differentiate between to nearby postal code points. For this need a high-resolution latitude-longitude data, Google Map API gives solution. As per its documentation, giving address, the latitude and longitude json files can be obtained with its API key, based on this and getting the API access key from here.</description>
    </item>
    
    <item>
      <title>Pandas JSON</title>
      <link>/working-notes/2014/wn_2014-11/pandas_json/</link>
      <pubDate>Wed, 26 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/pandas_json/</guid>
      <description>####Pandas to JSON#### #####It is for this script /scripts/PandasJSONscript.py#####
 To convert pandas dataframe into JSON, converted pandas dataframe into csv and then using this script to convert csv into json. But it was not giving satisfactory json file, the fields where completely shuffled. Then used pandas in built function of to_json but it was giving nested json file.  import pandas import pandas as pd d=pd.read_csv(&#39;/home/swl-sacon-dst/Documents/GISE_2013/LAB/Aerocet_DATA/Oct_sample/json/A08102014M.csv&#39;) db=d[[&#39;Time&#39;,&#39;long&#39;,&#39;lat&#39;,&#39;PM1(ug/m3)&#39;,&#39;PM2.5(ug/m3)&#39;,&#39;PM4(ug/m3)&#39;,&#39;PM7(ug/m3)&#39;,&#39;PM10(ug/m3)&#39;,&#39;TSP(ug/m3)&#39;]] db[&#39;geojson&#39;]=&#39;{\\&amp;quot;type\\&amp;quot;:\\&amp;quot;Point\\&amp;quot;,\\&amp;quot;coordinates\\&amp;quot;:[&#39;+db[&#39;long&#39;].astype(str)+&#39;,&#39;+db[&#39;lat&#39;].astype(str)+&#39;]}&#39; db2=db[[&#39;Time&#39;,&#39;PM2.5(ug/m3)&#39;,&#39;PM10(ug/m3)&#39;,&#39;TSP(ug/m3)&#39;,&#39;geojson&#39;]] db2.</description>
    </item>
    
    <item>
      <title>WRFCHEM output PM25PM10</title>
      <link>/working-notes/2014/wn_2014-11/wrfchem_output_pm25pm10/</link>
      <pubDate>Wed, 26 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/wrfchem_output_pm25pm10/</guid>
      <description>####WRF CHEM output into PM25 and PM10####
 To get various variables from WRF OUTPUT following python codes and references were used #####to get WRF output NETCDF into python##### The below code import netcdf file into python and subset the data based on Coimbatore domain of  from netCDF4 import Dataset import numpy as np wrfoutput = &#39;wrfout_d01_2014-06-05_05:00:00_D03&#39; fh = Dataset(wrfoutput, mode=&#39;r&#39;) #to view all the variables in imported wrf output netcdf vars = fh.</description>
    </item>
    
    <item>
      <title>Ubunut1204 recovery</title>
      <link>/working-notes/2014/wn_2014-11/ubunut1204_recovery/</link>
      <pubDate>Sat, 22 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/ubunut1204_recovery/</guid>
      <description>###Ubunut 12.04 recovery###
 Changing/ editing the file /etc/environment has a huge problem after a reboot in Ubuntu. Added the environment variables for ncl language in /etc/environment as export NCARG_ROOT=/usr/local/ncl-6.2.1; export PATH=$NCARG_ROOT/bin:$PATH. Which resultant in the problematic reboot of the system. The system gets instead of into login page, gets flickering command log. By reboot went inside of command line and there no system command like ls or sudo was not working and indicating no /bin folder in PATH environment.</description>
    </item>
    
    <item>
      <title>NCL install ubunut1204</title>
      <link>/working-notes/2014/wn_2014-11/ncl_install_ubunut1204/</link>
      <pubDate>Thu, 20 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/ncl_install_ubunut1204/</guid>
      <description>http://www.ncl.ucar.edu/Download/install.shtml</description>
    </item>
    
    <item>
      <title>Plot Aerocet sample D3JS</title>
      <link>/working-notes/2014/wn_2014-11/plot_aerocet_sample_d3js/</link>
      <pubDate>Sun, 16 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/plot_aerocet_sample_d3js/</guid>
      <description>####Plot aerocet data with D3 JS scrip####
 Based on this and its source code. One index.html file combines the code for the meteor web application. Web map marker codes are in the file JS drawMap.js. The index.html was edited to remove the CHART in line 14 to 19 contains  &amp;lt;div id=&amp;quot;year-chart&amp;quot; class=&amp;quot;chart display&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;title&amp;quot;&amp;gt;Year of Impact&amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div id=&amp;quot;mass-chart&amp;quot; class=&amp;quot;chart display&amp;quot;&amp;gt; &amp;lt;div class=&amp;quot;title&amp;quot;&amp;gt;Mass (g)&amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;   IN drawMap.</description>
    </item>
    
    <item>
      <title>git delete routine</title>
      <link>/working-notes/2014/wn_2014-11/git_delete_routine/</link>
      <pubDate>Sat, 15 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/git_delete_routine/</guid>
      <description>####git delete routine####
 Completely remove or delete a committed file in git repo require following steps. Even though the file gets removed with delete button option of OS. The committed files are still inside the git version control system, and it will make problem in uploading it into GitHub if it is large file size &amp;gt;100MB accidentally committed. First to list the files deleted in the OS but not in git system, based on this.</description>
    </item>
    
    <item>
      <title>Redeploy Dylos TDM</title>
      <link>/working-notes/2014/wn_2014-11/redeploy_dylos_tdm/</link>
      <pubDate>Wed, 05 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/redeploy_dylos_tdm/</guid>
      <description>###Redeploy Dylos in TDM###
 After running with the problem of excessive particle reading in UPS back up power, this note discusses its redeployment in the same station. The problem of UPS back up excessive power reading was contained by using pure sine wave UPS. In redeployment as an addition, added DHT11, humidity temperature sensor along with Dylos, RPi, USB data card setup. Attached the DHT 11 humidity temperature sensor with RPI following this as per the below .</description>
    </item>
    
    <item>
      <title>DOCX table into python</title>
      <link>/working-notes/2014/wn_2014-11/docx_table_into_python/</link>
      <pubDate>Sat, 01 Nov 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-11/docx_table_into_python/</guid>
      <description>###Docx table into pandas#### ####FOr this script /scripts/DOCXPANDASSEARCH.py#### 1. to import the table in docx format has to use window based python library pywin32. Instead of this the table can be all select, copy paste in Libre calculator and save it as csv. 1. This CSV file will have one blank line after every row as of DOCX table. To remove this row use this script, edited with considering the special delimiter used to silence the &amp;lsquo;,&amp;rsquo; in the address column.</description>
    </item>
    
    <item>
      <title>plot NETCDF</title>
      <link>/working-notes/2014/wn_2014-10/plot_netcdf/</link>
      <pubDate>Fri, 31 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-10/plot_netcdf/</guid>
      <description>###Plot NETCDF and create xlsx file for pyWRFChemEmiss####
 The 2010 anthropogenic emission file of Asian region is available in NetCDF format. To use it with WRF CHEM, PREPCHEMSRC is limited use mainly because the emission source has to specify, and its conversion factor might not be correct. Complexity/understandability is case for similar emission converter program such as Air Emissions Processor program and SMOKE. In this regard pyWRFChemEmiss give a more modular and easy interface to convert emission file to be used with wrf chem.</description>
    </item>
    
    <item>
      <title>Excel Pandas LATEX PDF</title>
      <link>/working-notes/2014/wn_2014-10/excel_pandas_latex_pdf/</link>
      <pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-10/excel_pandas_latex_pdf/</guid>
      <description>###CSV data into pandas and LATEX then into PDF and then print!###
 To import csv file with date into pandas data frame. Pandas can parse the date column, but it won&amp;rsquo;t be correct. The date column formate has top be mentioned and parsed for this, like below based on [this], then this can be imported into pandas data frame indicating the date parser.  dateparse = lambda x: pd.datetime.strptime(x, &amp;lsquo;%d/%m/%y&amp;rsquo;) pro=pd.</description>
    </item>
    
    <item>
      <title>PythonGDALproblem</title>
      <link>/working-notes/2014/wn_2014-10/pythongdalproblem/</link>
      <pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-10/pythongdalproblem/</guid>
      <description>####Python GDAL problem####
ImportError: /usr/lib/libgdal.so.1: undefined symbol: sqlite3columntable_name
main.gdalconfigerror: [Errno 2] No such file or directory
sudo apt-cache showpkg sqlite3
ogrinfo grid.shp -dialect sqlite -sql &amp;ldquo;select sqlite_version()&amp;rdquo;
ldconfig -p
install.packages(filenameand_path, repos = NULL, type=&amp;ldquo;source&amp;rdquo;)
http://marc.info/?l=grass-dev&amp;amp;m=138736238422272&amp;amp;w=2
http://askubuntu.com/questions/443379/sqlite-header-and-source-version-mismatch
http://stackoverflow.com/questions/16095942/sqlite-header-and-source-version-mismatch/16366457#16366457
for R based NetCDF view
https://gist.github.com/xuanlongma/5874674
http://stackoverflow.com/questions/11319698/how-to-install-r-packages-rnetcdf-and-ncdf-on-ubuntu
install.packages(repos=c(&amp;lsquo;http://cran.fhcrc.org/&#39;),pkgs=c(&#39;ncdf&#39;),lib=&amp;quot;/usr/lib/R/site-library/&amp;quot;,configure.args=&amp;quot;--with-netcdf-include=/usr/local/include &amp;ndash;with-netcdf-lib=/usr/local/lib&amp;rdquo;)
1.correcting the grid of Coimbatore urban 1</description>
    </item>
    
    <item>
      <title>Timeseries Pandas</title>
      <link>/working-notes/2014/wn_2014-10/timeseries_pandas/</link>
      <pubDate>Tue, 28 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-10/timeseries_pandas/</guid>
      <description>###Time series data plot and table creation with pandas and latex###
 to import the csv file into pandas df = pd.read_csv(&#39;/home/swl-sacon-dst/Documents/GISE_2013/LAB/Aerocet_DATA/TDM/TDM_MASS_20102014_171059-073359.csv&#39;), based on this To sepcifiy date time index in the dateframe df = df.set_index(pd.DatetimeIndex(df[&#39;Time&#39;])), based on this To resample 1 minute data into 15 minutes by avergae method, bars=df.resample(&#39;15min&#39;), here default method is mean. based on this To select specific columns in pandas df1=df[[&#39;Time&#39;,&#39;PM2.5(ug/m3)&#39;,&#39;PM10(ug/m3)&#39;,&#39;TSP(ug/m3)&#39;,&#39;AT(C)&#39;,&#39;RH(%)&#39;]] based on this To plot datframe by import matplotlib.</description>
    </item>
    
    <item>
      <title>Cubietruck cluster WRFCHEM</title>
      <link>/working-notes/2014/wn_2014-10/cubietruck_cluster_wrfchem/</link>
      <pubDate>Sat, 25 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-10/cubietruck_cluster_wrfchem/</guid>
      <description>####cubie truck cluster for WRF CHEM####
This note explains the steps in creating Cubietruck cluster for WRF CHEM modeling.
#####Installing debian##### 1. Cubietruck is having Android OS preinstalled as its main os for its inbuilt nand storage of 8 GB hard disk. To change this into Debian os following steps were followed. 1. Made several attempts for running Debian or ubuntu os image in Cubietruck. Later found a tweaked image from here at first sight shows promising for WRF CHEM.</description>
    </item>
    
    <item>
      <title>Aerocet sampleData KML Pandas</title>
      <link>/working-notes/2014/wn_2014-10/aerocet_sampledata_kml_pandas/</link>
      <pubDate>Wed, 15 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-10/aerocet_sampledata_kml_pandas/</guid>
      <description>####Aerocet Sampling Data, using kml files and pandas####
 The data from dust pollution monitoring in Coimbatore using Aerocet was copy paste into a csv file from miniterm.py program and commanded 2 given to aerocet. Made separate CSV files for count and mass mode sampling. Sampling point locations made into a point feature in Google earth with the name of respective sampling time. Further saved as kml file and opened in QGIS, converted into shapefile to add geometry.</description>
    </item>
    
    <item>
      <title>Dylos RPi Serial problem</title>
      <link>/working-notes/2014/wn_2014-10/dylos_rpi_serial_problem/</link>
      <pubDate>Tue, 07 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-10/dylos_rpi_serial_problem/</guid>
      <description>####DYLOS RPi serial Problem#### 1. RPi(Raspberry pi) has a problem in the serial read of dylos air quality monitor data. The serial read ends with wrong or misplaced data from the dylos monitor, this problem is mentioned in this (note)(Serialportproblemlbm1knmr.md). For example, running of the python command for getting dylos serial read ends as follows
 pi@raspberrypi ~/SMS $ python &amp;gt;&amp;gt;&amp;gt; import serial &amp;gt;&amp;gt;&amp;gt; ser = serial.Serial(&#39;/dev/dylos&#39;, 9600, timeout=60) &amp;gt;&amp;gt;&amp;gt; line = ser.</description>
    </item>
    
    <item>
      <title>WRFCHEM output python</title>
      <link>/working-notes/2014/wn_2014-09/wrfchem_output_python/</link>
      <pubDate>Mon, 29 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/wrfchem_output_python/</guid>
      <description>###WRFCHEM_output pythonic####
 Related to the last note on getting txt time series data from wrf output here. This note details getting numpy array from wrf output and plotting data in python. The variables required from wrfoutput are Temperature, Relative humidity, Wind direction, wind speed, PM2.5 and PM 10. Numpy array from these variables will be used for air earth application, to insert the grid data into Postgresql. Data plots is for a quick view of the wrf output.</description>
    </item>
    
    <item>
      <title>WRFCHEM CBE lazyWRF py</title>
      <link>/working-notes/2014/wn_2014-09/wrfchem_cbe_lazywrf_py/</link>
      <pubDate>Thu, 25 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/wrfchem_cbe_lazywrf_py/</guid>
      <description>###Use script to execute WRF CHEM CBE### 1. To run in real time and in cluster environment(AWS or ARM), various steps of WRF have to be made into script. 1. There are several python based scripts avaiable for running WRF. The script lazyWRF is a neat and simple script to run wrf from WPS stage. 1. The script has minor problem in running, it was forked from its git hub repository and edited.</description>
    </item>
    
    <item>
      <title>compileGSIonUbuntu1404</title>
      <link>/working-notes/2014/wn_2014-09/compilegsionubuntu1404/</link>
      <pubDate>Sat, 20 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/compilegsionubuntu1404/</guid>
      <description>###compile GSI in Ubunut 14.04###
 GSI was tried to compile in Ubuntu 12.04, but ended with failure. GSI requires Gfortran 4.7 and above, but 12.04 repository for up to 4.6, so ubuntu has to upgraded from 12.04 to 14.04 to have gfortran 4.7. Due to this Server was upgraded from Ubuntu 12.04 to 14.04 and compiled WRFV4.3.1 and WPS 4.3.1 in it. It was by Gfortran 4.8.2, Netcdf 4.1.3, and other necessary libraries such as libpng, zlib and jasper as per apt-get method.</description>
    </item>
    
    <item>
      <title>EmisInv REAS Pandas</title>
      <link>/working-notes/2014/wn_2014-09/emisinv_reas_pandas/</link>
      <pubDate>Sun, 14 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/emisinv_reas_pandas/</guid>
      <description>###Working on REAS emission inventory to feed into WRF-CHEM### 1. REAS is an emission inventory for Asia, it has separate files for different pollutant with each category of emission source. For example, pollutant SO2 has 9 data files for each emission source types such as Aviation, Domestic, Industry etc for the year 2008.
1. The files are space separated text files having fields (columns) for longitude, latitude and monthly emission value 1.</description>
    </item>
    
    <item>
      <title>Aerocet531S serial read</title>
      <link>/working-notes/2014/wn_2014-09/aerocet531s_serial_read/</link>
      <pubDate>Wed, 10 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/aerocet531s_serial_read/</guid>
      <description>####Serial read from Aerocet 531S###
 Aerocet 531S(A531S) has serial communication and commanding interface through USB connection. By connecting USB to Linux ubunut 12.04, running the program serial port terminal and giving the command such as S starts the A531S. These commands are elaborated in its user manual. There number of other commands to view the data and get more about the A531S operation as per its user manual. To make the process of running the machine and entering the command in Python to start the machine or read output is the aim of this note.</description>
    </item>
    
    <item>
      <title>WRF CHEM Compile completenote SERVER</title>
      <link>/working-notes/2014/wn_2014-09/wrf_chem_compile_completenote_server/</link>
      <pubDate>Wed, 10 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/wrf_chem_compile_completenote_server/</guid>
      <description>###Reinstall due to GSI netcdf tested requirement###
 Compiled netcdf4.2.1.1 based on the [compile.wrf] but it didn&amp;rsquo;t generating nay netcdf.inc as needed by the wrf fortran code and so fails in netcdf gfortran testing stage. Another option is to use netcdf3.6.3, and it gets compiled and generated netcdf.inc, the codes used for compilations are, export CC=gcc export CXX=g++ export FC=gfortran export FCFLAGS=-m64 export F77=gfortran export FFLAGS=-m64
./configure &amp;ndash;prefix=$DIR/netcdf36 make make install  with this netcdf version, wrfV4.</description>
    </item>
    
    <item>
      <title>PM Emission Inventory CBE SD</title>
      <link>/working-notes/2014/wn_2014-09/pm_emission_inventory_cbe_sd/</link>
      <pubDate>Tue, 09 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/pm_emission_inventory_cbe_sd/</guid>
      <description>####PMEmissionInventoryCoimbatoreStudy design####
Particulate pollution level and vehicular count sample 1. A sample consists of 3 minutes of PM pollution monitoring(1-minute particle count mode, 2-minute mass mode) and 2-minute video recording. 1. Sampling carried on a CNG auto rickshaw, equipped with handheld PM monitor and a web camera based video recorder attached to right side at 0.75 meters from ground of Autorickshaw.
1. Sixty random points in Coimbatore urban limits, sampling every 4th day for 20 sampling days.</description>
    </item>
    
    <item>
      <title>WRFCHEM CBE aOptionwithAWS StarCluster</title>
      <link>/working-notes/2014/wn_2014-09/wrfchem_cbe_aoptionwithaws_starcluster/</link>
      <pubDate>Tue, 09 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/wrfchem_cbe_aoptionwithaws_starcluster/</guid>
      <description>####WRF-CHEM for Coimabtore: An option with AWS and starcluster####
 Considering the huge time requirement for running wrf chem over Coimbatore with high resolution (It roughly took 8 hour 15 minutes X 7 for domain 4 alone, 6 second time steps with average 50 seconds), Amazon Web Service (AWS) t2.medium instace and StarCluster compute cluster would be a viable option to make wrf-chem run in &amp;lsquo;real time&amp;rsquo;. ####AWS charges for a typical cluster####  ####upgrading aws free tier into ubunut 14.</description>
    </item>
    
    <item>
      <title>WRFCHEM CBE A1</title>
      <link>/working-notes/2014/wn_2014-09/wrfchem_cbe_a1/</link>
      <pubDate>Mon, 08 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/wrfchem_cbe_a1/</guid>
      <description>###WRF CHEM for Coimbatore A1 using ndown.exe###
####WPS#### 1. The WPS components geogrid.exe, ungrib.exe and metgrid.exe was from the output of last run on detailed in this
####real.exe#### 1. Real.exe also from the last run output. The output comprised of files namely wrfbdyd01, wrfinputd01, wrfinputd02, wrfinputd03, wrfinputd04 were used as such for convertemiss.exe run.
####PrepChemSrc.exe#### 1. It was executed as per the tutorial and PrepChemSrc.exe was executed by following point 3 in that tutorial.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part70 CBE DustOnlyTutorial</title>
      <link>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part70_cbe_dustonlytutorial/</link>
      <pubDate>Sat, 06 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part70_cbe_dustonlytutorial/</guid>
      <description>##Trail for WRF-CHEM, WRF CHEM simulation with dust only for Coimbatore using TWO WAY NESTING TWO files method## 1. It is based on this to run compiled wrf chem in serial mode with only dust.
###WPS:Geogrid### 9. As per this point 7, file GEOGRID.TBL.ARW_CHEM in WPS geogrid directory was linked as GEOGRID.TBL using GUI of Ubuntu 12.04. 10. The namelist.wps used was as follows,
 &amp;amp;share max_dom = 4, start_date =&#39;2014-06-05_00:00:00&#39;,&#39;2014-06-05_00:00:00&#39;,&#39;2014-06-05_00:00:00&#39;,&#39;2014-06-05_00:00:00&#39; end_date =&#39;2014-06-05_06:00:00&#39;,&#39;2014-06-05_06:00:00&#39;,&#39;2014-06-05_06:00:00&#39;,&#39;2014-06-05_06:00:00&#39; interval_seconds = 10800, io_form_geogrid = 2, / &amp;amp;geogrid parent_id = 1, 1, 2, 3 parent_grid_ratio = 1, 3, 3, 3 i_parent_start = 1, 26, 22, 22 j_parent_start = 1, 7, 15, 32 e_we = 90, 76, 97, 136 e_sn = 85, 73, 106, 157 geog_data_res = &#39;10m&#39;, &#39;5m&#39;, &#39;30s&#39;, &#39;30s&#39; dx = 27000 dy = 27000 map_proj = &#39;lambert&#39; ref_lat = 18.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part71 emission inventory CBE Ndown</title>
      <link>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part71_emission_inventory_cbe_ndown/</link>
      <pubDate>Sat, 06 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/working_with_wrf_chem_part71_emission_inventory_cbe_ndown/</guid>
      <description>###WRF CHEM for Coimbatore domain with emission inventory and one way nesting using ndown.exe###
####WPS#### 1. The WPS components geogrid.exe, ungrib.exe and metgrid.exe was from the output of last run on detailed in this
####real.exe#### 1. Real.exe also from the last run output. The output comprised of files namely wrfbdyd01, wrfinputd01, wrfinputd02, wrfinputd03, wrfinputd04 were used as such for convertemiss.exe run.
####PrepChemSrc.exe#### 1. It was executed as per the tutorial and PrepChemSrc.</description>
    </item>
    
    <item>
      <title>WRFCHEM domain view python</title>
      <link>/working-notes/2014/wn_2014-09/wrfchem_domain_view_python/</link>
      <pubDate>Thu, 04 Sep 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-09/wrfchem_domain_view_python/</guid>
      <description>####WRF_CHEM domain view by python script####
 WRF CHEM domain in nested has to be visualized to correct the nesting and for domain size adjustment. There are few tools available to do that such as dwiz of wrf ems and plotgrid.ncl. There is an alternative with python based on this, it is a wrf script tools for visualizing and nesting based simulation. Set of scripts available has a function to visualize the namelist.</description>
    </item>
    
    <item>
      <title>GSI assimilation PM25PM10 WRFCHEM</title>
      <link>/working-notes/2014/wn_2014-08/gsi_assimilation_pm25pm10_wrfchem/</link>
      <pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/gsi_assimilation_pm25pm10_wrfchem/</guid>
      <description>###PM10,PM2.5 observation assimilation for WRF CHEM based on GSI###
based on this paper Implementation of aerosol assimilation in Gridpoint Statistical Interpolation (v. 3.2) and WRF-Chem (v. 3.4.1)
http://www.geosci-model-dev.net/7/1621/2014/gmd-7-1621-2014.pdf</description>
    </item>
    
    <item>
      <title>SOSforWRFCHEM output</title>
      <link>/working-notes/2014/wn_2014-08/sosforwrfchem_output/</link>
      <pubDate>Sat, 30 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/sosforwrfchem_output/</guid>
      <description>###Sensor Observation service for WRF CHEM output###</description>
    </item>
    
    <item>
      <title>WRF CHEM compilingInARM BeagleBoneBlack</title>
      <link>/working-notes/2014/wn_2014-08/wrf_chem_compilinginarm_beagleboneblack/</link>
      <pubDate>Sat, 23 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/wrf_chem_compilinginarm_beagleboneblack/</guid>
      <description>###WRF CHEM compiling in ARM architecture, Beagle Bone Black###
This is based on http://www2.mmm.ucar.edu/wrf/OnLineTutorial/index.php ####compiler test#### went successful, except in test4 using 64 bit ###libraries### ####Netcdf compiling#### 1. Environment has to be properly setup, following was correct for ARM arch, clarified based on this
 export DIR=/media/card/wrf/lib export CC=gcc export CXX=g++ export FC=gfortran export FCFLAGS=-g export F77=gfortran export FFLAGS=-g export CXXFLAGS=-g   Faced error in make after configure command of .</description>
    </item>
    
    <item>
      <title>Parallella for WRF CHEM</title>
      <link>/working-notes/2014/wn_2014-08/parallella_for_wrf_chem/</link>
      <pubDate>Thu, 21 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/parallella_for_wrf_chem/</guid>
      <description>###Parallella cluster for WRF CHEM### 1. WRF-CHEM is a computationally intensive model, for example, &amp;ldquo;On a typically-sized 40×40 grid with 20 horizontal layers, the meteorological part of the simulation (the WRF weather model itself) is only 160 × 106 floating point operations per time step, about 2.5% the cost of the full WRF-Chem with both chemical kinetics and aerosol&amp;rdquo;[1]. It is generally executed with parallel mode to reduce time latency of model execution time.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part63 WRF CoimbatoreNesting</title>
      <link>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part63_wrf_coimbatorenesting/</link>
      <pubDate>Tue, 19 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part63_wrf_coimbatorenesting/</guid>
      <description>##Trail for WRF-CHEM, WRF simulation with domain for Coimbatore and one-way Nesting-Ndown.exe## 1. The trail is based on this page to run compiled WRF in serial mode to execute domain over Coimbatore. Trying the nested model run by Ndown.exe using one-way nesting.
###WPS: Ungrib###
 The WPS ungrib stage was carried out as per page, The gfs files downloaded used, average size around 45MB for each file. The data for period 00,03, 06 was kept under a folder gfs.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part64 DustOnlyTutorial</title>
      <link>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part64_dustonlytutorial/</link>
      <pubDate>Tue, 19 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part64_dustonlytutorial/</guid>
      <description>##Trail for WRF-CHEM, WRF CHEM simulation with dust only tutorial## 1. It is based on this to run compiled wrf chem in serial mode with only dust.
###WPS:Geogrid### 9. As per this point 7, file GEOGRID.TBL.ARW_CHEM in WPS geogrid directory was linked as GEOGRID.TBL using GUI of ubuntu 12.04. 10. The namelist.wps from the page was used. The namelist.wps is as follows,
 &amp;amp;share wrf_core = &#39;ARW&#39;, max_dom = 1, start_date = &#39;2010-07-14_00:00:00&#39;,&#39;2010-07-14_00:00:00&#39;, end_date = &#39;2010-07-19_00:00:00&#39;,&#39;2010-07-19_00:00:00&#39;, interval_seconds = 10800, io_form_geogrid = 2, / &amp;amp;geogrid parent_id = 1, 1, 1, parent_grid_ratio = 1, 5, 5, i_parent_start = 1, 6, 105, j_parent_start = 1, 65, 25, e_we = 41, 201, 226, e_sn = 41, 311, 231, geog_data_res = &#39;10m&#39;, &#39;2m&#39;, &#39;30s&#39; dx = 100000, dy = 100000, map_proj = &#39;lambert&#39;, ref_lat = 35.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part60 dustonly CBE</title>
      <link>/working-notes/2014/wn_2014-08/working-with-wrf-chem-part60-dustonly-cbe/</link>
      <pubDate>Tue, 12 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working-with-wrf-chem-part60-dustonly-cbe/</guid>
      <description>In a measure to make the compiled executables reuse, made a copy of WPS folder and WRFv3, parallel compiled folder test/real_em* into a new folder of cbe_domain. The link gets broken, and no correct files were copied by doing mere copy paste. Instead, a try was made as per the m2lab tutorial and copied the files from em_real as like this cp /em_real/* ., this time the link was not made, but the executables get copied, it has to check whether it is working For starting WPS of WRF with geogrid, used the namelist.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part61 WRF KatrinaSingleDomain</title>
      <link>/working-notes/2014/wn_2014-08/working-with-wrf-chem-part61-wrf-katrinasingledomain/</link>
      <pubDate>Tue, 12 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working-with-wrf-chem-part61-wrf-katrinasingledomain/</guid>
      <description>Trail for WRF-CHEM, working with WRF simulation with Katrina single domain case
 It is based on this to run compiled WRF in serial mode to execute the Katrina case with a single domain. As per this page, the WPS ungrib stage was carried out  WPS: Ungrib
 The tar file provided with the tutorial was unzipped and compiled WPS folder was copied and bothe these folders were kept under a folder named katrina.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part62 WRF KatrinaNesting</title>
      <link>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part62_wrf_katrinanesting/</link>
      <pubDate>Tue, 12 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/working_with_wrf_chem_part62_wrf_katrinanesting/</guid>
      <description>##Trail for WRF-CHEM, WRF simulation with Katrina case and one-way Nesting-Ndown.exe## 1. Based on this to run compiled wrf in serial mode to execute the Katrina case with the nested model run by Ndown.exe based one-way nesting.
###WPS: Ungrib###
 As per this page, the WPS ungrib stage was carried out. Unzipped the tar file provided with the tutorial. Copied WPS folder both under a shared folder named as katrina_nesting. The vtable was linked inside the WPS folder by ln -sf ungrib/Variable_Tables/Vtable.</description>
    </item>
    
    <item>
      <title>Insert Observation in istSOS xml</title>
      <link>/working-notes/2014/wn_2014-08/python-script-insertobservation-istsos-xml/</link>
      <pubDate>Wed, 06 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/python-script-insertobservation-istsos-xml/</guid>
      <description>Based on the early experience of inserting observation in xml way with python using HTTP post in 52North SOS, made a try with Ist SOS, another way to do is using calling another python script to import data in CSV format. Based on this note on OWS service requests. to get the DescribeSensor information used this link, it always failed in specifying formate, but another request was successful
http://54.255.173.125/istsos/cbed?request=describeSensor&amp;amp;procedure=KNMR&amp;amp;outputFormat= text xml;subtype=&amp;quot;sensorML/1.</description>
    </item>
    
    <item>
      <title>Find the tense and voices in a sentence by English grammar checker</title>
      <link>/working-notes/2014/wn_2014-08/english-grammer-checker/</link>
      <pubDate>Fri, 01 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-08/english-grammer-checker/</guid>
      <description>Search made to find various type of tense in a sentence. Ended in this, page. Stanford core NLP and NLTK with python are good natural language processors used for this purposes. NLP gives a statistical measure of the possible role of words in a sentence, Standford NLP is giving a web service with visualization of various statical measures in part-of-speech tags. Its grammatical state is inferred using the Penn tags, yes machine based!</description>
    </item>
    
    <item>
      <title>The Github pages, HTML, css and js</title>
      <link>/working-notes/2014/wn_2014-07/gh-pages-html-css-js/</link>
      <pubDate>Tue, 29 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/gh-pages-html-css-js/</guid>
      <description> The files HTML, css and js, is enough for creating a web site. In GitHub this is detrimental in any gh-pages intended to create. There are several templates available to render web pages A simple markdown template used is, . https://github.com/aplib/markdown-site-template Final usage of this web site was selected based on its demonstration  </description>
    </item>
    
    <item>
      <title>Latex beamer based pdf presentation</title>
      <link>/working-notes/2014/wn_2014-07/latex-beamer-based-pdf-presentation/</link>
      <pubDate>Fri, 18 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/latex-beamer-based-pdf-presentation/</guid>
      <description> For creating a presentation with many images in it, better to use latex based presentation It is easier than ppt with tight control on formatting and content referencing The presentation is based on this note and chosen Berlin theme within beamer&amp;rsquo;s n number of various themes. The main problem faced is with BibTeX based referencing, and could not be solved and used normal superscript referencing. Another problem faced was box and its colour choosing, it was solved following this excellent tutorial  </description>
    </item>
    
    <item>
      <title>Workflow with Docear, Mendely and Python</title>
      <link>/working-notes/2014/wn_2014-07/workflow-docear-mendely-python/</link>
      <pubDate>Fri, 18 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/workflow-docear-mendely-python/</guid>
      <description>A script to select pdf files from a bunch of folders, subfolders, html files and create folders with only pdf files, the code is
import os count = 0 d=[] f=[] for (dirname, dirs, files) in os.walk(&#39;/home/swl-sacon-dst/Documents/GISE_2013/LAB/lab_notes/&#39;): #sepecifiying the directory to search for pdf files [from](http://stackoverflow.com/questions/273192/check-if-a-directory-exists-and-create-it-if-necessary) for filename in files: if filename.endswith(&#39;.pdf&#39;) : thefile = os.path.join(dirname,filename) dire = os.path.dirname(thefile) f.append(thefile) d.append(dire) #makung a list of ifle names and folders for copying #using list comphrehsnsion to change the folder path [from](http://stackoverflow.</description>
    </item>
    
    <item>
      <title>Checkng Null in Python for loop</title>
      <link>/working-notes/2014/wn_2014-07/check_null_in_python/</link>
      <pubDate>Tue, 15 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/check_null_in_python/</guid>
      <description>Checking for null in python
 The Dylos montior, connected with tp-link and to upload data to thinkspeak internet of things services, following script is used,
import sqlite3 as lite import logging import httplib, urllib from time import localtime, strftime import time logger = logging.getLogger(&#39;lbm1&#39;) hdlr = logging.FileHandler(&#39;/home/pi/SMS/pyts.log&#39;) formatter = logging.Formatter(&#39;%(asctime)s: %(levelname)s %(message)s&#39;) headers = {&amp;quot;Content-type&amp;quot;: &amp;quot;application/x-www-form-urlencoded&amp;quot;,&amp;quot;Accept&amp;quot;: &amp;quot;text/plain&amp;quot;} conn = httplib.HTTPConnection(&amp;quot;api.thingspeak.com:80&amp;quot;) hdlr.setFormatter(formatter) logger.addHandler(hdlr) logger.setLevel(logging.INFO) logger.info(&amp;quot;tss&amp;quot;) con = lite.connect(&#39;/home/pi/SMS/dylos.db&#39;) try: with con: cur = con.</description>
    </item>
    
    <item>
      <title>Serial port problem in Dylos Air quality monitor</title>
      <link>/working-notes/2014/wn_2014-07/serial-port-problem-lbm1-knmr/</link>
      <pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/serial-port-problem-lbm1-knmr/</guid>
      <description>A monitor is set up with three three-pin plugs extension box serially wired, and connected to another serially connected extension box with two pin plug. In which three plugs, one Dylos air quality monitor is connected, another 2A 7port USB hub adapter is connected and third and final pin from right to left, TP-link 3G router is connected. With the above setup, router LAN connected with raspberry pi which connected with 7hub USB, Dylos serial port connected with RPI, RPi accessed via wireless connectivity through a router.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part4 cbe domain run</title>
      <link>/working-notes/2014/wn_2014-07/working_with_wrf_chem_part4_cbe_domain_run/</link>
      <pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/working_with_wrf_chem_part4_cbe_domain_run/</guid>
      <description>Coimbatore domain made with four nests has been run with WRF-EMS with resolution starting from 100&amp;gt;27&amp;gt;9&amp;gt;3&amp;gt;1km. The domain is made using dwiz application in WRF-EMS. However, to test the domain above the Coimbatore region, the output was subject for wrfncxnj.py utility. It took 4 hours and 33 minutes to run this domain in IBM X4 series server computer with 16 GB ram. To overlay the Coimbatore city shape file above the WRF output NetCDF from wrfcnxnj.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part5 prep emis</title>
      <link>/working-notes/2014/wn_2014-07/working-with-wrf-chem-part5-prep-emiss/</link>
      <pubDate>Sun, 13 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/working-with-wrf-chem-part5-prep-emiss/</guid>
      <description>Running prep-chem-src
 Based on part 1 prep-chem-SRC was compiled and ready to execute for emission inventory creation. Following WRF-chem Nepal tutorial and PREP-CHEM-SRC, README edited prep_chem_sources.inp. Then run the program by executing ./prep_chem_sources_RADM_WRF_FIM.exe, it ran for some steps but exited with error of Warning! ***HDF5 library version mismatched error*** saying the HDF5 version used in compiling is 1.8.8 and version for running PREP-CHEM-SRC is 1.8.12 To check what hdf5 is used by prepchesrc, run a command h5dump -V which gives h5dump: Version 1.</description>
    </item>
    
    <item>
      <title>LAN based Internet problem and th Raspberry Pi and solution</title>
      <link>/working-notes/2014/wn_2014-07/lan_based_internet_problem_and_solutionforraspberry/</link>
      <pubDate>Wed, 02 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-07/lan_based_internet_problem_and_solutionforraspberry/</guid>
      <description>LAN based internet for Raspberry pi (RPi) is not on the go, the netsat, network interfaces has to be edited, and also based on the gateway of server it has to be changed The current case used server was TP-LINK MR3420, it is a single small board computer based 3G modem, wireless device, its gateway address is 192.168.0.254. A 3G dongle connected with device and LAN from this device connected to RPi.</description>
    </item>
    
    <item>
      <title>Textfile from WRFoutput using Python</title>
      <link>/working-notes/2014/wn_2014-06/textfile-from-wrfoutput-pythonic/</link>
      <pubDate>Sat, 28 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/textfile-from-wrfoutput-pythonic/</guid>
      <description>To convert wrf output into a text file for the value of a specified lat long Using the script WrfncXnj.py, convert wrf output in cf abiding nc file The steps are as follows, cp wrfout_d04* /home/hoopoe/wrfncxnj-0.1_r2120/. To get the list of available files in a directory
import os a=[] for file in os.listdir(&amp;quot;/home/hoopoe/wrfncxnj-0.1_r2120/&amp;quot;): if file.startswith(&amp;quot;wrfout_d04&amp;quot;): a.append(file)  To run a external python script from another python script,
import subprocess a= wrfout_d04_2014-06-11_00:00:00 subprocess.</description>
    </item>
    
    <item>
      <title>Convert Latex into HTML</title>
      <link>/working-notes/2014/wn_2014-06/latex-into-html/</link>
      <pubDate>Fri, 27 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/latex-into-html/</guid>
      <description> For a workflow with latex and peer editing, latex in PDF has to converted into some editable formate can be used in text editor/word processor Based on this, latex tex files can be directly converted into HTML without going for pdf. Have to use this command htlatex mydocument.tex, but asks to install the program sudo apt-get install tex4ht, so install tex4ht Then execute the command of htlatex mydocument.tex and it produced HTML with css and other required files, this can be open in Libre text editor to make peers to edit or correct the content  </description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part1 PrepChemSrc</title>
      <link>/working-notes/2014/wn_2014-06/working-with-wrf-chem-part1-prepchemsrc/</link>
      <pubDate>Thu, 26 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/working-with-wrf-chem-part1-prepchemsrc/</guid>
      <description>compiling and install prep-chem-src
 To install prep-chem-src, it requires to install HDF5,ZLIB and NETCDF based on the readme of PREPCHEMSRC as follows
Third Party Software Requirements --------------------------------- 1. ZLIB 1.2.5(libz.a) or later distribution. You may download the software from the http://www.zlib.net/ site. 2. HDF5-1.8.8(libhdf5_fortran.a, libhdf5hl_fortran.a) or later distribution. You may download the software from http://www.hdfgroup.org/HDF5/release/obtain5.html. 3. NetCDF 4.1.1 (libnetcdf.a). You may download the software from http://www.unidata.ucar.edu/downloads/netcdf/netcdf-4_0_1/index.jsp Configuring/Installing HDF 5 ------------------------------ When compiling PREP-CHEM-SRC codes on a Linux system using the PGI (Intel) compiler, make sure the netCDF and HDF* library has been installed using the same PGI (Intel) compiler.</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part2 WPS</title>
      <link>/working-notes/2014/wn_2014-06/working-with-wrf-chem-part2-wps/</link>
      <pubDate>Thu, 26 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/working-with-wrf-chem-part2-wps/</guid>
      <description>working with WRF-CHEM: Running WPS
The steps for running WPS as per the m2lab tutorial
Geogrid
 Create namelist and geogrid Copy wps folder and edit namelist for domain Use ncl program to view the domain Reflat and reflon center point of the domain, ewe, esn, number of point in x and y-direction Edit geog file location in name list and geogrid output location Then run geogrid.exe, run geogrid after editing the name list as .</description>
    </item>
    
    <item>
      <title>Working with WRF CHEM part3 compile wrf exe</title>
      <link>/working-notes/2014/wn_2014-06/working-with-wrf-chem-part3-compile-wrf-exe/</link>
      <pubDate>Thu, 26 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/working-with-wrf-chem-part3-compile-wrf-exe/</guid>
      <description>For running wrf.exe after wps, wrf.exe and real.exe is not being made during the compilation of wrf-chem. Made a new compilation of wrf alone to make it work. The compilation involves only setting environment, installing dependent packages and running, ./configure then ./compile em_real and ./compile emi_conv The first step of .configure ended with an error saying cannot find NetCDF in the path, the path specified is the wrong one. The file sudo nano bash.</description>
    </item>
    
    <item>
      <title>Editing shapefile using QGIS</title>
      <link>/working-notes/2014/wn_2014-06/shapefile-edit-qgis/</link>
      <pubDate>Wed, 18 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/shapefile-edit-qgis/</guid>
      <description> To edit multipart shapefile in QGIS into a one outline map Used QGIS multipart into a single feature by selected attributes It converts shapefile with multiple attributes feature(multiple rows) into single rows The resultant shapefile need to be edited to delete any residuals in operation, used delete ring in QGIS for that  </description>
    </item>
    
    <item>
      <title>Installing GDAL with Python in Ubunut1204</title>
      <link>/working-notes/2014/wn_2014-06/installing-gdal-with-python-in-ubunut1204/</link>
      <pubDate>Wed, 18 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/installing-gdal-with-python-in-ubunut1204/</guid>
      <description>Convert WRF output into geotiff. Python script requires gdal such as for this from osgeo import gdal and from osgeo import osr To install it in Ubuntu 12.04, it requires to install gdal-bin, libgdal1 and python-gdal, after installing and running python import of these libs ends up in crashes with this message segmentation error(core dumped) Detected error due to version difference, so installed with deb packages available for the version for 1.</description>
    </item>
    
    <item>
      <title>Reprojecting SHAPE file uing Python</title>
      <link>/working-notes/2014/wn_2014-06/reprojecting-shape-file-python/</link>
      <pubDate>Wed, 18 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/reprojecting-shape-file-python/</guid>
      <description>WRF output netcdf(nc) or wrfncx.py output nc files converted into geotiff(tiff) file with custom projections, see the codes for nc to tiff conversion.
from osgeo import gdal from osgeo import osr import numpy import numpy.ma as ma datafile = &#39;ZZZG3wrfout_psl.nc&#39; proj_out = osr.SpatialReference() proj_out.SetMercator(0.0, 115.02, 0.98931892612652, 0.0, 0.0) ds_in = gdal.Open(datafile) #subdatasets = ds_in.GetSubDatasets() #variables = [] #for subdataset in subdatasets: # variables.append(subdataset[1].split(&amp;quot; &amp;quot;)[1]) ds_lon = gdal.Open(&#39;NETCDF:&amp;quot;ZZZG3wrfout_psl.nc&amp;quot;:lon&#39;) ds_lat = gdal.</description>
    </item>
    
    <item>
      <title>Installing R and Openair in Ubuntu1204</title>
      <link>/working-notes/2014/wn_2014-06/installing-r-and-openair-in-ubuntu1204/</link>
      <pubDate>Tue, 17 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/installing-r-and-openair-in-ubuntu1204/</guid>
      <description>Installation of R in Ubuntu 12.04, there are deb packages available for R from its source page here However, after installation, it shows error in installing openair package, and dependency chain goes from 2 to 10 and much more package with version mismatch problem, starting from lattice saying Error: package ‘lattice’ was built before R 3.0.0: please re-install it Even calling package by the library(lattice&amp;rsquo;) given error So installation in apt-get library might be a problem-solving step, followed these</description>
    </item>
    
    <item>
      <title>Working with CSV using Python Pandas</title>
      <link>/working-notes/2014/wn_2014-06/data-editing-with-pandas/</link>
      <pubDate>Tue, 17 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/data-editing-with-pandas/</guid>
      <description>To import data into python
import pandas as pd data=pd.read_csv(&#39;value.txt&#39;)  To view data head or specified rows
data.iloc[:5, :4]  To sort data based on a specific column, here data column
d2=d1.sort([&#39;observation_time&#39;])  To make a dataetime column recognized as date column in padnas dataframe
d1[&#39;SamplingDate&#39;] = pd.to_datetime(d1[&#39;SamplingDate&#39;])  To remove NaN valued rows in any of the columns of the data frame
 d1=data.dropna()  To reindex data frame with date time columns</description>
    </item>
    
    <item>
      <title>Parsing the WRF logfile in Python</title>
      <link>/working-notes/2014/wn_2014-06/editing-wrf-logfile-python/</link>
      <pubDate>Sun, 08 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/editing-wrf-logfile-python/</guid>
      <description>To get a sum of seconds WRF is running, the wrf.out.log was imported into python to sum the elapsed seconds in each domain To get the log file into wrf bf = open(&#39;run_wrfm.log&#39;, &#39;r&#39;) To read each lines in the file bf_lines=bf.readlines() To make the lines into list array and select only the list with particular words in it, here the word &amp;ldquo;Timing for Writing&amp;rdquo;
f=[] for line in bf_lines: if &#39;Timing for Writing&#39; in line: f.</description>
    </item>
    
    <item>
      <title>Compiling grib2json in AWS</title>
      <link>/working-notes/2014/wn_2014-06/compiling-grib2json-in-aws/</link>
      <pubDate>Fri, 06 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/compiling-grib2json-in-aws/</guid>
      <description>To install the latest java and JDK follow this (http://askubuntu.com/questions/117189/apt-get-install-openjdk-7-jdk-doesnt-install-javac-why) Install the latest maven
wget http://archive.apache.org/dist/maven/binaries/apache-maven-3.0.4-bin.tar.gz tar -zxf apache-maven-3.0.4-bin.tar.gz sudo cp -R apache-maven-3.0.4 /usr/local sudo ln -s /usr/local/apache-maven-3.0.4/bin/mvn /usr/bin/mvn  hen edit .bashrc file and add this line, not working, don&amp;rsquo;t do this
nano .bashrc &amp;gt;&amp;gt;&amp;gt;JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-amd64  Check the installation with mvn version and mvn package gives spurious errors
 Never mind the error, download the grib2json as told here</description>
    </item>
    
    <item>
      <title>Bewolf cluster setup or WRF EMS</title>
      <link>/working-notes/2014/wn_2014-06/bewolf-cluster-for-wrf-ems/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/bewolf-cluster-for-wrf-ems/</guid>
      <description>The four domain running of WRF EMS with resolution starting from 27km, 9km, 3km to 1km gives an error of exit with the status of -9. This showing that it would because of out of memory statusmore from this, this gives motivation further for a cluster running of WRF-EMS Mere passwordless SSH is not sufficient for cluster running of WRF EMS, as specified in run_ncpus.conf, the created cluster has to be checked with netcheck script given with WRF-EMS, it is in strc folder hint So while running netcheck with passwordless ssh, it gives an error that ssh localhostname hostname is not doing passwordless ssh, in the begning, this error seems to be ridiculous, how and what!</description>
    </item>
    
    <item>
      <title>Four Nested WRF-EMS simulation and its log file</title>
      <link>/working-notes/2014/wn_2014-06/wrf-ems-log/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/wrf-ems-log/</guid>
      <description>WRF-EMS installed through a local file in the laptop WRF-EMS was installed by online and script in server WRF-EMS was installed by online and script in server Four nested domain is not running in desktop saying error code 9 during wrf real.exe  Four nested domain run of WRF-EMS above Trivandrum area
 The domain made for Trivandrum tvm-gfs4, after running a test, it gives history output for every 30 minutes and took some four hours to complete To check the domain is covering the Trivandrum districts, after converting the WRF output into NetCDF using wrfncxnj.</description>
    </item>
    
    <item>
      <title>Installing and running the WRF EMS</title>
      <link>/working-notes/2014/wn_2014-06/running-wrf-ems/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/running-wrf-ems/</guid>
      <description>WRF EMS is a pearl scripted implementation of weather research and forecast (WRF) model. Relatively simple scripting implementations, it hides the complex compiling and running steps in the WRF model for operational and research purposes. Its installation is by running a Perl script provided by mail request; the total installation size goes around 22 GB, and so a long process. The first step in model execution in the creation of model domain, for high-resolution forecasting of weather in Coimbatore, has to make a four nested domain.</description>
    </item>
    
    <item>
      <title>Local install tion of WRF-EMS</title>
      <link>/working-notes/2014/wn_2014-06/local-install-wrf-ems/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/local-install-wrf-ems/</guid>
      <description>WRF ems can be installed locally using the file inside the folder releases Based on the WRF ems manual, chapter2, installation from local network, use ./ems_install.pl --install --repodir &#39;releases folder content&#39; After this the ems will be installed, for running the user&amp;rsquo;s default shell has to be set into tcsh, by following this as follows echo $SHELL if it gives /bin/bash change into tcsh by chsh -s /bin/tcsh, still after this it was showing the terminal without any $ or username.</description>
    </item>
    
    <item>
      <title>The SSH for WRF EMS Cluster running</title>
      <link>/working-notes/2014/wn_2014-06/ssh-for-wrf-ems-cluster-running/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/ssh-for-wrf-ems-cluster-running/</guid>
      <description>For executing the WRF EMS in cluster computer mode, pass wordless ssh is required, it is specified in .../runs/domain/config/ems_run/run_ncpus.conf, the content of files is as follows
LOG: R.Rozumalski - NWS January 2012 - I&#39;m still gruven # # Running the WRF on a cluster? # # If so, then make sure you that you have passwordless SSH on machines that # you plan to use when running on multiple nodes (not necessary for single # workstation runs).</description>
    </item>
    
    <item>
      <title>WRF EMS install and running in IBM X3100 M4</title>
      <link>/working-notes/2014/wn_2014-06/wrf-ems-install-and-running-in-ibm-x3100-m4/</link>
      <pubDate>Thu, 05 Jun 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-06/wrf-ems-install-and-running-in-ibm-x3100-m4/</guid>
      <description>Memory limit error is getting for 1km resolution domains in HP i5 system with a quad-core processor and 8GB memory. So made a try to make a cluster with another HP laptop with the same configuration, this step also returns EXIT file9, memory limit error. Changed the domain to make it 3 km resolution and ran in a cluster setup and it took 1 hour for four domains from 81km to 3km at 3-hour interval, so for 48-hour simulation, it would be taking a 16-hour model running.</description>
    </item>
    
    <item>
      <title>Python to query and edit json files</title>
      <link>/working-notes/2014/wn_2014-05/python-querying-and-editing-json/</link>
      <pubDate>Wed, 28 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/python-querying-and-editing-json/</guid>
      <description>For most of the works related to this and this It involves editing and querying of JSON and its formats such as GeoJson and Topojson In cbe-air web application, topjson is going to act as map element, and its editing required for real-time map generation and map styling In node.js based web application for visualizing model output, NetCDF output from WRF has to converted into geojson and made similar with the earth wind data formate.</description>
    </item>
    
    <item>
      <title>Merge multiple geojsons into single file</title>
      <link>/working-notes/2014/wn_2014-05/merge-multiple-geojsons-into-single-file/</link>
      <pubDate>Sat, 24 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/merge-multiple-geojsons-into-single-file/</guid>
      <description>For converting qs into cbe-air, the map is rendered using geojson rendering capability of GitHub The marker was easily made into geojson from QGIS and org2ogr as a shapefile. based on this under section &amp;ldquo;getting map data,&amp;rdquo; the command is
ogr2ogr -f GeoJSON point.json point.shp  and adding this script line in html
&amp;lt;script src=&amp;quot;https://embed.github.com/view/geojson/saconswl/cbeair/gh-pages/cbe-s.json?height=530&amp;amp;width=1300&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;  under div map
 To include Coimbatore city limits along with point marker, adding another script line of GitHub embed renderers another map!</description>
    </item>
    
    <item>
      <title>Node js with AIRwind and Earthwind applications</title>
      <link>/working-notes/2014/wn_2014-05/node-js-withairwindandearthwind/</link>
      <pubDate>Fri, 23 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/node-js-withairwindandearthwind/</guid>
      <description>To address objective2 of web processing service for real-time air pollution model, the implementation with node.jsearth or airis planned. These are the perfect match for this objective in showing the grandioseness of air circulation and how air pollution effect this grandioseness in real time animation of wind As a node.js web application, it has to installed as specified in the project&amp;rsquo;s readme. For node installation followed this wonderful tutorial, it involves and checking with node -v and npm -v</description>
    </item>
    
    <item>
      <title>The json data into SQLinsert with python</title>
      <link>/working-notes/2014/wn_2014-05/json-data-into-sqlinsert-with-python/</link>
      <pubDate>Fri, 23 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/json-data-into-sqlinsert-with-python/</guid>
      <description>To start with json data in python and loop over its elements
import json json_data=open(&#39;data.json&#39;) data=json.load(json_data) a=data[0][&#39;samples&#39;] for rs in a: print rs[&#39;wind&#39;]  To join two list as a column in python, to join two list inpython
for c1, c2 in zip(de, c): print &amp;quot;%-9s %s&amp;quot; % (c1, c2)  To append loop items into a array
c=[] for rs in a: c.append(rs[&#39;wind&#39;])  To remove u from list elemnt</description>
    </item>
    
    <item>
      <title>Querying netcdf with python and KDtree algorithm</title>
      <link>/working-notes/2014/wn_2014-05/querying-netcdf-with-python-kdtree/</link>
      <pubDate>Wed, 21 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/querying-netcdf-with-python-kdtree/</guid>
      <description>To query a NetCDF with latitude and longitude is required for objective three, in which user pointed lat-long, revived as SMS from Android app has to parsed and find its model and nearest Dylos monitoring station to send replay. There is handy tutorial on this with elaboration on different implementations advantages In which most advanced querying based on KDtree, this implementation was used to query NetCDF generated from WRF model the code is as follows</description>
    </item>
    
    <item>
      <title>Installing netcdf python in Ubuntu1204</title>
      <link>/working-notes/2014/wn_2014-05/installing-netcdf-python-in-ubuntu1204/</link>
      <pubDate>Tue, 20 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/installing-netcdf-python-in-ubuntu1204/</guid>
      <description>To install NetCDF-python requires HDF, based on this The HDF installation from source got failed, used the synaptic package manager instead to install HDF5 Downloaded NetCDF-python, extracted and run python setup.py install Failed, saying NetCDF is not found in usr/ So followed this, downloaded version of NetCDF-4.0.1, placed in /usr/local Doing cd into netcdf-4.0.1, and run the code
LDFLAGS=-L/usr/local/lib CPPFLAGS=-I/usr/local/include ./configure --enable-netcdf-4 --enable-dap --enable-shared --prefix=/usr/local  Then doing sudo make and then sudo make install, seems got installed, then went into netcdf4-python as given in this</description>
    </item>
    
    <item>
      <title>Converting WRF ouput netcdf into json</title>
      <link>/working-notes/2014/wn_2014-05/converting-wrf-ouput-netcdf-into-json/</link>
      <pubDate>Mon, 19 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/converting-wrf-ouput-netcdf-into-json/</guid>
      <description>Tried with grib2json for converting NetCDF into JSON 1 For this NetCDF has to converted into grib2 For converting into grib2, python based iris is useful 2, but only work with cf compliant NetCDF WRF output in NetCDF is not a cf compliant So has to use a tool which converts WRF NetCDF into CF compliant The Wrfncxnj.py tool 3 exactly do this with more functions such as extraction of variables.</description>
    </item>
    
    <item>
      <title>The database MySQL for Gammu</title>
      <link>/working-notes/2014/wn_2014-05/gammu-mysql/</link>
      <pubDate>Fri, 16 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/gammu-mysql/</guid>
      <description>Followed this,this The running, file-based gammu-smsd backend configuration file is as follows
 nano /etc/gammu-smsdrc # Configuration file for Gammu SMS Daemon # Gammu library configuration, see gammurc(5) [gammu] # Please configure this! port = /dev/ttyUSB2 model = connection = at synchronizetime = yes #logfile = /home/debian/gammulog #logformat = textalldate use_locking = gammuloc = # SMSD configuration, see gammu-smsdrc(5) [smsd] #debuglevel = 255 #Service = sql #Driver = sqlite3 #database = kalkun.</description>
    </item>
    
    <item>
      <title>QualitySCHU to cbe air</title>
      <link>/working-notes/2014/wn_2014-05/qualityschu-to-cbe-air/</link>
      <pubDate>Thu, 15 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/qualityschu-to-cbe-air/</guid>
      <description>Qualityschu(qs) is a web application coupled with istsos sensor web; it&amp;rsquo;s elegant, simple HTML with javascript design makes it easy to work with and to be educative. It is base code to build particulate matter air quality monitors web application in Coimbatore named cbeair web application. The app&amp;rsquo;s main difference with the source(qs) would be its ability to work with GitHub pages It is an HTML file with map and menu javascript files, main functions it provides is map view, table, chart view, and download functions coupled with istSOS.</description>
    </item>
    
    <item>
      <title>Edit a table in postgresql</title>
      <link>/working-notes/2014/wn_2014-05/edit-a-table-in-postgresql/</link>
      <pubDate>Mon, 12 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/edit-a-table-in-postgresql/</guid>
      <description>to list databases  \list
 to list tables in a database based on this  SELECT tableschema,tablename FROM informationschema.tables ORDER BY tableschema,table_name;
 to view details about a table  \d table.name
 to edit a column in a table  ALTER TABLE cbed.measures ALTER COLUMN val_msr TYPE numeric(14,6);</description>
    </item>
    
    <item>
      <title>Convert pandas dataframe into a PDF file using Latex</title>
      <link>/working-notes/2014/wn_2014-05/pandas-dataframe-into-latex-pdf/</link>
      <pubDate>Sat, 10 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/pandas-dataframe-into-latex-pdf/</guid>
      <description>Huge data frames which go for multiple A4 pages landscape is difficult to make in excel The alternative uses python pandas and convert the pandas&amp;rsquo; data frame into pdf through latex or HTML, latex is promising for just printing Basically from ([1])(http://stackoverflow.com/questions/14380371/export-a-latex-table-from-pandas-dataframe) for convert data frame into tex and this for converting Tex into landscape pdf document The python script to make table text is as follows, It is mostly from third answer [1], and bold column heading write python trick from</description>
    </item>
    
    <item>
      <title>IStSOS Data formating using Python</title>
      <link>/working-notes/2014/wn_2014-05/onpython/</link>
      <pubDate>Sat, 10 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/onpython/</guid>
      <description>Use Python with SQLite, istsos data formate and timeseries subsampling (downscaling).
 With sqlite to convert a list, for example cur.fetch from database like sqlite,
[&#39;2014-04-30T10:25,2797,147&#39;, &#39;2014-04-30T10:27,2639,174&#39;, &#39;2014-04-30T10:29,2645,158&#39;, &#39;2014-04-30T10:31,2676,149&#39;]  Use print &amp;ldquo;\n&amp;rdquo;.join(b) based on this gives
&amp;quot;2014-04-30T10:25,2797,147 2014-04-30T10:27,2639,174 2014-04-30T10:29,2645,158 2014-04-30T10:31,2676,149&amp;quot;  To remove double quotes from above to write into a.DAT, tried almost two hours then find out that the used method will not do this. The full code is as follows with uncommented lines showing failed attempts.</description>
    </item>
    
    <item>
      <title>Installing mysql database in Beagle Bone Black</title>
      <link>/working-notes/2014/wn_2014-05/mysql-bbb/</link>
      <pubDate>Tue, 06 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/mysql-bbb/</guid>
      <description>Based on 1 and 2
apt-get install mysql-server apt-get install phpmyadmin  The phpmyadmin not showing up, so working with mysql command line followed this to get into mysql shell /usr/bin/mysql -u root -p
 To create a new database followed this by create database gammu in mysql shell
 To execute gammu mysql.sql file to create tables for gammu back end, used this mysql -u root -p’passwd’ and then run the command gammu &amp;lt; /home/debian/gammu-1.</description>
    </item>
    
    <item>
      <title>Mount SDcard in Beagle Bone Black</title>
      <link>/working-notes/2014/wn_2014-05/mount-sdcard-with-bbb/</link>
      <pubDate>Tue, 06 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/mount-sdcard-with-bbb/</guid>
      <description>Mounting SD card is not so easy with BBB, it is due to the problem of MySQL error, saying sock file is missing and while starting saying /var folder is full By checking df -h shows it is true. So only way to increase the partition in sd card, but regular sd card with partition cannot mount with BBB, due to some kernel inclination There is a workaround, based on [this] under title code listing 7.</description>
    </item>
    
    <item>
      <title>Compile Gammu in Beagle Bone Black</title>
      <link>/working-notes/2014/wn_2014-05/gammu-compile-bbb/</link>
      <pubDate>Mon, 05 May 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-05/gammu-compile-bbb/</guid>
      <description>Gammu was compiled in BBB following this The dependencies were installed using this command
apt-get install cmake python-dev pkg-config libmysqlclient-dev libpq-dev \ libcurl4-gnutls-dev libusb-1.0-0-dev libdbi0-dev libbluetooth-dev \ libgudev-1.0-dev libglib2.0-dev unixodbc-dev  The command ./configure gives error of CMake Error: CMake was unable to find a build program corresponding to &amp;quot;Unix Makefiles&amp;quot;.
 Based on this installed make by apt-get install make after this the error goes away
 Executed this command cmake .</description>
    </item>
    
    <item>
      <title>Cross Origin Resource Sharing</title>
      <link>/working-notes/2014/wn_2014-04/cross-origin-resource-sharing/</link>
      <pubDate>Wed, 30 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/cross-origin-resource-sharing/</guid>
      <description>While working with qs into cbe-air, in populating table and charts with istsos JSON data, stuck with an error The error was not informative in firebug. firefox, a simple correct response of 200 with red fonts and also not giving any console.log in JS After trying with changing jquery version, different old edited version of qs, experimenting with different JSON URL found no clue, finally Tried to run the server in chromium and its developer tools option gives an error of</description>
    </item>
    
    <item>
      <title>File system based SOS using github AJAX</title>
      <link>/working-notes/2014/wn_2014-04/file-system-based-sos-using-github-ajax/</link>
      <pubDate>Thu, 24 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/file-system-based-sos-using-github-ajax/</guid>
      <description> Found an article discussing similar to this line. https://www.academia.edu/1502083/A_flexible_geospatial_sensor_observation_service_for_diverse_sensor_data_based_on Some related paperes are Monitoring real-time environmental information using Web 2.0 and GIServices technology and Integrating Sensor Webs with Modeling and Data-assimilation Applications: An SOA Implementation Also see  http://buyya.com/papers/SensorWebChapter.pdf http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=4526452&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4526452  For simple SOS implementation https://github.com/jcu-eresearch/python-simplesos  </description>
    </item>
    
    <item>
      <title>IRIS install</title>
      <link>/working-notes/2014/wn_2014-04/iris-install/</link>
      <pubDate>Thu, 24 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/iris-install/</guid>
      <description>IRIS is a python tool for working with NetCDF and Grib files. It is installed to convert WRF output in netCDF to grib2 formate, which is the need for grib2json tool. IRIS is dependent on a large number of scientific python libraries. Most of the libraries are the python and installed through
pip install library  Installation further gets erroneous due to unavailability of NetCDF, HDF5, NetCDF-python packages.
 Has to follow this note (http://code.</description>
    </item>
    
    <item>
      <title>Package using maven</title>
      <link>/working-notes/2014/wn_2014-04/package-using-maven/</link>
      <pubDate>Thu, 24 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/package-using-maven/</guid>
      <description>Java-based tools such as grib2json have to compiled with maven The latest maven is installed, following as per this note (https://github.com/saconswl/Real_time_air_pollution_Mod_Proj-2013-2014/blob/home/working_notes/wn_2013-10/Installing_maven_in_Ubuntu_12.04.md) The tool grib2json requires java 1.7
 Done search due to error while trying for
mvn package  It gives the error of
Error: JAVA_HOME is not defined correctly. We cannot execute ”/usr/lib/jvm/jdk1.7.0”/bin/java  Later found that the system does not contain jdk1.7.0
 Installed jdk1.7.0 following (http://askubuntu.com/questions/117189/apt-get-install-openjdk-7-jdk-doesnt-install-javac-why)</description>
    </item>
    
    <item>
      <title>Gammu smsd shared memeory error for Huwaei E303F</title>
      <link>/working-notes/2014/wn_2014-04/gammu-smsd-shared-memeory-error-for-huwaei-e303f/</link>
      <pubDate>Mon, 21 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/gammu-smsd-shared-memeory-error-for-huwaei-e303f/</guid>
      <description>Running gammu and gammu-smsd backed by Mysql in Ubuntu 12.04, All these setup was running without error using Huwaei E173 data card. However, upgraded model of this, Huwaei E303F, working fine with gammu, but starting gammu-smsd collapsing gammu. For example gammu --identify says phone not connected Looking into file of gammurc ~/.gammurc
port = /dev/ttyUSB0 model = auto connection = at synchronizetime = yes logfile = /home/user/gammu.log logformat = textalldate use_locking = gammuloc =  Looking into file gammu-smsdrc ~/etc/gammu-smsdrc</description>
    </item>
    
    <item>
      <title>Program for usbreset</title>
      <link>/working-notes/2014/wn_2014-04/usbreset-program/</link>
      <pubDate>Mon, 21 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/usbreset-program/</guid>
      <description>From this ask ubuntu answer. to reset the USB data card Saving this code as usbreset.c
/* usbreset -- send a USB port reset to a USB device */ #include &amp;lt;stdio.h&amp;gt; #include &amp;lt;unistd.h&amp;gt; #include &amp;lt;fcntl.h&amp;gt; #include &amp;lt;errno.h&amp;gt; #include &amp;lt;sys/ioctl.h&amp;gt; #include &amp;lt;linux/usbdevice_fs.h&amp;gt; int main(int argc, char **argv) { const char *filename; int fd; int rc; if (argc != 2) { fprintf(stderr, &amp;quot;Usage: usbreset device-filename\n&amp;quot;); return 1; } filename = argv[1]; fd = open(filename, O_WRONLY); if (fd &amp;lt; 0) { perror(&amp;quot;Error opening output file&amp;quot;); return 1; } printf(&amp;quot;Resetting USB device %s\n&amp;quot;, filename); rc = ioctl(fd, USBDEVFS_RESET, 0); if (rc &amp;lt; 0) { perror(&amp;quot;Error in ioctl&amp;quot;); return 1; } printf(&amp;quot;Reset successful\n&amp;quot;); close(fd); return 0; }  Compile code using cc usbreset.</description>
    </item>
    
    <item>
      <title>Compiling WRF CHEM</title>
      <link>/working-notes/2014/wn_2014-04/compiling-wrf-chem/</link>
      <pubDate>Fri, 18 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/compiling-wrf-chem/</guid>
      <description>Based on this tutorial
 In WRFV3 folder, (the &amp;lsquo;chem&amp;rsquo; folder has to copied inside of this folder) entering
$ ./configure Checking for perl5... no checking for perl... found /usr/bin/perl (perl) ** WARNING: No path to NETCDF and environment variable NETCDF not set. ** would you like me to try to fix? [y] y Enter full path to NetCDF include directory on your system /usr/include Enter full path to NetCDF library directory on your system /usr/lib created new .</description>
    </item>
    
    <item>
      <title>CSV edit by pandas</title>
      <link>/working-notes/2014/wn_2014-04/csv-edit-by-pandas/</link>
      <pubDate>Thu, 10 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/csv-edit-by-pandas/</guid>
      <description>to import csv file into python
import pandas data = pd.read_csv(&#39;/home/hoopoe/Documents/Real_time_air_pollution_Mod_Proj-2013-2014/obj2/237.csv&#39;)  to query the specific column in the data frame
data[&#39;SamplingDate&#39;]  to specify the column as DateTime formate column for pandas
data[&#39;SamplingDate&#39;] = pd.to_datetime(data[&#39;SamplingDate&#39;])  to avoid date and month mismatch specify the format of date as
data[&#39;SamplingDate&#39;] = pd.to_datetime(data[&#39;SamplingDate&#39;],format=&#39;%d/%m/%Y&#39;)  to sort the data based on date column descending
dataso=data.sort(&#39;SamplingDate&#39;, ascending=False)  to join two data frame in particular column</description>
    </item>
    
    <item>
      <title>Dylos monitor setup full with log</title>
      <link>/working-notes/2014/wn_2014-04/dylos-monitor-setup-full-with-log/</link>
      <pubDate>Thu, 10 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/dylos-monitor-setup-full-with-log/</guid>
      <description>Following http://hintshop.ludvig.co.nz/show/persistent-names-usb-serial-devices/ and UdevrulesforUSB&amp;rsquo;sattachedtoRPi
 Create a udev rule for RPI, edited the file /etc/udev/rules.d using command sudo nano /etc/udev/rules.d/90-phone.rules and add following lines to giving a persistent name for USB data card (HUWAEI E303F) and USB to serial cable for Dylos monitor.
KERNEL==&amp;quot;ttyUSB*&amp;quot;, ATTRS{idVendor}==&amp;quot;12d1&amp;quot;, ATTRS{idProduct}==&amp;quot;1506&amp;quot;, NAME=&amp;quot;phone&amp;quot;, MODE=&amp;quot;0666&amp;quot;,SYMLINK+=&amp;quot;mobile&amp;quot; KERNEL==&amp;quot;ttyUSB*&amp;quot;, ATTRS{idVendor}==&amp;quot;067b&amp;quot;, ATTRS{idProduct}==&amp;quot;2303&amp;quot;, NAME=&amp;quot;dylos&amp;quot;, MODE=&amp;quot;0666&amp;quot;,SYMLINK+=&amp;quot;dylos&amp;quot;  Adding udev rules gives option select devices, while connecting two or more devices. The folder /dev shows the files for USB data card and USB to serial cable for Dylos monitor.</description>
    </item>
    
    <item>
      <title>Sending SMS with AT and python</title>
      <link>/working-notes/2014/wn_2014-04/sending-sms-with-at-and-python/</link>
      <pubDate>Tue, 08 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/sending-sms-with-at-and-python/</guid>
      <description>Huwaei E303F is not working with Gammu, especially in RPi. So found a method to send SMS using this data card without using gammu but using simple AT commands.
 primarly based on this 1 and this 2 the code written for sending SMS from Dylos serial is as follows
#!/usr/bin/python import serial import time from curses import ascii import sqlite3 as lite import logging logger = logging.getLogger(&#39;lbm1&#39;) hdlr = logging.</description>
    </item>
    
    <item>
      <title>HYSPLIT compile</title>
      <link>/working-notes/2014/wn_2014-04/hysplit_compile/</link>
      <pubDate>Fri, 04 Apr 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-04/hysplit_compile/</guid>
      <description>Compiling HYSPLIT in Ubuntu 12.04 was hurdled by a error related with netcdf.
 Installation of netcdf is through compiling and package installation from synaptic package manager. Synaptic and latest source download compilation gives error of &amp;ldquo;no netcdf.inc&amp;rdquo; in hysplit compile. It is due to a fortran binding lapse in latest verision. So used a old version of the netcdf 3.6.3 and compiled following this&amp;ndash; http://code.google.com/p/netcdf4-python/wiki/UbuntuInstall has to give sudo in make and make install, with the first comment disable-shared</description>
    </item>
    
    <item>
      <title>Sending SMS with Beagle bone black</title>
      <link>/working-notes/2014/wn_2014-03/sending_sms_with_beagle_bone_black/</link>
      <pubDate>Tue, 25 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>/working-notes/2014/wn_2014-03/sending_sms_with_beagle_bone_black/</guid>
      <description>BBB is setup with Angstrom Linux, first connected to Ubuntu lap using USB wire from BBB, then a USB was connected to the powered USB hub, in which a serial USB connector and a Huwaei e 173 data (This tested for Huwaei E303F, it worked) card were connected. ssh&amp;rsquo;s into it using ssh 192.168.7.2 -l root and password blank (an enter) lsusb shows Huwaei with modem and serial usb. Need to make usb_modeswitch for Huwaei GSM to sent SMS, so usb_modeswitch has to be installed in the Angstrom, downloaded ipk file (deb in angstrom) from http://feeds.</description>
    </item>
    
  </channel>
</rss>